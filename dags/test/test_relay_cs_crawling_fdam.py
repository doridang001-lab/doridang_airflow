# dags/relay_cs_crawling_fdam.py

"""
Relay FMS ë§¤ì¥CS í¬ë¡¤ë§ DAG
- ë¡œì»¬ DB ì €ì¥ â†’ OneDrive ë°±ì—…
"""

import os
import sys
from pathlib import Path
import shutil
from datetime import datetime

import pandas as pd
import pendulum
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.trigger_rule import TriggerRule
from airflow.providers.smtp.operators.smtp import EmailOperator

# modules ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from modules.extract.crawling_relay_cs_fdam import run_relay_cs_crawling

# í˜„ì¬ íŒŒì¼ëª…
filename = os.path.basename(__file__)

# ========== ì„¤ì • ==========
RELAY_ID = "ì¡°ë¯¼ì¤€"
RELAY_PW = "1234"

# ë¡œì»¬ DB ê²½ë¡œ (ì»¨í…Œì´ë„ˆ ì•ˆ)
LOCAL_CSV = Path("/opt/airflow/Doridang/ë¬¸ì˜_DB/relay_cs.csv")

# OneDrive ë°±ì—… ê²½ë¡œ
ONEDRIVE_BACKUP = Path("/opt/airflow/onedrive_backup/ë¬¸ì˜_DB")

to_members = ['a17019@kakao.com']


# ============================================================
# í•¨ìˆ˜: CSV ì €ì¥ (ì¤‘ë³µ ì œê±°)
# ============================================================
def save_cs_to_csv(
    new_df: pd.DataFrame,
    csv_path: Path,
    duplicate_subset: list = None
) -> dict:
    """
    CS ë°ì´í„°ë¥¼ CSVì— ì €ì¥ (ì¤‘ë³µ ì œê±° í›„ ì‹ ê·œê±´ë§Œ ì¶”ê°€)
    """
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    if duplicate_subset is None:
        duplicate_subset = ['ì ‘ìˆ˜_ì ‘ìˆ˜ë²ˆí˜¸']
    
    try:
        # ê¸°ì¡´ CSV ë¡œë“œ
        if csv_path.exists():
            existing_df = pd.read_csv(csv_path, encoding='utf-8-sig')
            print(f"ğŸ“ ê¸°ì¡´ ë°ì´í„°: {len(existing_df)}ê±´")
            
            # ì¤‘ë³µ ì œê±°
            before_count = len(new_df)
            merged_df = pd.concat([existing_df, new_df], ignore_index=True)
            merged_df = merged_df.drop_duplicates(
                subset=duplicate_subset, 
                keep='last'
            )
            
            new_count = len(merged_df) - len(existing_df)
            print(f"âœ… ì‹ ê·œ ë°ì´í„°: {new_count}ê±´ (ì´ {before_count}ê±´ ì¤‘)")
            
        else:
            merged_df = new_df
            new_count = len(new_df)
            print(f"âœ… ì‹ ê·œ íŒŒì¼ ìƒì„±: {new_count}ê±´")
        
        # CSV ì €ì¥
        merged_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        return {
            'success': True,
            'total': len(merged_df),
            'new': new_count,
            'message': f'ì €ì¥ ì™„ë£Œ: ì´ {len(merged_df)}ê±´ (ì‹ ê·œ {new_count}ê±´)'
        }
        
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'error': str(e),
            'message': f'ì €ì¥ ì‹¤íŒ¨: {e}'
        }


# ============================================================
# í•¨ìˆ˜: OneDrive ë°±ì—…
# ============================================================
def backup_file_to_onedrive(
    local_file_path: Path,
    onedrive_path: Path,
    max_backups: int = 20
) -> dict:
    """
    ë¡œì»¬ íŒŒì¼ì„ OneDriveë¡œ ë°±ì—…
    """
    try:
        onedrive_path.mkdir(parents=True, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë°±ì—…
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = onedrive_path / f"backup_{timestamp}_{local_file_path.name}"
        
        # ë³µì‚¬
        shutil.copy2(local_file_path, backup_file)
        print(f"âœ… íŒŒì¼ ë°±ì—…: {backup_file}")
        
        # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
        backups = sorted(onedrive_path.glob("backup_*.csv"))
        if len(backups) > max_backups:
            for old_backup in backups[:-max_backups]:
                old_backup.unlink()
                print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {old_backup.name}")
        
        return {
            'success': True,
            'backup_path': str(backup_file),
            'message': f'ë°±ì—… ì™„ë£Œ: {backup_file.name}'
        }
        
    except Exception as e:
        print(f"âŒ ë°±ì—… ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'error': str(e),
            'message': f'ë°±ì—… ì‹¤íŒ¨: {e}'
        }


# ============================================================
# Task 1: í¬ë¡¤ë§
# ============================================================
def crawl_relay_cs(**context):
    """Relay FMS ë§¤ì¥CS í¬ë¡¤ë§"""
    df = run_relay_cs_crawling(
        user_id=RELAY_ID,
        password=RELAY_PW,
        headless=True,  # ì„œë²„ í™˜ê²½
        window_size='normal'
    )
    
    # XComì— ì €ì¥
    context['task_instance'].xcom_push(key='df_json', value=df.to_json())
    context['task_instance'].xcom_push(key='row_count', value=len(df))
    
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(df)}ê±´")
    return f"í¬ë¡¤ë§ ì™„ë£Œ: {len(df)}ê±´"


# ============================================================
# Task 2: ë¡œì»¬ CSV ì €ì¥
# ============================================================
def save_to_local_csv(**context):
    """ë¡œì»¬ DBì— CSV ì €ì¥"""
    ti = context['task_instance']
    df_json = ti.xcom_pull(task_ids='crawl_task', key='df_json')
    
    df = pd.read_json(df_json)
    
    result = save_cs_to_csv(
        new_df=df,
        csv_path=LOCAL_CSV,
        duplicate_subset=['ì ‘ìˆ˜_ì ‘ìˆ˜ë²ˆí˜¸']
    )
    
    # XComì— ê²°ê³¼ ì €ì¥
    ti.xcom_push(key='save_result', value=result['message'])
    
    print(result['message'])
    return result['message']


# ============================================================
# Task 3: OneDrive ë°±ì—…
# ============================================================
def backup_to_onedrive_task(**context):
    """OneDriveë¡œ ë°±ì—…"""
    result = backup_file_to_onedrive(
        local_file_path=LOCAL_CSV,
        onedrive_path=ONEDRIVE_BACKUP,
        max_backups=20
    )
    
    # XComì— ì €ì¥
    context['task_instance'].xcom_push(key='backup_result', value=result['message'])
    
    print(result['message'])
    return result['message']


# ============================================================
# DAG ì •ì˜
# ============================================================
with DAG(
    dag_id=filename.replace('.py', ''),
    schedule="0 9 * * *",  # ë§¤ì¼ ì˜¤ì „ 9ì‹œ
    start_date=pendulum.datetime(2026, 1, 1, tz="Asia/Seoul"),
    catchup=False,
    max_active_runs=1,
    tags=['crawling', 'relay', 'cs', 'fdam']
) as dag:
    
    # Task 1: í¬ë¡¤ë§
    crawl_task = PythonOperator(
        task_id='crawl_task',
        python_callable=crawl_relay_cs,
    )
    
    # Task 2: ë¡œì»¬ CSV ì €ì¥
    save_task = PythonOperator(
        task_id='save_local_csv',
        python_callable=save_to_local_csv,
    )
    
    # Task 3: OneDrive ë°±ì—…
    backup_task = PythonOperator(
        task_id='backup_to_onedrive',
        python_callable=backup_to_onedrive_task,
        trigger_rule=TriggerRule.ALL_SUCCESS
    )
    
    # Task 4: ì´ë©”ì¼ ì•Œë¦¼
    email_task = EmailOperator(
        task_id='send_email',
        conn_id='conn_smtp_gmail',
        to=to_members,
        subject='[Relay CS] í¬ë¡¤ë§ ì™„ë£Œ',
        html_content="""
        <html>
            <body>
                <h3>âœ… Relay FMS ë§¤ì¥CS í¬ë¡¤ë§ ì™„ë£Œ</h3>
                
                <p><strong>ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:</strong></p>
                <pre>{{ task_instance.xcom_pull(task_ids='crawl_task', key='return_value') }}</pre>
                
                <p><strong>ğŸ’¾ ì €ì¥ ê²°ê³¼:</strong></p>
                <pre>{{ task_instance.xcom_pull(task_ids='save_local_csv', key='save_result') }}</pre>
                
                <p><strong>â˜ï¸ ë°±ì—… ê²°ê³¼:</strong></p>
                <pre>{{ task_instance.xcom_pull(task_ids='backup_to_onedrive', key='backup_result') }}</pre>
                
                <hr>
                <p style="color: gray; font-size: 12px;">
                    ì‹¤í–‰ ì‹œê°„: {{ ts }}<br>
                    DAG ID: {{ dag.dag_id }}
                </p>
            </body>
        </html>
        """,
        trigger_rule=TriggerRule.ALL_SUCCESS
    )
    
    # Task ì˜ì¡´ì„±
    crawl_task >> save_task >> backup_task >> email_task