import glob
import os
import pandas as pd
from pathlib import Path
from typing import List, Union, Optional
from modules.transform.utility.paths import ONEDRIVE_DB, COLLECT_DB, LOCAL_DB, TEMP_DIR

# ============================================================
# CSV ë¡œë“œ í•¨ìˆ˜ë“¤
# ============================================================
def read_csv_glob(
    files: Union[str, List[str]],
    add_source_col: bool = False,
    source_col_name: str = "source_file",
    ignore_index: bool = True,
    on_error: str = "raise",
    **read_csv_kwargs,
) -> pd.DataFrame:
    """Glob íŒ¨í„´ ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ì—¬ëŸ¬ CSVë¥¼ ì½ì–´ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë°˜í™˜"""
    if isinstance(files, str):
        file_list = sorted(glob.glob(files))
    elif isinstance(files, list):
        file_list = files
    else:
        raise TypeError("filesëŠ” glob íŒ¨í„´(str) ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸(list[str])ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    if not file_list:
        raise FileNotFoundError("ë§¤ì¹­ë˜ëŠ” CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    dfs: List[pd.DataFrame] = []
    for fpath in file_list:
        try:
            df = pd.read_csv(fpath, **read_csv_kwargs)
            if add_source_col:
                df[source_col_name] = os.path.basename(fpath)
            dfs.append(df)
        except Exception as e:
            if on_error == "skip":
                print(f"[ê²½ê³ ] íŒŒì¼ '{fpath}'ì„(ë¥¼) ê±´ë„ˆëœë‹ˆë‹¤. ì´ìœ : {e}")
                continue
            raise

    if not dfs:
        return pd.DataFrame()

    return pd.concat(dfs, ignore_index=ignore_index)


def load_data(
    file_path,
    xcom_key='parquet_path',
    use_glob=True,
    dedup_key=None,
    add_row_hash=False,  # â­ ìƒˆ ì˜µì…˜: í–‰ í•´ì‹œ ì¶”ê°€
    **context
):
    """
    CSV ë°ì´í„° ë²”ìš© ë¡œë“œ í•¨ìˆ˜ (ì¤‘ë³µ ì œê±° ë¡œì§ ê°œì„ )
    
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ ë˜ëŠ” glob íŒ¨í„´
        xcom_key: XComì— ì €ì¥í•  í‚¤ ì´ë¦„
        use_glob: Trueë©´ glob íŒ¨í„´ìœ¼ë¡œ ì²˜ë¦¬, Falseë©´ ë‹¨ì¼ íŒŒì¼
        dedup_key: ì¤‘ë³µ ì œê±° ê¸°ì¤€ ì»¬ëŸ¼ (str ë˜ëŠ” list, Noneì´ë©´ ì¤‘ë³µ ì œê±° ì•ˆí•¨)
        add_row_hash: Trueë©´ ê° í–‰ì— ê³ ìœ  í•´ì‹œ ì¶”ê°€ (_row_hash ì»¬ëŸ¼)
                      ê°™ì€ ì£¼ë¬¸ì˜ ê°™ì€ ë©”ë‰´ 2ê°œ(ì™„ì „ ë™ì¼ í–‰)ë„ êµ¬ë¶„ ê°€ëŠ¥
        
    Returns:
        str: ì²˜ë¦¬ ê²°ê³¼ ë©”ì‹œì§€
        
    ì¤‘ë³µ ì œê±° ì „ëµ:
    ================
    1. collected_atì„ í‚¤ì—ì„œ ì œì™¸í•˜ì—¬ ë‹¤ë¥¸ ì‹œì  ì—…ë¡œë“œ ì‹œì—ë„ ì¤‘ë³µ ê°ì§€
    2. add_row_hash=True ì‚¬ìš© ì‹œ ë™ì¼ í–‰ë„ êµ¬ë¶„ ê°€ëŠ¥ (í–‰ ì¸ë±ìŠ¤ + ì†ŒìŠ¤íŒŒì¼ ê¸°ë°˜)
    3. ê°™ì€ ì£¼ë¬¸ì˜ ê°™ì€ ë©”ë‰´ 2ê°œ ì£¼ë¬¸ â†’ ë³´ì¡´ë¨ (row_hashê°€ ë‹¤ë¦„)
    """
    import glob as glob_module
    import hashlib
    from pathlib import Path
    ti = context['task_instance']
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    parquet_path = temp_dir / f"{xcom_key}_{context['ds_nodash']}.parquet"
    
    try:
        # 1. íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        if use_glob:
            file_list = sorted(glob_module.glob(str(file_path)))
            print(f"[DEBUG] glob íŒ¨í„´: {file_path}")
            print(f"[DEBUG] ì°¾ì€ íŒŒì¼: {len(file_list)}ê°œ")
        else:
            file_list = [str(file_path)] if Path(file_path).exists() else []
            print(f"[DEBUG] ë‹¨ì¼ íŒŒì¼: {file_path}")
        
        if not file_list:
            print(f"[ê²½ê³ ] íŒŒì¼ ì—†ìŒ: {file_path}")
            ti.xcom_push(key=xcom_key, value=None)
            return f"0ê±´ (íŒŒì¼ ì—†ìŒ)"
        
        print(f"[INFO] ë¡œë“œí•  íŒŒì¼: {len(file_list)}ê°œ")
        for fpath in file_list:
            print(f"   âœ“ {Path(fpath).name}")
        
        # 2. ì¸ì½”ë”© ìë™ ê°ì§€í•˜ë©° CSV ì½ê¸°
        dfs = []
        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']
        
        for file_idx, fpath in enumerate(file_list):
            df = None
            for encoding in encodings:
                try:
                    print(f"   ì‹œë„: {Path(fpath).name} ({encoding})")
                    df = pd.read_csv(fpath, encoding=encoding)
                    print(f"   âœ“ {encoding}ìœ¼ë¡œ ì½ìŒ: {len(df)}í–‰")
                    
                    # â­ í–‰ í•´ì‹œ ì¶”ê°€ (ì˜µì…˜) - íŒŒì¼ëª… ì œì™¸, ìˆœìˆ˜ ë°ì´í„° ê¸°ë°˜
                    if add_row_hash:
                        # â­ ìˆ˜ì •: íŒŒì¼ëª… ì œì™¸! ìˆœìˆ˜ ë°ì´í„° ê°’ë§Œìœ¼ë¡œ í•´ì‹œ ìƒì„±
                        # ì´ë ‡ê²Œ í•˜ë©´ ê°™ì€ ë‚´ìš©ì´ë©´ íŒŒì¼ëª…ì´ ë‹¬ë¼ë„ ê°™ì€ í•´ì‹œ
                        df['_row_hash'] = df.apply(
                            lambda row: hashlib.md5(
                                str(tuple(row.values)).encode('utf-8')
                            ).hexdigest()[:12],
                            axis=1
                        )
                        print(f"   [í•´ì‹œ] ë°ì´í„° ê¸°ë°˜ í•´ì‹œ: {df['_row_hash'].nunique()}ê°œ ê³ ìœ ê°’")
                    
                    dfs.append(df)
                    break
                except (UnicodeDecodeError, LookupError):
                    continue
            
            if df is None:
                print(f"   âœ— ì½ê¸° ì‹¤íŒ¨: {fpath} (ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨)")
                continue
        
        if not dfs:
            print(f"[ê²½ê³ ] ì½ì„ ìˆ˜ ìˆëŠ” íŒŒì¼ ì—†ìŒ")
            ti.xcom_push(key=xcom_key, value=None)
            return f"0ê±´ (íŒŒì¼ ì½ê¸° ì‹¤íŒ¨)"
        
        # 3. ëª¨ë“  íŒŒì¼ ë³‘í•©
        result_df = pd.concat(dfs, ignore_index=True) if len(dfs) > 1 else dfs[0]
        print(f"\në³‘í•© ì™„ë£Œ: {len(result_df):,}í–‰")
        
        # 4. ì¤‘ë³µ ì œê±° (ê°œì„ ëœ ë¡œì§)
        if dedup_key:
            before = len(result_df)
            
            if isinstance(dedup_key, str):
                dedup_cols = [dedup_key]
            else:
                dedup_cols = list(dedup_key)
            
            # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©
            valid_cols = [col for col in dedup_cols if col in result_df.columns]
            
            if valid_cols:
                # â­ ìˆ˜ì •: í•´ì‹œ ì—†ì´ ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ë§Œìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                # _row_hashëŠ” sub_order_id ìƒì„±ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©
                print(f"[ì¤‘ë³µì œê±°] í‚¤: {valid_cols}")
                result_df.drop_duplicates(subset=valid_cols, keep='first', inplace=True)
                
                after = len(result_df)
                removed = before - after
                if removed > 0:
                    print(f"[ì¤‘ë³µì œê±°] {removed:,}ê±´ ì œê±°ë¨")
                else:
                    print(f"[ì¤‘ë³µì œê±°] ì¤‘ë³µ ì—†ìŒ")
            else:
                print(f"[ê²½ê³ ] ì¤‘ë³µ ì œê±° ì»¬ëŸ¼ ì—†ìŒ: {dedup_cols}")
                print(f"[ê²½ê³ ] ì‹¤ì œ ì»¬ëŸ¼: {list(result_df.columns)[:10]}...")
        
        # 5. Parquetë¡œ ì €ì¥
        for col in result_df.columns:
            if result_df[col].dtype == 'object':
                if col in ['ìº í˜ì¸ID', 'ad_id', 'store_id']:
                    result_df[col] = result_df[col].astype(str)
        
        result_df.to_parquet(parquet_path, index=False, engine='pyarrow')
        print(f"\nâœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(result_df):,}ê±´")
        print(f"   ì €ì¥ ê²½ë¡œ: {parquet_path}")
        
        # 6. XComì— ê²½ë¡œ ì €ì¥
        ti.xcom_push(key=xcom_key, value=str(parquet_path))
        return f"{len(result_df):,}ê±´"
        
    except Exception as e:
        print(f"[ì—ëŸ¬] ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(traceback.format_exc())
        ti.xcom_push(key=xcom_key, value=None)
        return f"0ê±´ (ì—ëŸ¬)"


# ============================================================
# ë‚˜ë¨¸ì§€ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼
# ============================================================

def preprocess_df(
    input_task_id,
    input_xcom_key,
    output_xcom_key,
    **context
):
    """ë²”ìš© ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    import numpy as np
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(
        task_ids=input_task_id,
        key=input_xcom_key
    )
    
    df = pd.read_parquet(parquet_path)
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(df):,}í–‰")
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    processed_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    df.to_parquet(processed_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(processed_path))
    
    return f"ì „ì²˜ë¦¬: {len(df):,}í–‰"


def concat_data(input_tasks, output_xcom_key='merged_path', **context):
    """ì—¬ëŸ¬ ì „ì²˜ë¦¬ ë°ì´í„° ë³‘í•©"""
    ti = context['task_instance']
    
    if isinstance(input_tasks, list):
        input_tasks = {t: 'processed_path' for t in input_tasks}
    
    dfs = []
    for task_id, xcom_key in input_tasks.items():
        path = ti.xcom_pull(task_ids=task_id, key=xcom_key)
        if path:
            try:
                df = pd.read_parquet(path)
                if len(df) > 0:
                    if 'platform' in df.columns:
                        platforms = df['platform'].value_counts()
                        print(f"[{task_id}] Platform ë¶„í¬: {platforms.to_dict()}")
                    else:
                        print(f"âš ï¸ [{task_id}] platform ì»¬ëŸ¼ ì—†ìŒ!")
                    
                    dfs.append(df)
                    print(f"[{task_id}] {len(df):,}í–‰ ë¡œë“œ")
            except Exception as e:
                print(f"[{task_id}] ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    if not dfs:
        print("ë³‘í•©í•  ë°ì´í„° ì—†ìŒ")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "ë³‘í•© ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ"
    
    merged_df = pd.concat(dfs, ignore_index=True)
    
    if 'platform' in merged_df.columns:
        platforms_after = merged_df['platform'].value_counts()
        print(f"ë³‘í•© ì™„ë£Œ: {len(merged_df):,}í–‰ - Platform ë¶„í¬: {platforms_after.to_dict()}")
    else:
        print(f"ë³‘í•© ì™„ë£Œ: {len(merged_df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    merged_df.to_parquet(output_path, index=False)
    
    ti.xcom_push(key=output_xcom_key, value=str(output_path))
    return f"ë³‘í•© ì™„ë£Œ: {len(merged_df):,}í–‰"


def join_task(
    left_task,
    right_task,
    on=None,
    left_on=None,
    right_on=None,
    how='left',
    output_xcom_key='joined_path',
    **context
):
    """ë‘ taskì˜ ë°ì´í„°ë¥¼ join"""
    ti = context['task_instance']
    
    if isinstance(left_task, str):
        left_task = {'task_id': left_task, 'xcom_key': 'processed_path'}
    if isinstance(right_task, str):
        right_task = {'task_id': right_task, 'xcom_key': 'processed_path'}
    
    left_path = ti.xcom_pull(
        task_ids=left_task['task_id'],
        key=left_task['xcom_key']
    )
    if not left_path:
        print(f"[ì—ëŸ¬] ì™¼ìª½ ë°ì´í„° ì—†ìŒ: {left_task['task_id']}")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "join ì‹¤íŒ¨: ì™¼ìª½ ë°ì´í„° ì—†ìŒ"
    
    left_df = pd.read_parquet(left_path)
    print(f"[ì™¼ìª½] {left_task['task_id']}: {len(left_df):,}í–‰")
    
    right_path = ti.xcom_pull(
        task_ids=right_task['task_id'],
        key=right_task['xcom_key']
    )
    if not right_path:
        print(f"[ì—ëŸ¬] ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ: {right_task['task_id']}")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "join ì‹¤íŒ¨: ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ"
    
    right_df = pd.read_parquet(right_path)
    print(f"[ì˜¤ë¥¸ìª½] {right_task['task_id']}: {len(right_df):,}í–‰")
    
    if on is not None:
        print(f"\njoin ì‹¤í–‰: how={how}, on={on}")
        joined_df = left_df.merge(right_df, on=on, how=how)
    elif left_on is not None and right_on is not None:
        print(f"\njoin ì‹¤í–‰: how={how}, left_on={left_on}, right_on={right_on}")
        joined_df = left_df.merge(right_df, left_on=left_on, right_on=right_on, how=how)
    else:
        raise ValueError("on ë˜ëŠ” (left_on, right_on)ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    print(f"join ì™„ë£Œ: {len(joined_df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    joined_df.to_parquet(output_path, index=False)
    
    ti.xcom_push(key=output_xcom_key, value=str(output_path))
    return f"join ì™„ë£Œ: {len(joined_df):,}í–‰"

# csv ì €ì¥

def save_to_csv(
    input_task_id,
    input_xcom_key,
    output_csv_path=None,
    output_filename='sales_daily_orders.csv',
    output_subdir='ì˜ì—…ê´€ë¦¬ë¶€_DB',
    dedup_key='sub_order_id',
    mode='append',
    **context
):
    """Parquet ë°ì´í„°ë¥¼ ë¡œì»¬ DBì— CSVë¡œ ì €ì¥ (íƒ€ì… ìµœëŒ€í•œ ë³´ì¡´)"""
    import os
    import shutil
    import tempfile
    from pathlib import Path
    from modules.transform.utility.paths import LOCAL_DB
    
    ti = context['task_instance']
    
    # ============================================================
    # 1. ì…ë ¥ ë°ì´í„° ë¡œë“œ
    # ============================================================
    parquet_path = ti.xcom_pull(task_ids=input_task_id, key=input_xcom_key)
    if not parquet_path:
        print(f"[ì—ëŸ¬] ì…ë ¥ ë°ì´í„° ì—†ìŒ: {input_task_id}")
        return "ì €ì¥ ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ"
    
    # Parquet ì½ê¸° (íƒ€ì… ìœ ì§€)
    new_df = pd.read_parquet(parquet_path)
    print(f"\n{'='*60}")
    print(f"[ì…ë ¥] ìƒˆ ë°ì´í„°: {len(new_df):,}í–‰ Ã— {len(new_df.columns)}ì»¬ëŸ¼")
    
    # íƒ€ì… ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    print(f"[íƒ€ì…] ì…ë ¥ ë°ì´í„° íƒ€ì…:")
    for col in new_df.columns[:5]:  # ì²˜ìŒ 5ê°œë§Œ
        print(f"  - {col}: {new_df[col].dtype}")
    if len(new_df.columns) > 5:
        print(f"  ... (ì´ {len(new_df.columns)}ê°œ ì»¬ëŸ¼)")
    
    # ì¤‘ë³µ ì œê±° í‚¤ í™•ì¸
    if isinstance(dedup_key, str):
        dedup_cols = [dedup_key]
    else:
        dedup_cols = dedup_key
    
    for key in dedup_cols:
        if key not in new_df.columns:
            print(f"[ì—ëŸ¬] ì¤‘ë³µ ì œê±° í‚¤ ì—†ìŒ: {key}")
            return f"ì €ì¥ ì‹¤íŒ¨: {key} ì»¬ëŸ¼ ì—†ìŒ"
    
    print(f"[í™•ì¸] ì¤‘ë³µ ì œê±° í‚¤: {dedup_cols}")
    for key in dedup_cols:
        print(f"  - {key}: {new_df[key].nunique():,}ê°œ ê³ ìœ ê°’")
    
    # ============================================================
    # 2. ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    # ============================================================
    if output_csv_path:
        local_csv_path = Path(output_csv_path)
    else:
        output_dir = LOCAL_DB / output_subdir
        output_dir.mkdir(parents=True, exist_ok=True)
        local_csv_path = output_dir / output_filename
    
    local_csv_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"\n[ë¡œì»¬DB] ê²½ë¡œ: {local_csv_path}")
    
    # ============================================================
    # 3. ê¸°ì¡´ CSV ì½ê¸° (append ëª¨ë“œ)
    # ============================================================
    existing_df = None
    if mode == 'append' and local_csv_path.exists():
        try:
            if local_csv_path.stat().st_size == 0:
                print("[ì •ë³´] ê¸°ì¡´ CSVê°€ ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤.")
            else:
                print(f"[ê¸°ì¡´] CSV ì½ê¸° ì‹œë„: {local_csv_path.name}")
                
                # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
                encodings_to_try = ['utf-8-sig', 'utf-8', 'cp949', 'euc-kr']
                for encoding in encodings_to_try:
                    try:
                        # âš ï¸ CSV ì½ì„ ë•Œ íƒ€ì… ì¶”ë¡  (low_memory=False)
                        existing_df = pd.read_csv(
                            local_csv_path, 
                            encoding=encoding, 
                            low_memory=False  # íƒ€ì… ì¶”ë¡  í™œì„±í™”
                        )
                        print(f"[ê¸°ì¡´] âœ“ ì„±ê³µ: {len(existing_df):,}í–‰ ({encoding})")
                        
                        # ì¤‘ë³µ ì œê±° í‚¤ í™•ì¸
                        for key in dedup_cols:
                            if key in existing_df.columns:
                                print(f"[ê¸°ì¡´] {key}: {existing_df[key].nunique():,}ê°œ ê³ ìœ ê°’")
                        break
                    except (UnicodeDecodeError, LookupError):
                        continue
                    except Exception as e:
                        print(f"[ê¸°ì¡´]   {encoding}: {type(e).__name__}")
                        continue
                
                if existing_df is None:
                    print(f"[ê²½ê³ ] ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ - ê¸°ì¡´ íŒŒì¼ ë¬´ì‹œí•˜ê³  ì‹ ê·œë¡œ ì‹œì‘")
                    
        except Exception as e:
            print(f"[ê²½ê³ ] ê¸°ì¡´ CSV ì½ê¸° ì‹¤íŒ¨: {e}")
            existing_df = None
    
    # ============================================================
    # 4. ë°ì´í„° ë³‘í•©
    # ============================================================
    if existing_df is None or existing_df.empty:
        combined_df = new_df.copy()
        print(f"\n[ë³‘í•©] ì‹ ê·œ ìƒì„±: {len(combined_df):,}í–‰")
    else:
        # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ë³‘í•©
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        print(f"\n[ë³‘í•©] ê¸°ì¡´ {len(existing_df):,}í–‰ + ì‹ ê·œ {len(new_df):,}í–‰ = {len(combined_df):,}í–‰")
    
    # ============================================================
    # 5. ì¤‘ë³µ ì œê±°
    # ============================================================
    print(f"\n{'='*60}")
    print(f"[ì¤‘ë³µ ì œê±°] ì‹œì‘...")
    
    valid_cols = [c for c in dedup_cols if c in combined_df.columns]
    
    if valid_cols:
        before = len(combined_df)
        
        # 1ë‹¨ê³„: ê¸°ë³¸ í‚¤ ì¤‘ë³µ ì œê±° (keep='first' - ê¸°ì¡´ ë°ì´í„° ìš°ì„ , ì¤‘ë³µ ì—…ë¡œë“œ ë°©ì§€)
        combined_df.drop_duplicates(subset=valid_cols, keep='first', inplace=True)
        after_step1 = len(combined_df)
        print(f"[1ë‹¨ê³„] {valid_cols} ê¸°ì¤€: {before - after_step1:,}ê±´ ì œê±° â†’ {after_step1:,}í–‰")
        
        # 2ë‹¨ê³„: ì „ì²´ í–‰ ì¤‘ë³µ ì œê±°
        combined_df.drop_duplicates(inplace=True)
        after_step2 = len(combined_df)
        print(f"[2ë‹¨ê³„] ì „ì²´ í–‰ ê¸°ì¤€: {after_step1 - after_step2:,}ê±´ ì¶”ê°€ ì œê±° â†’ {after_step2:,}í–‰")
        
        total_removed = before - after_step2
        print(f"[ì¤‘ë³µ ì œê±°] ì´ ì œê±°: {total_removed:,}ê±´")
    else:
        print(f"[ê²½ê³ ] ì¤‘ë³µ ì œê±° í‚¤ ì—†ìŒ: {dedup_cols}")
    
    # ============================================================
    # 6. CSV ì €ì¥ (íƒ€ì… ìµœëŒ€í•œ ë³´ì¡´)
    # ============================================================
    print(f"\n{'='*60}")
    print(f"[ì €ì¥] CSV ì €ì¥ ì‹œì‘...")
    
    # datetime ì»¬ëŸ¼ í¬ë§· ëª…ì‹œ (íƒ€ì… ë³´ì¡´ ë…¸ë ¥)
    datetime_cols = combined_df.select_dtypes(include=['datetime64']).columns.tolist()
    if datetime_cols:
        print(f"[íƒ€ì…] datetime ì»¬ëŸ¼: {datetime_cols}")
        # datetimeì„ ISO 8601 í¬ë§·ìœ¼ë¡œ ì €ì¥ (YYYY-MM-DD HH:MM:SS)
        for col in datetime_cols:
            combined_df[col] = combined_df[col].dt.strftime('%Y-%m-%d %H:%M:%S')
            print(f"  - {col}: datetime â†’ ë¬¸ìì—´ (ISO 8601)")
    
    tmp_path = None
    try:
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(
            mode='w', delete=False, dir=str(local_csv_path.parent),
            prefix='tmp_', suffix='.csv', encoding='utf-8-sig'
        ) as tmp_file:
            tmp_path = tmp_file.name
        
        # CSV ì €ì¥
        combined_df.to_csv(tmp_path, index=False, encoding='utf-8-sig')
        
        # ê¸°ì¡´ íŒŒì¼ ë°±ì—… (ê°™ì€ ë””ë ‰í† ë¦¬)
        backup_path = None
        if local_csv_path.exists():
            backup_path = local_csv_path.parent / f"{local_csv_path.name}.bak"
            shutil.copy2(local_csv_path, backup_path)
        
        # ì„ì‹œ íŒŒì¼ì„ ì‹¤ì œ ê²½ë¡œë¡œ ì´ë™
        shutil.move(tmp_path, str(local_csv_path))
        
        csv_size = local_csv_path.stat().st_size / (1024 * 1024)
        print(f"[ì €ì¥] âœ… CSV ì €ì¥ ì™„ë£Œ: {len(combined_df):,}ê±´ ({csv_size:.2f} MB)")
        
        # ë°±ì—… íŒŒì¼ ì‚­ì œ
        if backup_path and backup_path.exists():
            backup_path.unlink()
        
        # ============================================================
        # 7. OneDrive ë°±ì—…
        # ============================================================
        try:
            from modules.transform.utility.paths import ONEDRIVE_DB
            from modules.load.backup_to_onedrive import backup_to_onedrive
            
            onedrive_csv_path = ONEDRIVE_DB / output_subdir / output_filename
            print(f"\n[ë°±ì—…] OneDrive ë°±ì—… ì‹œì‘...")
            print(f"[ë°±ì—…] ëŒ€ìƒ: {onedrive_csv_path}")
            
            backup_result = backup_to_onedrive(local_csv_path, onedrive_csv_path)
            
            if backup_result['success']:
                print(f"[ë°±ì—…] âœ… OneDrive ë°±ì—… ì™„ë£Œ")
            else:
                print(f"[ë°±ì—…] âš ï¸ OneDrive ë°±ì—… ì‹¤íŒ¨: {backup_result['message']}")
        except Exception as e:
            print(f"[ë°±ì—…] âš ï¸ OneDrive ë°±ì—… ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        
    except Exception as e:
        print(f"[ì—ëŸ¬] CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        if tmp_path and os.path.exists(tmp_path):
            os.remove(tmp_path)
        return f"ì €ì¥ ì‹¤íŒ¨: {e}"
    
    print(f"{'='*60}\n")
    return f"âœ… ì €ì¥ ì™„ë£Œ: {len(combined_df):,}ê±´"




# modules/load/load_df_glob.py ë˜ëŠ” modules/transform/utility/io.py

def cleanup_collected_csvs(
    patterns=None,  # ì‚­ì œí•  íŒŒì¼ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
    **context
):
    """OneDrive ìˆ˜ì§‘ ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ ì‚­ì œ"""
    from pathlib import Path
    from modules.transform.utility.paths import COLLECT_DB
    
    collect_dir = COLLECT_DB / 'ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘'
    
    if not collect_dir.exists():
        print(f"[ì •ë¦¬] ìˆ˜ì§‘ ë””ë ‰í† ë¦¬ ì—†ìŒ: {collect_dir}")
        return "ì‚­ì œí•  íŒŒì¼ ì—†ìŒ (ë””ë ‰í† ë¦¬ ì—†ìŒ)"
    
    # ì‚­ì œí•  íŒŒì¼ íŒ¨í„´ ì„¤ì • (ê¸°ë³¸ê°’: ëª¨ë“  CSV)
    if patterns is None:
        patterns = [
            'baemin_change_history_*.csv',
            'baemin_metrics_*.csv',
            'toorder_review_*.xlsx'
        ]
    
    total_deleted = 0
    total_size = 0
    failed_files = []
    
    print(f"\n{'='*60}")
    print(f"[ì •ë¦¬] OneDrive ìˆ˜ì§‘ í´ë” ì •ë¦¬ ì‹œì‘")
    print(f"[ê²½ë¡œ] {collect_dir}")
    print(f"{'='*60}")
    
    for pattern in patterns:
        files = list(collect_dir.glob(pattern))
        
        if not files:
            print(f"[íŒ¨í„´] {pattern}: íŒŒì¼ ì—†ìŒ")
            continue
        
        print(f"\n[íŒ¨í„´] {pattern}: {len(files)}ê°œ íŒŒì¼")
        
        for file_path in files:
            try:
                file_size = file_path.stat().st_size / (1024 * 1024)  # MB
                file_path.unlink()
                total_deleted += 1
                total_size += file_size
                print(f"  âœ“ ì‚­ì œ: {file_path.name} ({file_size:.2f} MB)")
            except Exception as e:
                failed_files.append((file_path.name, str(e)))
                print(f"  âœ— ì‹¤íŒ¨: {file_path.name} - {e}")
    
    print(f"\n{'='*60}")
    print(f"[ì™„ë£Œ] ì‚­ì œ ì™„ë£Œ: {total_deleted}ê°œ íŒŒì¼ ({total_size:.2f} MB)")
    
    if failed_files:
        print(f"[ê²½ê³ ] ì‹¤íŒ¨: {len(failed_files)}ê°œ")
        for fname, error in failed_files:
            print(f"  - {fname}: {error}")
    
    print(f"{'='*60}\n")
    
    return f"âœ… ì •ë¦¬ ì™„ë£Œ: {total_deleted}ê°œ íŒŒì¼ ì‚­ì œ"


def upload_final_csv(
    source_filename='sales_daily_orders_upload.csv',
    source_subdir='ì˜ì—…ê´€ë¦¬ë¶€_DB',
    dest_dir='E:/down/ì—…ë¡œë“œ_temp',
    **context
):
    """ìµœì¢… CSV íŒŒì¼ì„ ì—…ë¡œë“œ í´ë”ë¡œ ë³µì‚¬"""
    import shutil
    from pathlib import Path
    from modules.transform.utility.paths import LOCAL_DB
    
    print(f"\n{'='*60}")
    print(f"[ì—…ë¡œë“œ] ìµœì¢… íŒŒì¼ ë³µì‚¬ ì‹œì‘")
    print(f"{'='*60}")
    
    # 1. ì›ë³¸ íŒŒì¼ ê²½ë¡œ
    source_path = LOCAL_DB / source_subdir / source_filename
    
    if not source_path.exists():
        error_msg = f"ì›ë³¸ íŒŒì¼ ì—†ìŒ: {source_path}"
        print(f"[ì—ëŸ¬] {error_msg}")
        return f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {error_msg}"
    
    source_size = source_path.stat().st_size / (1024 * 1024)
    print(f"[ì›ë³¸] {source_path}")
    print(f"       í¬ê¸°: {source_size:.2f} MB")
    
    # 2. ëŒ€ìƒ ë””ë ‰í† ë¦¬ ìƒì„±
    dest_path = Path(dest_dir)
    dest_path.mkdir(parents=True, exist_ok=True)
    
    dest_file = dest_path / source_filename
    print(f"\n[ëŒ€ìƒ] {dest_file}")
    
    # ê¸°ì¡´ íŒŒì¼ í™•ì¸
    if dest_file.exists():
        old_size = dest_file.stat().st_size / (1024 * 1024)
        print(f"       ê¸°ì¡´ íŒŒì¼ ì¡´ì¬ ({old_size:.2f} MB) â†’ ë®ì–´ì“°ê¸° ì˜ˆì •")
    
    # 3. íŒŒì¼ ë³µì‚¬
    try:
        shutil.copy2(source_path, dest_file)
        print(f"\n[ë³µì‚¬] âœ… ë³µì‚¬ ì™„ë£Œ")
        
        # ë³µì‚¬ í™•ì¸
        if dest_file.exists():
            copied_size = dest_file.stat().st_size / (1024 * 1024)
            print(f"       ë³µì‚¬ëœ íŒŒì¼: {copied_size:.2f} MB")
            
            # íŒŒì¼ í¬ê¸° ê²€ì¦
            if abs(source_size - copied_size) < 0.01:  # 10KB ì´ë‚´ ì˜¤ì°¨
                print(f"       ê²€ì¦: âœ“ íŒŒì¼ í¬ê¸° ì¼ì¹˜")
            else:
                print(f"       ê²½ê³ : íŒŒì¼ í¬ê¸° ë¶ˆì¼ì¹˜ ({source_size:.2f} vs {copied_size:.2f} MB)")
        else:
            raise FileNotFoundError("ë³µì‚¬ í›„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
    except Exception as e:
        error_msg = f"íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {e}"
        print(f"[ì—ëŸ¬] {error_msg}")
        return f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {error_msg}"
    
    print(f"{'='*60}\n")
    return f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {dest_file}"





def text_to_html(text):
    """ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ ë³€í™˜"""
    text = text.replace('\n', '<br>')
    
    html = f"""<html>
<head>
<meta charset="UTF-8">
</head>
<body style="font-family: 'Malgun Gothic', Arial, sans-serif; margin: 20px; line-height: 1.6;">
<div style="background: #f8f9fa; padding: 20px; border-radius: 5px; border-left: 4px solid #27ae60;">
{text}
</div>
<p style="color: #999; font-size: 12px; margin-top: 20px;">
ì´ ë©”ì¼ì€ ìë™ìœ¼ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.
</p>
</body>
</html>"""
    
    return html


def send_email(
    subject,
    html_content,
    to_emails,
    conn_id='doridang_conn_smtp_gmail',
    **context
):
    """ì´ë©”ì¼ ë°œì†¡ (Airflow SMTP Connection ì‚¬ìš©)"""
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart
    from airflow.hooks.base import BaseHook
    
    connection = BaseHook.get_connection(conn_id)
    
    smtp_host = connection.host
    smtp_port = connection.port
    smtp_user = connection.login
    smtp_password = connection.password
    from_email = connection.extra_dejson.get('from_email') or smtp_user
    
    if isinstance(to_emails, str):
        to_list = [to_emails]
    else:
        to_list = to_emails
    
    msg = MIMEMultipart('alternative')
    msg['Subject'] = subject
    msg['From'] = from_email
    msg['To'] = ', '.join(to_list)
    
    html_part = MIMEText(html_content, 'html', 'utf-8')
    msg.attach(html_part)
    
    try:
        print(f"ğŸ“§ ì´ë©”ì¼ ë°œì†¡ ì‹œì‘...")
        print(f"   - ë°œì‹ : {from_email}")
        print(f"   - ìˆ˜ì‹ : {to_list}")
        print(f"   - ì œëª©: {subject}")
        
        with smtplib.SMTP(smtp_host, smtp_port) as server:
            server.starttls()
            server.login(smtp_user, smtp_password)
            server.send_message(msg)
        
        print(f"âœ… ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ: {len(to_list)}ëª…")
        return f"ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ: {len(to_list)}ëª…"
    
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
        raise


def cleanup_temp_parquets(**context):
    """temp ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  parquet íŒŒì¼ ì‚­ì œ (OneDrive temp + Local temp)"""
    import shutil
    from pathlib import Path
    
    temp_dirs = [TEMP_DIR]
    summary = []
    
    for temp_dir in temp_dirs:
        if not temp_dir.exists():
            print(f"[ì •ë¦¬] temp ë””ë ‰í† ë¦¬ ì—†ìŒ: {temp_dir}")
            continue
        
        parquet_files = list(temp_dir.glob('*.parquet'))
        
        if not parquet_files:
            print(f"[ì •ë¦¬] ì‚­ì œí•  parquet íŒŒì¼ ì—†ìŒ: {temp_dir}")
            continue
        
        total_size = sum(f.stat().st_size for f in parquet_files)
        total_size_mb = total_size / (1024 * 1024)
        
        print(f"[ì •ë¦¬] ëŒ€ìƒ: {temp_dir} -> {len(parquet_files)}ê°œ ({total_size_mb:.2f} MB)")
        
        deleted_count = 0
        failed_files = []
        
        for parquet_file in parquet_files:
            try:
                parquet_file.unlink()
                deleted_count += 1
                print(f"[ì •ë¦¬] ì‚­ì œ: {parquet_file.name}")
            except Exception as e:
                failed_files.append((parquet_file.name, str(e)))
                print(f"[ê²½ê³ ] ì‚­ì œ ì‹¤íŒ¨: {parquet_file.name} - {e}")
        
        result = f"{temp_dir}: ì‚­ì œ {deleted_count}ê°œ ({total_size_mb:.2f} MB)"
        if failed_files:
            result += f", ì‹¤íŒ¨ {len(failed_files)}ê°œ"
        summary.append(result)
    
    if not summary:
        return "ì‚­ì œí•  íŒŒì¼ ì—†ìŒ"
    return " | ".join(summary)


def cleanup_collected_csvs(**context):
    """ì˜ì—…íŒ€_ìˆ˜ì§‘ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  CSV íŒŒì¼ì„ ì—…ë¡œë“œ_tempë¡œ ì´ë™ (ì‚­ì œ X)"""
    import shutil
    from pathlib import Path
    
    collect_dir = COLLECT_DB / 'ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘'
    upload_temp_dir = Path('/opt/airflow/download/ì—…ë¡œë“œ_temp/sales_orders')  # Docker ê²½ë¡œ
    
    # Windows ê²½ë¡œë„ ì§€ì›
    if not upload_temp_dir.exists():
        upload_temp_dir = Path('C:/airflow/download/ì—…ë¡œë“œ_temp/sales_orders')
    
    if not collect_dir.exists():
        print(f"[ì •ë¦¬] ìˆ˜ì§‘ ë””ë ‰í† ë¦¬ ì—†ìŒ: {collect_dir}")
        return "ì´ë™í•  íŒŒì¼ ì—†ìŒ (ë””ë ‰í† ë¦¬ ì—†ìŒ)"
    
    csv_files = list(collect_dir.glob('*.csv'))
    
    if not csv_files:
        print(f"[ì •ë¦¬] ì´ë™í•  CSV íŒŒì¼ ì—†ìŒ")
        return "ì´ë™í•  íŒŒì¼ ì—†ìŒ"
    
    total_size = sum(f.stat().st_size for f in csv_files)
    total_size_mb = total_size / (1024 * 1024)
    
    print(f"[ì •ë¦¬] ì´ë™ ëŒ€ìƒ: {len(csv_files)}ê°œ CSV íŒŒì¼ ({total_size_mb:.2f} MB)")
    print(f"[ì •ë¦¬] ëª©ì ì§€: {upload_temp_dir}")
    
    # ëª©ì ì§€ ë””ë ‰í† ë¦¬ ìƒì„±
    upload_temp_dir.mkdir(parents=True, exist_ok=True)
    
    moved_count = 0
    failed_files = []
    
    for csv_file in csv_files:
        try:
            dest_file = upload_temp_dir / csv_file.name
            
            # ë™ì¼í•œ ì´ë¦„ì˜ íŒŒì¼ì´ ìˆìœ¼ë©´ ë°±ì—…
            if dest_file.exists():
                backup_file = upload_temp_dir / f"{csv_file.name}.bak"
                shutil.move(str(dest_file), str(backup_file))
                print(f"[ì •ë¦¬] ê¸°ì¡´ íŒŒì¼ ë°±ì—…: {backup_file.name}")
            
            shutil.move(str(csv_file), str(dest_file))
            moved_count += 1
            print(f"[ì •ë¦¬] ì´ë™ ì™„ë£Œ: {csv_file.name} â†’ {upload_temp_dir.name}/")
        except Exception as e:
            failed_files.append((csv_file.name, str(e)))
            print(f"[ê²½ê³ ] ì´ë™ ì‹¤íŒ¨: {csv_file.name} - {e}")
    
    result = f"CSV ì´ë™ ì™„ë£Œ: {moved_count}ê°œ íŒŒì¼ ({total_size_mb:.2f} MB) â†’ ì—…ë¡œë“œ_temp"
    
    if failed_files:
        result += f", ì‹¤íŒ¨: {len(failed_files)}ê°œ"
    
    return result


def csv_to_parquet_backup(
    csv_filename: str,
    onedrive_subfolder: str = "ì˜ì—…ê´€ë¦¬ë¶€_DB",
    **context
) -> str:
    """ë¡œì»¬DBì˜ CSV íŒŒì¼ì„ ì½ì–´ Parquetìœ¼ë¡œ ë³€í™˜ í›„ OneDriveì— ì €ì¥"""
    try:
        print("=" * 60)
        print("ğŸ”„ CSV â†’ Parquet ë³€í™˜ ë° ë°±ì—… ì‹œì‘")
        print("=" * 60)
        
        csv_path = LOCAL_DB / csv_filename
        print(f"ğŸ“‚ ë¡œì»¬DB ê²½ë¡œ: {csv_path}")
        
        if not csv_path.exists():
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {csv_path}")
            return f"ì‹¤íŒ¨: íŒŒì¼ ì—†ìŒ ({csv_filename})"
        
        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']
        df = None
        used_encoding = None
        
        for encoding in encodings:
            try:
                df = pd.read_csv(csv_path, encoding=encoding)
                used_encoding = encoding
                print(f"   âœ… {encoding}ìœ¼ë¡œ ì„±ê³µ ({len(df):,}í–‰)")
                break
            except (UnicodeDecodeError, LookupError):
                continue
        
        if df is None:
            return f"ì‹¤íŒ¨: CSV ì½ê¸° ì‹¤íŒ¨ ({csv_filename})"
        
        parquet_filename = csv_path.stem + ".parquet"
        
        onedrive_path = ONEDRIVE_DB / onedrive_subfolder
        onedrive_path.mkdir(parents=True, exist_ok=True)
        
        parquet_output_path = onedrive_path / parquet_filename
        
        df.to_parquet(
            parquet_output_path,
            index=False,
            engine='pyarrow',
            compression='snappy'
        )
        
        parquet_size = parquet_output_path.stat().st_size / (1024 * 1024)
        csv_size = csv_path.stat().st_size / (1024 * 1024)
        compression_ratio = (1 - parquet_size / csv_size) * 100 if csv_size > 0 else 0
        
        print(f"âœ… CSV â†’ Parquet ë³€í™˜ ì™„ë£Œ")
        print(f"   - CSV: {csv_size:.2f} MB â†’ Parquet: {parquet_size:.2f} MB ({compression_ratio:.1f}% ì••ì¶•)")
        
        return f"âœ… ì™„ë£Œ: {len(df):,}í–‰ ({parquet_filename})"
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return f"âŒ ì‹¤íŒ¨: {str(e)}"


# ============================================================
# sub_order_id ìƒì„± í•¨ìˆ˜ë“¤
# ============================================================

def create_sub_order_id_simple(
    df: pd.DataFrame,
    order_col: str = 'order_id',
    output_col: str = 'sub_order_id'
) -> pd.DataFrame:
    """
    ì£¼ë¬¸ë²ˆí˜¸ + ìˆœë²ˆ ë°©ì‹ìœ¼ë¡œ sub_order_id ìƒì„±
    
    ì˜ˆì‹œ: B22V00DE2A_1, B22V00DE2A_2, ...
    """
    df = df.copy()
    
    df[output_col] = (
        df[order_col].astype(str) + '_' + 
        (df.groupby(order_col).cumcount() + 1).astype(str)
    )
    
    print(f"[sub_order_id] ë‹¨ìˆœ ìˆœë²ˆ ë°©ì‹ìœ¼ë¡œ ìƒì„± ì™„ë£Œ")
    print(f"  - ì›ë³¸ ë°ì´í„°: {len(df):,}í–‰")
    print(f"  - ê³ ìœ  ID: {df[output_col].nunique():,}ê°œ")
    
    duplicates = df[df.duplicated(subset=output_col, keep=False)]
    if len(duplicates) > 0:
        print(f"  âš ï¸ ê²½ê³ : {len(duplicates)}ê°œ ì¤‘ë³µ ë°œê²¬!")
    else:
        print(f"  âœ… ì¤‘ë³µ ì—†ìŒ")
    
    return df


def create_sub_order_id_hash(
    df: pd.DataFrame,
    natural_key_cols: list,
    output_col: str = 'sub_order_id'
) -> pd.DataFrame:
    """
    í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ sub_order_id ìƒì„± (ë³µí•©í‚¤ í•„ìš” ì‹œ)
    
    ì˜ˆì‹œ: a1b2c3d4e5f6g7h8 (16ìë¦¬ í•´ì‹œ)
    """
    import hashlib
    
    df = df.copy()
    
    key_parts = [df[col].astype(str) for col in natural_key_cols]
    uk_series = pd.concat(key_parts, axis=1).agg('|'.join, axis=1)
    
    df[output_col] = uk_series.apply(
        lambda s: hashlib.sha1(s.encode('utf-8')).hexdigest()[:16]
    )
    
    print(f"[sub_order_id] í•´ì‹œ ë°©ì‹ìœ¼ë¡œ ìƒì„± ì™„ë£Œ")
    print(f"  - ì›ë³¸ ë°ì´í„°: {len(df):,}í–‰")
    print(f"  - ê³ ìœ  ID: {df[output_col].nunique():,}ê°œ")
    
    duplicates = df[df.duplicated(subset=output_col, keep=False)]
    if len(duplicates) > 0:
        print(f"  âš ï¸ ê²½ê³ : {len(duplicates)}ê°œ ì¤‘ë³µ ë°œê²¬!")
    else:
        print(f"  âœ… ì¤‘ë³µ ì—†ìŒ")
    
    return df