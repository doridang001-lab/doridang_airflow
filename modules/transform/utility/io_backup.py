import glob
import os
import pandas as pd
from pathlib import Path
from typing import List, Union, Optional
from modules.transform.utility.paths import ONEDRIVE_DB, COLLECT_DB, LOCAL_DB

# ============================================================
# CSV ë¡œë“œ í•¨ìˆ˜ë“¤
# ============================================================
def read_csv_glob(
    files: Union[str, List[str]],
    add_source_col: bool = False,
    source_col_name: str = "source_file",
    ignore_index: bool = True,
    on_error: str = "raise",  # 'raise' | 'skip'
    **read_csv_kwargs,
) -> pd.DataFrame:
    """
    Glob íŒ¨í„´ ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ì—¬ëŸ¬ CSVë¥¼ ì½ì–´ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters
    ----------
    files : str | list[str]
        - str: glob íŒ¨í„´ (ì˜ˆ: 'data/baemin/*.csv')
        - list[str]: CSV íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    add_source_col : bool, default False
        ê° í–‰ì˜ ì›ë³¸ íŒŒì¼ëª…ì„ ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€í• ì§€ ì—¬ë¶€.
    source_col_name : str, default 'source_file'
        ì›ë³¸ íŒŒì¼ëª… ì»¬ëŸ¼ëª….
    ignore_index : bool, default True
        concat ì‹œ ì¸ë±ìŠ¤ë¥¼ ë¬´ì‹œí•˜ê³  0..n-1ë¡œ ì¬ì§€ì •.
    on_error : {'raise', 'skip'}, default 'raise'
        íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ:
        - 'raise': ì¦‰ì‹œ ì˜ˆì™¸ ë°œìƒ
        - 'skip': ë¬¸ì œ íŒŒì¼ì„ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰(ê²½ê³  ì¶œë ¥)
    **read_csv_kwargs :
        `pd.read_csv`ì— ê·¸ëŒ€ë¡œ ì „ë‹¬ë  ì¸ìë“¤.

    Returns
    -------
    pd.DataFrame
        ëª¨ë“  CSVë¥¼ ì„¸ë¡œ ë°©í–¥ìœ¼ë¡œ ê²°í•©í•œ ë°ì´í„°í”„ë ˆì„.
    """
    if isinstance(files, str):
        file_list = sorted(glob.glob(files))
    elif isinstance(files, list):
        file_list = files
    else:
        raise TypeError("filesëŠ” glob íŒ¨í„´(str) ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸(list[str])ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    if not file_list:
        raise FileNotFoundError("ë§¤ì¹­ë˜ëŠ” CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    dfs: List[pd.DataFrame] = []
    for fpath in file_list:
        try:
            df = pd.read_csv(fpath, **read_csv_kwargs)
            if add_source_col:
                df[source_col_name] = os.path.basename(fpath)
            dfs.append(df)
        except Exception as e:
            if on_error == "skip":
                print(f"[ê²½ê³ ] íŒŒì¼ '{fpath}'ì„(ë¥¼) ê±´ë„ˆëœë‹ˆë‹¤. ì´ìœ : {e}")
                continue
            raise

    if not dfs:
        return pd.DataFrame()

    return pd.concat(dfs, ignore_index=ignore_index)



def load_data(
    file_path,
    xcom_key='parquet_path',
    use_glob=True,
    dedup_key=None,  # ì¤‘ë³µ ì œê±° í‚¤ ì¶”ê°€
    **context
):
    """CSV ë°ì´í„° ë²”ìš© ë¡œë“œ í•¨ìˆ˜
    
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ ë˜ëŠ” glob íŒ¨í„´
        xcom_key: XComì— ì €ì¥í•  í‚¤ ì´ë¦„
        use_glob: Trueë©´ glob íŒ¨í„´ìœ¼ë¡œ ì²˜ë¦¬, Falseë©´ ë‹¨ì¼ íŒŒì¼
        dedup_key: ì¤‘ë³µ ì œê±° ê¸°ì¤€ ì»¬ëŸ¼ (str ë˜ëŠ” list, Noneì´ë©´ ì¤‘ë³µ ì œê±° ì•ˆí•¨)
                  ì˜ˆ: 'order_id' ë˜ëŠ” ['order_id', 'store_id']
        
    Returns:
        str: ì²˜ë¦¬ ê²°ê³¼ ë©”ì‹œì§€
        
    XCom:
        {xcom_key}: parquet íŒŒì¼ ê²½ë¡œ
    """
    import glob as glob_module
    from pathlib import Path
    ti = context['task_instance']
    
    temp_dir = ONEDRIVE_DB / 'temp'
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    parquet_path = temp_dir / f"{xcom_key}_{context['ds_nodash']}.parquet"
    
    try:
        # 1. íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        if use_glob:
            file_list = sorted(glob_module.glob(str(file_path)))
            print(f"[DEBUG] glob íŒ¨í„´: {file_path}")
            print(f"[DEBUG] ì°¾ì€ íŒŒì¼: {len(file_list)}ê°œ")
        else:
            file_list = [str(file_path)] if Path(file_path).exists() else []
            print(f"[DEBUG] ë‹¨ì¼ íŒŒì¼: {file_path}")
        
        if not file_list:
            print(f"[ê²½ê³ ] íŒŒì¼ ì—†ìŒ: {file_path}")
            ti.xcom_push(key=xcom_key, value=None)
            return f"0ê±´ (íŒŒì¼ ì—†ìŒ)"
        
        # íŒŒì¼ëª…ì— (1), (2) ë“±ì´ ìˆì–´ë„ ëª¨ë‘ ë‹¤ë¥¸ ìˆ˜ì§‘ ë°ì´í„°ì´ë¯€ë¡œ ëª¨ë‘ ë¡œë“œ
        print(f"[INFO] ë¡œë“œí•  íŒŒì¼: {len(file_list)}ê°œ")
        for fpath in file_list:
            print(f"   âœ“ {Path(fpath).name}")
        
        # 2. ì¸ì½”ë”© ìë™ ê°ì§€í•˜ë©° CSV ì½ê¸°
        dfs = []
        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']
        
        for fpath in file_list:
            df = None
            for encoding in encodings:
                try:
                    print(f"   ì‹œë„: {Path(fpath).name} ({encoding})")
                    df = pd.read_csv(fpath, encoding=encoding)
                    print(f"   âœ“ {encoding}ìœ¼ë¡œ ì½ìŒ: {len(df)}í–‰")
                    dfs.append(df)
                    break
                except (UnicodeDecodeError, LookupError):
                    continue
            
            if df is None:
                print(f"   âœ— ì½ê¸° ì‹¤íŒ¨: {fpath} (ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨)")
                continue
        
        if not dfs:
            print(f"[ê²½ê³ ] ì½ì„ ìˆ˜ ìˆëŠ” íŒŒì¼ ì—†ìŒ")
            ti.xcom_push(key=xcom_key, value=None)
            return f"0ê±´ (íŒŒì¼ ì½ê¸° ì‹¤íŒ¨)"
        
        # 3. ëª¨ë“  íŒŒì¼ ë³‘í•©
        result_df = pd.concat(dfs, ignore_index=True) if len(dfs) > 1 else dfs[0]
        print(f"\në³‘í•© ì™„ë£Œ: {len(result_df):,}í–‰")
        
        # 4. ê°œë³„ í–‰ ì¤‘ë³µ ì œê±° (ì„ íƒì )
        # ë™ì¼ íŒŒì¼ ì¬ìˆ˜ì§‘ ë°©ì§€ë¥¼ ìœ„í•œ ìš©ë„ì´ë¯€ë¡œ, íŒŒì¼ì´ 2ê°œ ì´ìƒì¼ ë•Œë§Œ ì ìš©
        if dedup_key and len(file_list) > 1:
            before = len(result_df)

            if isinstance(dedup_key, str):
                dedup_cols = [dedup_key]
            else:
                dedup_cols = list(dedup_key)

            valid_cols = [col for col in dedup_cols if col in result_df.columns]

            if valid_cols:
                result_df.drop_duplicates(subset=valid_cols, keep='first', inplace=True)
                after = len(result_df)
                removed = before - after
                if removed > 0:
                    print(f"ì¤‘ë³µ ì œê±°: {removed:,}ê±´ ì œê±° (ê¸°ì¤€: {valid_cols})")
            else:
                print(f"[ê²½ê³ ] ì¤‘ë³µ ì œê±° ì»¬ëŸ¼ ì—†ìŒ: {dedup_cols}")
        elif dedup_key:
            print("[ì •ë³´] ë‹¨ì¼ íŒŒì¼ì´ë¯€ë¡œ í–‰ ì¤‘ë³µ ì œê±° ìƒëµ")
        
        # 5. Parquetë¡œ ì €ì¥ (ë°ì´í„° íƒ€ì… ë³´ì¡´)
        # í˜¼í•© íƒ€ì… ì»¬ëŸ¼ ì•ˆì „ ì²˜ë¦¬ (ìº í˜ì¸ID ë“±)
        for col in result_df.columns:
            if result_df[col].dtype == 'object':
                # ìˆ«ìì²˜ëŸ¼ ë³´ì´ëŠ” object ì»¬ëŸ¼ì€ ë¬¸ìì—´ë¡œ ëª…ì‹œì  ë³€í™˜
                if col in ['ìº í˜ì¸ID', 'ad_id', 'store_id']:
                    result_df[col] = result_df[col].astype(str)
        
        result_df.to_parquet(parquet_path, index=False, engine='pyarrow')
        print(f"\nâœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(result_df):,}ê±´")
        print(f"   ì €ì¥ ê²½ë¡œ: {parquet_path}")
        print(f"   ì»¬ëŸ¼: {list(result_df.columns)}")
        
        # 6. XComì— ê²½ë¡œ ì €ì¥
        ti.xcom_push(key=xcom_key, value=str(parquet_path))
        return f"{len(result_df):,}ê±´"
        
    except Exception as e:
        print(f"[ì—ëŸ¬] ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(traceback.format_exc())
        ti.xcom_push(key=xcom_key, value=None)
        return f"0ê±´ (ì—ëŸ¬)"
    
    


# ============================================================
# ì „ì²˜ë¦¬ ì–‘ì‹
# ============================================================
def load_baemin_data(**context):
    """ë°°ë¯¼ ì£¼ë¬¸ ë°ì´í„° ë¡œë“œ (wrapper)"""
    baemin_dir = COLLECT_DB / 'ì˜ì—…íŒ€'
    file_pattern = f"{baemin_dir}/baemin_orders*.csv"
    
    return load_data(
        file_path=file_pattern,
        xcom_key='baemin_parquet_path',
        use_glob=True,  # glob íŒ¨í„´ ì‚¬ìš©
        **context
    )



# ============================================================
# ì „ì²˜ë¦¬ ì–‘ì‹
# ============================================================

def preprocess_df(
    input_task_id,      # íŒŒë¼ë¯¸í„°ë¡œ ë°›ê¸°
    input_xcom_key,     # íŒŒë¼ë¯¸í„°ë¡œ ë°›ê¸°
    output_xcom_key,    # íŒŒë¼ë¯¸í„°ë¡œ ë°›ê¸°
    **context
):
    """ë²”ìš© ì „ì²˜ë¦¬ í•¨ìˆ˜ - ë°°ë¯¼/ì¿ íŒ¡ ê³µí†µ"""
    import numpy as np
    ti = context['task_instance']
    
    # ============================================================
    # 1ï¸.ì…ë ¥: XComì—ì„œ Parquet ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    # ============================================================
    parquet_path = ti.xcom_pull(
        task_ids=input_task_id,      # â† íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
        key=input_xcom_key            # â† íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
    )
    
    
    # ============================================================
    # 2ï¸. ì½ê¸°
    # ============================================================
    df = pd.read_parquet(parquet_path)
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(df):,}í–‰")
    
    # ============================================================
    # 3ï¸. ì „ì²˜ë¦¬ ë¡œì§
    # ============================================================

    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df):,}í–‰")
    
    # ============================================================
    # 4ï¸. ì €ì¥
    # ============================================================
    temp_dir = ONEDRIVE_DB / 'temp'
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    processed_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    df.to_parquet(processed_path, index=False, engine='pyarrow')
    
    # ============================================================
    # 5ï¸. ì¶œë ¥
    # ============================================================
    ti.xcom_push(key=output_xcom_key, value=str(processed_path))
    
    return f"ì „ì²˜ë¦¬: {len(df):,}í–‰"


# ë°ì´í„° ë³‘í•©
def concat_data(input_tasks, output_xcom_key='merged_path', **context):
    """
    ì—¬ëŸ¬ ì „ì²˜ë¦¬ ë°ì´í„° ë³‘í•©
    
    ì‚¬ìš©ë²•:
        # dict ë°©ì‹
        concat_data(
            input_tasks={'task1': 'key1', 'task2': 'key2'},
            output_xcom_key='merged_path'
        )
        
        # list ë°©ì‹ (ê°™ì€ key)
        concat_data(
            input_tasks=['task1', 'task2'],
            output_xcom_key='merged_path'
        )
    """
    ti = context['task_instance']
    
    # listë©´ ê¸°ë³¸ key ì‚¬ìš©
    if isinstance(input_tasks, list):
        input_tasks = {t: 'processed_path' for t in input_tasks}
    
    # ê° taskì—ì„œ ë°ì´í„° ë¡œë“œ
    dfs = []
    for task_id, xcom_key in input_tasks.items():
        path = ti.xcom_pull(task_ids=task_id, key=xcom_key)
        if path:
            try:
                df = pd.read_parquet(path)
                if len(df) > 0:
                    # âœ… Platform ê²€ì¦
                    if 'platform' in df.columns:
                        platforms = df['platform'].value_counts()
                        print(f"[{task_id}] Platform ë¶„í¬: {platforms.to_dict()}")
                    else:
                        print(f"âš ï¸ [{task_id}] platform ì»¬ëŸ¼ ì—†ìŒ!")
                    
                    dfs.append(df)
                    print(f"[{task_id}] {len(df):,}í–‰ ë¡œë“œ")
            except Exception as e:
                print(f"[{task_id}] ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ë³‘í•©
    if not dfs:
        print("ë³‘í•©í•  ë°ì´í„° ì—†ìŒ")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "ë³‘í•© ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ"
    
    merged_df = pd.concat(dfs, ignore_index=True)
    
    # âœ… ë³‘í•© í›„ Platform ê²€ì¦
    if 'platform' in merged_df.columns:
        platforms_after = merged_df['platform'].value_counts()
        print(f"ë³‘í•© ì™„ë£Œ: {len(merged_df):,}í–‰ - Platform ë¶„í¬: {platforms_after.to_dict()}")
        
        # ë°°ë¯¼/ì¿ íŒ¡ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
        if len(platforms_after) < 2:
            print(f"âš ï¸ [ê²½ê³ ] Platformì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            print(f"   ê¸°ëŒ€: ë°°ë¯¼ + ì¿ íŒ¡, ì‹¤ì œ: {list(platforms_after.index)}")
    else:
        print(f"ë³‘í•© ì™„ë£Œ: {len(merged_df):,}í–‰")
    
    # ì €ì¥
    temp_dir = ONEDRIVE_DB / 'temp'
    temp_dir.mkdir(exist_ok=True, parents=True)
    output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    merged_df.to_parquet(output_path, index=False)
    
    ti.xcom_push(key=output_xcom_key, value=str(output_path))
    return f"ë³‘í•© ì™„ë£Œ: {len(merged_df):,}í–‰"





'''
join_orders_with_stores = PythonOperator(
    task_id='join_orders_stores',
    python_callable=join_data,
    op_kwargs={
        'left_task': {
            'task_id': 'merge_orders',
            'xcom_key': 'merged_orders_path'
        },
        'right_task': {
            'task_id': 'load_store_info',
            'xcom_key': 'store_info_path'
        },
        'on': 'store_id',  # ë‹¨ì¼ í‚¤
        'how': 'left',
        'output_xcom_key': 'orders_with_stores',
    },
    dag=dag,
)

'''

def join_task(
    left_task,          # ì™¼ìª½ í…Œì´ë¸” task ì •ë³´
    right_task,         # ì˜¤ë¥¸ìª½ í…Œì´ë¸” task ì •ë³´
    on=None,            # join í‚¤ (ì–‘ìª½ ë™ì¼í•  ë•Œ)
    left_on=None,       # ì™¼ìª½ í…Œì´ë¸” join í‚¤
    right_on=None,      # ì˜¤ë¥¸ìª½ í…Œì´ë¸” join í‚¤
    how='left',         # join íƒ€ì…: 'left', 'right', 'inner', 'outer'
    output_xcom_key='joined_path',
    **context
):
    """
    ë‘ taskì˜ ë°ì´í„°ë¥¼ join
    
    ì‚¬ìš©ë²•:
        # ë°©ë²• 1: ì–‘ìª½ í‚¤ê°€ ê°™ì„ ë•Œ
        join_data(
            left_task={'task_id': 'preprocess_baemin', 'xcom_key': 'baemin_path'},
            right_task={'task_id': 'load_store', 'xcom_key': 'store_path'},
            on='store_id',
            how='left'
        )
        
        # ë°©ë²• 2: ì–‘ìª½ í‚¤ê°€ ë‹¤ë¥¼ ë•Œ
        join_data(
            left_task='merge_orders',
            right_task='preprocess_employee',
            left_on='store_name',
            right_on='store_names',
            how='left'
        )
    
    Args:
        left_task: dict {'task_id': ..., 'xcom_key': ...} ë˜ëŠ” str (task_id)
        right_task: dict {'task_id': ..., 'xcom_key': ...} ë˜ëŠ” str (task_id)
        on: join í‚¤ (ì–‘ìª½ ë™ì¼í•  ë•Œ, str ë˜ëŠ” list)
        left_on: ì™¼ìª½ í…Œì´ë¸” join í‚¤ (onê³¼ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€)
        right_on: ì˜¤ë¥¸ìª½ í…Œì´ë¸” join í‚¤ (onê³¼ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€)
        how: 'left', 'right', 'inner', 'outer'
        output_xcom_key: ê²°ê³¼ ì €ì¥ XCom í‚¤
    """
    ti = context['task_instance']
    
    # task ì •ë³´ íŒŒì‹±
    if isinstance(left_task, str):
        left_task = {'task_id': left_task, 'xcom_key': 'processed_path'}
    if isinstance(right_task, str):
        right_task = {'task_id': right_task, 'xcom_key': 'processed_path'}
    
    # ì™¼ìª½ ë°ì´í„° ë¡œë“œ
    left_path = ti.xcom_pull(
        task_ids=left_task['task_id'],
        key=left_task['xcom_key']
    )
    if not left_path:
        print(f"[ì—ëŸ¬] ì™¼ìª½ ë°ì´í„° ì—†ìŒ: {left_task['task_id']}")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "join ì‹¤íŒ¨: ì™¼ìª½ ë°ì´í„° ì—†ìŒ"
    
    left_df = pd.read_parquet(left_path)
    print(f"[ì™¼ìª½] {left_task['task_id']}: {len(left_df):,}í–‰")
    
    # ì˜¤ë¥¸ìª½ ë°ì´í„° ë¡œë“œ
    right_path = ti.xcom_pull(
        task_ids=right_task['task_id'],
        key=right_task['xcom_key']
    )
    if not right_path:
        print(f"[ì—ëŸ¬] ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ: {right_task['task_id']}")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "join ì‹¤íŒ¨: ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ"
    
    right_df = pd.read_parquet(right_path)
    print(f"[ì˜¤ë¥¸ìª½] {right_task['task_id']}: {len(right_df):,}í–‰")
    
    # join ì‹¤í–‰
    if on is not None:
        # ì–‘ìª½ í‚¤ê°€ ê°™ì€ ê²½ìš°
        print(f"\njoin ì‹¤í–‰: how={how}, on={on}")
        joined_df = left_df.merge(right_df, on=on, how=how)
    elif left_on is not None and right_on is not None:
        # ì–‘ìª½ í‚¤ê°€ ë‹¤ë¥¸ ê²½ìš°
        print(f"\njoin ì‹¤í–‰: how={how}, left_on={left_on}, right_on={right_on}")
        joined_df = left_df.merge(right_df, left_on=left_on, right_on=right_on, how=how)
    else:
        raise ValueError("on ë˜ëŠ” (left_on, right_on)ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    print(f"join ì™„ë£Œ: {len(joined_df):,}í–‰")
    
    # ì €ì¥
    temp_dir = ONEDRIVE_DB / 'temp'
    temp_dir.mkdir(exist_ok=True, parents=True)
    output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    joined_df.to_parquet(output_path, index=False)
    
    ti.xcom_push(key=output_xcom_key, value=str(output_path))
    return f"join ì™„ë£Œ: {len(joined_df):,}í–‰"





# csvì— ì €ì¥
"""
Parquetì„ CSVë¡œ ë³€í™˜ ë° ì €ì¥ (ì¤‘ë³µ ì œê±° í¬í•¨)
save_csv_task1 = PythonOperator(
    
    task_id='save_to_csv_orders',
    python_callable=save_to_csv,
    op_kwargs={
        'input_task_id': 'preprocess_join_orders_with_stores',
        'input_xcom_key': 'orders_with_stores_processed',
        'output_csv_path': '/opt/airflow/Doridang/ì˜ì—…íŒ€_DB/sales_daily_orders.csv',
        'dedup_key': 'order_id',
        'mode': 'append'
    },
    dag=dag,
)

"""


def save_to_csv(
    input_task_id,
    input_xcom_key,
    output_csv_path=None,
    output_filename='sales_daily_orders.csv',
    output_subdir='ì˜ì—…ê´€ë¦¬ë¶€_DB',
    dedup_key='sub_order_id',  # â­ sub_order_id ê¸°ì¤€
    mode='append',
    **context
):
    """
    ë¡œì»¬ DBì— CSV ì €ì¥ í›„ OneDriveì— Parquetìœ¼ë¡œ ë°±ì—…
    
    ì¤‘ë³µ ì œê±°:
    - ê¸°ì¤€: sub_order_id (ê³ ìœ  ID)
    - ì‹œì : ê¸°ì¡´ CSV + ìƒˆ ë°ì´í„° ë³‘í•© í›„
    - keep='last': ìµœì‹  ë°ì´í„° ìš°ì„ 
    """
    import os
    import shutil
    import tempfile
    from pathlib import Path
    from modules.transform.utility.paths import LOCAL_DB, ONEDRIVE_DB
    
    ti = context['task_instance']
    
    # 1. Parquet ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    parquet_path = ti.xcom_pull(task_ids=input_task_id, key=input_xcom_key)
    if not parquet_path:
        print(f"[ì—ëŸ¬] ì…ë ¥ ë°ì´í„° ì—†ìŒ: {input_task_id}")
        return "ì €ì¥ ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ"
    
    # 2. ìƒˆ ë°ì´í„° ì½ê¸°
    new_df = pd.read_parquet(parquet_path)
    print(f"\n{'='*60}")
    print(f"[ì…ë ¥] ìƒˆ ë°ì´í„°: {len(new_df):,}í–‰")
    
    # â­ sub_order_id ì¡´ì¬ í™•ì¸
    if 'sub_order_id' not in new_df.columns:
        print(f"[ì—ëŸ¬] sub_order_id ì»¬ëŸ¼ ì—†ìŒ!")
        print(f"[ì—ëŸ¬] ì‹¤ì œ ì»¬ëŸ¼: {list(new_df.columns)}")
        return "ì €ì¥ ì‹¤íŒ¨: sub_order_id ì—†ìŒ"
    else:
        print(f"[í™•ì¸] sub_order_id ì¡´ì¬: {new_df['sub_order_id'].nunique():,}ê°œ ê³ ìœ  ID")
    
    # Platform ê²€ì¦
    if 'platform' in new_df.columns:
        platform_counts = new_df['platform'].value_counts()
        print(f"[í™•ì¸] Platform ë¶„í¬: {platform_counts.to_dict()}")
    
    # 3. ë¡œì»¬ DB ê²½ë¡œ ì„¤ì •
    if output_csv_path:
        local_csv_path = Path(output_csv_path)
    else:
        output_dir = LOCAL_DB / output_subdir
        output_dir.mkdir(parents=True, exist_ok=True)
        local_csv_path = output_dir / output_filename
    
    local_csv_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"[ë¡œì»¬DB] ê²½ë¡œ: {local_csv_path}")
    
    # 4. ê¸°ì¡´ CSV ì½ê¸°
    existing_df = None
    if mode == 'append' and local_csv_path.exists():
        try:
            if local_csv_path.stat().st_size == 0:
                print("[ì •ë³´] ê¸°ì¡´ CSVê°€ ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤.")
            else:
                for encoding in ['utf-8-sig', 'utf-8', 'cp949']:
                    try:
                        existing_df = pd.read_csv(local_csv_path, encoding=encoding)
                        print(f"[ê¸°ì¡´] ë°ì´í„°: {len(existing_df):,}í–‰ ({encoding})")
                        
                        # â­ ê¸°ì¡´ ë°ì´í„°ì— sub_order_id ì—†ìœ¼ë©´ ê²½ê³ 
                        if 'sub_order_id' not in existing_df.columns:
                            print(f"[ê²½ê³ ] ê¸°ì¡´ CSVì— sub_order_id ì—†ìŒ!")
                            print(f"[ê²½ê³ ] ê¸°ì¡´ ë°ì´í„°ëŠ” ì¤‘ë³µ ì œê±° ë¶ˆê°€ëŠ¥")
                        else:
                            print(f"[í™•ì¸] ê¸°ì¡´ sub_order_id: {existing_df['sub_order_id'].nunique():,}ê°œ")
                        break
                    except UnicodeDecodeError:
                        continue
        except Exception as e:
            print(f"[ê²½ê³ ] ê¸°ì¡´ CSV ì½ê¸° ì‹¤íŒ¨: {e}")
            existing_df = None
    
    # 5. ë³‘í•©
    if existing_df is None or existing_df.empty:
        combined_df = new_df.copy()
        print(f"[ë³‘í•©] ì‹ ê·œ ìƒì„±: {len(combined_df):,}í–‰")
    else:
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        print(f"[ë³‘í•©] ë³‘í•© í›„: {len(combined_df):,}í–‰")
    
    # 6. â­ sub_order_id ê¸°ì¤€ ì¤‘ë³µ ì œê±° (ìµœì¢… ë‹¨ê³„)
    print(f"\n{'='*60}")
    print(f"[ì¤‘ë³µ ì œê±°] sub_order_id ê¸°ì¤€ ì¤‘ë³µ ì œê±° ì‹œì‘...")
    
    dedup_cols = [dedup_key] if isinstance(dedup_key, str) else dedup_key
    valid_cols = [c for c in dedup_cols if c in combined_df.columns]
    
    if valid_cols:
        before = len(combined_df)
        
        # ì¤‘ë³µ ì²´í¬ (ì‚­ì œ ì „)
        dup_check = combined_df[combined_df.duplicated(subset=valid_cols, keep=False)]
        if len(dup_check) > 0:
            print(f"[ì¤‘ë³µ ì œê±°] ë°œê²¬ëœ ì¤‘ë³µ: {len(dup_check):,}í–‰")
            print(f"[ì¤‘ë³µ ì œê±°] ì¤‘ë³µ ì˜ˆì‹œ (ìƒìœ„ 5ê°œ):")
            for idx, row in dup_check.head(5).iterrows():
                print(f"  - {row.get('order_id', 'N/A')} / {row.get('sub_order_id', 'N/A')}")
        else:
            print(f"[ì¤‘ë³µ ì œê±°] ì¤‘ë³µ ì—†ìŒ")
        
        # ì¤‘ë³µ ì œê±° ì‹¤í–‰
        combined_df.drop_duplicates(subset=valid_cols, keep='last', inplace=True)
        after = len(combined_df)
        removed = before - after
        
        print(f"[ì¤‘ë³µ ì œê±°] ì™„ë£Œ: {removed:,}ê±´ ì œê±° (ê¸°ì¤€: {valid_cols})")
        print(f"[ì¤‘ë³µ ì œê±°] ìµœì¢… ë°ì´í„°: {after:,}í–‰")
    else:
        print(f"[ê²½ê³ ] ì¤‘ë³µ ì œê±° í‚¤ ì—†ìŒ: {dedup_cols}")
    
    # 7. ë¡œì»¬ DBì— CSV ì €ì¥ (ì›ìì  ì“°ê¸°)
    print(f"\n{'='*60}")
    print(f"[ì €ì¥] CSV ì €ì¥ ì‹œì‘...")
    
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(
            mode='w',
            delete=False,
            dir=str(local_csv_path.parent),
            prefix='tmp_',
            suffix='.csv',
            encoding='utf-8-sig'
        ) as tmp_file:
            tmp_path = tmp_file.name
        
        combined_df.to_csv(tmp_path, index=False, encoding='utf-8-sig')
        
        backup_path = None
        if local_csv_path.exists():
            backup_path = local_csv_path.parent / f"{local_csv_path.name}.bak"
            shutil.copy2(local_csv_path, backup_path)
        
        shutil.move(tmp_path, str(local_csv_path))
        print(f"[ì €ì¥] âœ… ë¡œì»¬ CSV ì €ì¥ ì™„ë£Œ: {len(combined_df):,}ê±´")
        
        if backup_path and backup_path.exists():
            backup_path.unlink()
        
    except Exception as e:
        print(f"[ì—ëŸ¬] CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        if tmp_path and os.path.exists(tmp_path):
            os.remove(tmp_path)
        return f"ì €ì¥ ì‹¤íŒ¨: {e}"
    
    # 8. OneDriveì— Parquetìœ¼ë¡œ ë°±ì—…
    print(f"\n{'='*60}")
    print(f"[ë°±ì—…] OneDrive Parquet ë°±ì—… ì‹œì‘...")
    
    try:
        parquet_filename = local_csv_path.stem + '.parquet'
        onedrive_parquet_path = ONEDRIVE_DB / output_subdir / parquet_filename
        onedrive_parquet_path.parent.mkdir(parents=True, exist_ok=True)
        
        combined_df.to_parquet(
            onedrive_parquet_path,
            index=False,
            engine='pyarrow',
            compression='snappy'
        )
        
        csv_size = local_csv_path.stat().st_size / (1024 * 1024)
        parquet_size = onedrive_parquet_path.stat().st_size / (1024 * 1024)
        compression_ratio = (1 - parquet_size / csv_size) * 100 if csv_size > 0 else 0
        
        print(f"[ë°±ì—…] âœ… OneDrive Parquet ë°±ì—… ì™„ë£Œ")
        print(f"  - ê²½ë¡œ: {onedrive_parquet_path}")
        print(f"  - CSV í¬ê¸°: {csv_size:.2f} MB")
        print(f"  - Parquet í¬ê¸°: {parquet_size:.2f} MB")
        print(f"  - ì••ì¶•ë¥ : {compression_ratio:.1f}%")
        
    except Exception as e:
        print(f"[ë°±ì—…] âš ï¸ OneDrive Parquet ë°±ì—… ì‹¤íŒ¨: {e}")
    
    print(f"{'='*60}\n")
    return f"âœ… ì €ì¥ ì™„ë£Œ: {len(combined_df):,}ê±´ (ë¡œì»¬ CSV + OneDrive Parquet)"



def text_to_html(text):
    """
    ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ ë³€í™˜
    - ì¤„ë°”ê¿ˆ â†’ <br>
    - ìë™ ìŠ¤íƒ€ì¼ ì ìš©
    """
    # ì¤„ë°”ê¿ˆì„ <br>ë¡œ ë³€í™˜
    text = text.replace('\n', '<br>')
    
    # ê¸°ë³¸ HTML í…œí”Œë¦¿
    html = f"""<html>
<head>
<meta charset="UTF-8">
</head>
<body style="font-family: 'Malgun Gothic', Arial, sans-serif; margin: 20px; line-height: 1.6;">
<div style="background: #f8f9fa; padding: 20px; border-radius: 5px; border-left: 4px solid #27ae60;">
{text}
</div>
<p style="color: #999; font-size: 12px; margin-top: 20px;">
ì´ ë©”ì¼ì€ ìë™ìœ¼ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.
</p>
</body>
</html>"""
    
    return html


'''
# dag ì‚¬ìš©ë²•

def send_completion_email(**context):
    """ì²˜ë¦¬ ì™„ë£Œ ì•Œë¦¼ ì´ë©”ì¼"""
    
    # ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì‘ì„±
    message = f"""
#ì¼ì¼ ì£¼ë¬¸ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ

ì²˜ë¦¬ ì¼ì‹œ: {context['ds']}
DAG: {context['dag'].dag_id}
ì‹¤í–‰ ì‹œê°„: {context['ts']}
ìƒíƒœ: ì •ìƒ ì™„ë£Œ
"""
    
    return send_email(
        subject=f'[ë„ë¦¬ë‹¹] ì¼ì¼ ì£¼ë¬¸ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ ({context["ds"]})',
        html_content=text_to_html(message),
        to_emails=['a17019@kakao.com'],
        conn_id='conn_smtp_gmail',
        **context
    )
'''


# ì´ë©”ì¼ ë°œì†¡ (Airflow Connection ì‚¬ìš©)
def send_email(
    subject,
    html_content,
    to_emails,
    conn_id='conn_smtp_gmail',
    **context
):
    """
    ì´ë©”ì¼ ë°œì†¡ (Airflow SMTP Connection ì‚¬ìš©)
    
    ì‚¬ìš©ë²•:
        send_email(
            subject='[ë„ë¦¬ë‹¹] ê°€ë§¹ì  ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸',
            html_content='<h1>ì™„ë£Œ</h1><p>ì²˜ë¦¬ ì™„ë£Œ</p>',
            to_emails='a17019@kakao.com',  # ë˜ëŠ” ['email1', 'email2']
            conn_id='conn_smtp_gmail'
        )
    """
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart
    from airflow.hooks.base import BaseHook
    
    # Airflow Connectionì—ì„œ SMTP ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    connection = BaseHook.get_connection(conn_id)
    
    smtp_host = connection.host
    smtp_port = connection.port
    smtp_user = connection.login
    smtp_password = connection.password
    from_email = connection.extra_dejson.get('from_email') or smtp_user
    
    # to_emailsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(to_emails, str):
        to_list = [to_emails]
    else:
        to_list = to_emails
    
    # ì´ë©”ì¼ ìƒì„±
    msg = MIMEMultipart('alternative')
    msg['Subject'] = subject
    msg['From'] = from_email
    msg['To'] = ', '.join(to_list)
    
    # HTML ë³¸ë¬¸
    html_part = MIMEText(html_content, 'html', 'utf-8')
    msg.attach(html_part)
    
    # SMTP ë°œì†¡
    try:
        print(f"ğŸ“§ ì´ë©”ì¼ ë°œì†¡ ì‹œì‘...")
        print(f"   - ë°œì‹ : {from_email}")
        print(f"   - ìˆ˜ì‹ : {to_list}")
        print(f"   - ì œëª©: {subject}")
        
        with smtplib.SMTP(smtp_host, smtp_port) as server:
            server.starttls()
            server.login(smtp_user, smtp_password)
            server.send_message(msg)
        
        print(f"âœ… ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ: {len(to_list)}ëª…")
        return f"ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ: {len(to_list)}ëª…"
    
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
        raise


# Temp Parquet íŒŒì¼ ì •ë¦¬ í•¨ìˆ˜
def cleanup_temp_parquets(**context):
    """
    temp ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  parquet íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    
    DAG ì‚¬ìš©ë²•:
    -----------
    from modules.transform.utility.io import cleanup_temp_parquets
    
    cleanup_task = PythonOperator(
        task_id='cleanup_temp_files',
        python_callable=cleanup_temp_parquets,
        provide_context=True,
        dag=dag,
    )
    
    # ì¼ë°˜ì ìœ¼ë¡œ DAGì˜ ë§¨ ë§ˆì§€ë§‰ì— ë°°ì¹˜
    [task1, task2, task3] >> cleanup_task
    
    Returns:
    --------
    str: ì‚­ì œëœ íŒŒì¼ ìˆ˜ì™€ ìš©ëŸ‰ ì •ë³´
    """
    import shutil
    from pathlib import Path
    
    temp_dir = ONEDRIVE_DB / 'temp'
    
    if not temp_dir.exists():
        print(f"[ì •ë¦¬] temp ë””ë ‰í† ë¦¬ ì—†ìŒ: {temp_dir}")
        return "ì‚­ì œí•  íŒŒì¼ ì—†ìŒ (ë””ë ‰í† ë¦¬ ì—†ìŒ)"
    
    # parquet íŒŒì¼ ì°¾ê¸°
    parquet_files = list(temp_dir.glob('*.parquet'))
    
    if not parquet_files:
        print(f"[ì •ë¦¬] ì‚­ì œí•  parquet íŒŒì¼ ì—†ìŒ")
        return "ì‚­ì œí•  íŒŒì¼ ì—†ìŒ"
    
    # íŒŒì¼ í¬ê¸° ê³„ì‚°
    total_size = sum(f.stat().st_size for f in parquet_files)
    total_size_mb = total_size / (1024 * 1024)
    
    print(f"[ì •ë¦¬] ì‚­ì œ ëŒ€ìƒ: {len(parquet_files)}ê°œ íŒŒì¼ ({total_size_mb:.2f} MB)")
    
    # íŒŒì¼ ì‚­ì œ
    deleted_count = 0
    failed_files = []
    
    for parquet_file in parquet_files:
        try:
            parquet_file.unlink()
            deleted_count += 1
            print(f"[ì •ë¦¬] ì‚­ì œ: {parquet_file.name}")
        except Exception as e:
            failed_files.append((parquet_file.name, str(e)))
            print(f"[ê²½ê³ ] ì‚­ì œ ì‹¤íŒ¨: {parquet_file.name} - {e}")
    
    # ê²°ê³¼ ë©”ì‹œì§€
    result = f"ì‚­ì œ ì™„ë£Œ: {deleted_count}ê°œ íŒŒì¼ ({total_size_mb:.2f} MB)"
    
    if failed_files:
        result += f", ì‹¤íŒ¨: {len(failed_files)}ê°œ"
        print(f"[ê²½ê³ ] ì‚­ì œ ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡:")
        for fname, error in failed_files:
            print(f"  - {fname}: {error}")
    
    return result


# ìˆ˜ì§‘ CSV íŒŒì¼ ì •ë¦¬ í•¨ìˆ˜
def cleanup_collected_csvs(**context):
    """
    ì˜ì—…íŒ€_ìˆ˜ì§‘ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  CSV íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    
    DAG ì‚¬ìš©ë²•:
    -----------
    from modules.transform.utility.io import cleanup_collected_csvs
    
    cleanup_csv_task = PythonOperator(
        task_id='cleanup_collected_files',
        python_callable=cleanup_collected_csvs,
        provide_context=True,
        dag=dag,
    )
    
    # ì¼ë°˜ì ìœ¼ë¡œ DAGì˜ ë§¨ ë§ˆì§€ë§‰ì— ë°°ì¹˜ (temp íŒŒì¼ ì •ë¦¬ì™€ í•¨ê»˜)
    [email_task] >> cleanup_temp_task >> cleanup_csv_task
    
    Returns:
    --------
    str: ì‚­ì œëœ íŒŒì¼ ìˆ˜ì™€ ìš©ëŸ‰ ì •ë³´
    """
    import shutil
    from pathlib import Path
    
    collect_dir = COLLECT_DB / 'ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘'
    
    if not collect_dir.exists():
        print(f"[ì •ë¦¬] ìˆ˜ì§‘ ë””ë ‰í† ë¦¬ ì—†ìŒ: {collect_dir}")
        return "ì‚­ì œí•  íŒŒì¼ ì—†ìŒ (ë””ë ‰í† ë¦¬ ì—†ìŒ)"
    
    # CSV íŒŒì¼ ì°¾ê¸° (baemin, coupang ë“±)
    csv_files = list(collect_dir.glob('*.csv'))
    
    if not csv_files:
        print(f"[ì •ë¦¬] ì‚­ì œí•  CSV íŒŒì¼ ì—†ìŒ")
        return "ì‚­ì œí•  íŒŒì¼ ì—†ìŒ"
    
    # íŒŒì¼ í¬ê¸° ê³„ì‚°
    total_size = sum(f.stat().st_size for f in csv_files)
    total_size_mb = total_size / (1024 * 1024)
    
    print(f"[ì •ë¦¬] ì‚­ì œ ëŒ€ìƒ: {len(csv_files)}ê°œ CSV íŒŒì¼ ({total_size_mb:.2f} MB)")
    
    # íŒŒì¼ ì‚­ì œ
    deleted_count = 0
    failed_files = []
    
    for csv_file in csv_files:
        try:
            csv_file.unlink()
            deleted_count += 1
            print(f"[ì •ë¦¬] ì‚­ì œ: {csv_file.name}")
        except Exception as e:
            failed_files.append((csv_file.name, str(e)))
            print(f"[ê²½ê³ ] ì‚­ì œ ì‹¤íŒ¨: {csv_file.name} - {e}")
    
    # ê²°ê³¼ ë©”ì‹œì§€
    result = f"CSV ì‚­ì œ ì™„ë£Œ: {deleted_count}ê°œ íŒŒì¼ ({total_size_mb:.2f} MB)"
    
    if failed_files:
        result += f", ì‹¤íŒ¨: {len(failed_files)}ê°œ"
        print(f"[ê²½ê³ ] ì‚­ì œ ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡:")
        for fname, error in failed_files:
            print(f"  - {fname}: {error}")
    
    return result


# ============================================================
# CSV â†’ Parquet ë³€í™˜ ë° OneDrive ì €ì¥
# ============================================================
'''
from modules.transform.utility.io import csv_to_parquet_backup

'''


def csv_to_parquet_backup(
    csv_filename: str,
    onedrive_subfolder: str = "ì˜ì—…ê´€ë¦¬ë¶€_DB",
    **context
) -> str:
    """
    ë¡œì»¬DBì˜ CSV íŒŒì¼ì„ ì½ì–´ Parquetìœ¼ë¡œ ë³€í™˜ í›„ OneDriveì— ì €ì¥
    
    Args:
        csv_filename: ë¡œì»¬DBì—ì„œ ì½ì„ CSV íŒŒì¼ëª… (e.g., 'sales_employee.csv')
        onedrive_subfolder: OneDriveì— ì €ì¥í•  ì„œë¸Œí´ë” (ê¸°ë³¸ê°’: 'ì˜ì—…ê´€ë¦¬ë¶€_DB')
        **context: Airflow context
    
    Returns:
        str: ì²˜ë¦¬ ê²°ê³¼ ë©”ì‹œì§€
    
    Example:
        csv_to_parquet_backup('sales_employee.csv', 'ì˜ì—…ê´€ë¦¬ë¶€_DB')
        # LOCAL_DB/sales_employee.csv â†’ ONEDRIVE_DB/ì˜ì—…ê´€ë¦¬ë¶€_DB/sales_employee.parquet
    """
    try:
        print("=" * 60)
        print("ğŸ”„ CSV â†’ Parquet ë³€í™˜ ë° ë°±ì—… ì‹œì‘")
        print("=" * 60)
        
        # 1. ë¡œì»¬DBì—ì„œ CSV ì½ê¸°
        csv_path = LOCAL_DB / csv_filename
        print(f"ğŸ“‚ ë¡œì»¬DB ê²½ë¡œ: {csv_path}")
        
        if not csv_path.exists():
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {csv_path}")
            return f"ì‹¤íŒ¨: íŒŒì¼ ì—†ìŒ ({csv_filename})"
        
        # ì¸ì½”ë”© ìë™ ê°ì§€
        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']
        df = None
        used_encoding = None
        
        for encoding in encodings:
            try:
                print(f"   ì‹œë„: {encoding}ìœ¼ë¡œ ì½ê¸°...")
                df = pd.read_csv(csv_path, encoding=encoding)
                used_encoding = encoding
                print(f"   âœ… {encoding}ìœ¼ë¡œ ì„±ê³µ ({len(df):,}í–‰)")
                break
            except (UnicodeDecodeError, LookupError):
                continue
        
        if df is None:
            print(f"âŒ CSV ì½ê¸° ì‹¤íŒ¨ (ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨)")
            return f"ì‹¤íŒ¨: CSV ì½ê¸° ì‹¤íŒ¨ ({csv_filename})"
        
        print(f"ğŸ“Š CSV ì •ë³´:")
        print(f"   - íŒŒì¼ëª…: {csv_path.name}")
        print(f"   - í–‰ ìˆ˜: {len(df):,}")
        print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        print(f"   - ì»¬ëŸ¼ëª…: {list(df.columns)}")
        print(f"   - ì¸ì½”ë”©: {used_encoding}")
        
        # 2. Parquet íŒŒì¼ëª… ìƒì„± (í™•ì¥ì ë³€ê²½)
        parquet_filename = csv_path.stem + ".parquet"  # sales_employee.parquet
        
        # 3. OneDrive ì €ì¥ ê²½ë¡œ ìƒì„±
        onedrive_path = ONEDRIVE_DB / onedrive_subfolder
        onedrive_path.mkdir(parents=True, exist_ok=True)
        
        parquet_output_path = onedrive_path / parquet_filename
        print(f"\nğŸ’¾ OneDrive ì €ì¥ ê²½ë¡œ: {parquet_output_path}")
        
        # 4. Parquetìœ¼ë¡œ ë³€í™˜ ë° ì €ì¥
        df.to_parquet(
            parquet_output_path,
            index=False,
            engine='pyarrow',
            compression='snappy'
        )
        print(f"âœ… Parquet ì €ì¥ ì™„ë£Œ")
        
        # 5. íŒŒì¼ í¬ê¸° í™•ì¸
        parquet_size = parquet_output_path.stat().st_size / (1024 * 1024)  # MB
        csv_size = csv_path.stat().st_size / (1024 * 1024)  # MB
        compression_ratio = (1 - parquet_size / csv_size) * 100 if csv_size > 0 else 0
        
        print(f"\nğŸ“ˆ íŒŒì¼ í¬ê¸° ë¹„êµ:")
        print(f"   - CSV: {csv_size:.2f} MB")
        print(f"   - Parquet: {parquet_size:.2f} MB")
        print(f"   - ì••ì¶•ë¥ : {compression_ratio:.1f}%")
        
        print("=" * 60)
        print(f"ğŸ‰ CSV â†’ Parquet ë³€í™˜ ì™„ë£Œ: {len(df):,}í–‰")
        print("=" * 60)
        
        return f"âœ… ì™„ë£Œ: {len(df):,}í–‰ ({parquet_filename})"
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return f"âŒ ì‹¤íŒ¨: {str(e)}"


# ============================================================
# sub_order_id ìƒì„± í•¨ìˆ˜ë“¤
# ============================================================

def create_sub_order_id_simple(
    df: pd.DataFrame,
    order_col: str = 'order_id',
    output_col: str = 'sub_order_id'
) -> pd.DataFrame:
    """
    ì£¼ë¬¸ë²ˆí˜¸ + ìˆœë²ˆ ë°©ì‹ìœ¼ë¡œ sub_order_id ìƒì„± (ê¶Œì¥)
    
    ì˜ˆì‹œ: B22V00DE2A_1, B22V00DE2A_2, ...
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        order_col: ì£¼ë¬¸ë²ˆí˜¸ ì»¬ëŸ¼ëª…
        output_col: ì¶œë ¥ ì»¬ëŸ¼ëª…
    
    Returns:
        sub_order_idê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """
    df = df.copy()
    
    # ì£¼ë¬¸ë²ˆí˜¸ë³„ ìˆœë²ˆ ë¶€ì—¬
    df[output_col] = (
        df[order_col].astype(str) + '_' + 
        (df.groupby(order_col).cumcount() + 1).astype(str)
    )
    
    print(f"[sub_order_id] ë‹¨ìˆœ ìˆœë²ˆ ë°©ì‹ìœ¼ë¡œ ìƒì„± ì™„ë£Œ")
    print(f"  - ì›ë³¸ ë°ì´í„°: {len(df):,}í–‰")
    print(f"  - ê³ ìœ  ID: {df[output_col].nunique():,}ê°œ")
    
    # ì¤‘ë³µ ì²´í¬
    duplicates = df[df.duplicated(subset=output_col, keep=False)]
    if len(duplicates) > 0:
        print(f"  âš ï¸ ê²½ê³ : {len(duplicates)}ê°œ ì¤‘ë³µ ë°œê²¬!")
    else:
        print(f"  âœ… ì¤‘ë³µ ì—†ìŒ")
    
    return df


def create_sub_order_id_hash(
    df: pd.DataFrame,
    natural_key_cols: list,
    output_col: str = 'sub_order_id'
) -> pd.DataFrame:
    """
    í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ sub_order_id ìƒì„± (ë³µí•©í‚¤ í•„ìš” ì‹œ)
    
    ì˜ˆì‹œ: a1b2c3d4e5f6g7h8 (16ìë¦¬ í•´ì‹œ)
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        natural_key_cols: ìì—°í‚¤ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['order_id', 'collected_at', 'order_summary'])
        output_col: ì¶œë ¥ ì»¬ëŸ¼ëª…
    
    Returns:
        sub_order_idê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """
    import hashlib
    
    df = df.copy()
    
    # ìì—°í‚¤ ì»¬ëŸ¼ë“¤ì„ ë¬¸ìì—´ë¡œ ê²°í•©
    key_parts = [df[col].astype(str) for col in natural_key_cols]
    uk_series = pd.concat(key_parts, axis=1).agg('|'.join, axis=1)
    
    # SHA1 í•´ì‹œì˜ ì• 16ìë¦¬ ì‚¬ìš©
    df[output_col] = uk_series.apply(
        lambda s: hashlib.sha1(s.encode('utf-8')).hexdigest()[:16]
    )
    
    print(f"[sub_order_id] í•´ì‹œ ë°©ì‹ìœ¼ë¡œ ìƒì„± ì™„ë£Œ")
    print(f"  - ì›ë³¸ ë°ì´í„°: {len(df):,}í–‰")
    print(f"  - ê³ ìœ  ID: {df[output_col].nunique():,}ê°œ")
    print(f"  - ìì—°í‚¤: {natural_key_cols}")
    
    # ì¤‘ë³µ ì²´í¬
    duplicates = df[df.duplicated(subset=output_col, keep=False)]
    if len(duplicates) > 0:
        print(f"  âš ï¸ ê²½ê³ : {len(duplicates)}ê°œ ì¤‘ë³µ ë°œê²¬!")
    else:
        print(f"  âœ… ì¤‘ë³µ ì—†ìŒ")
    
    return df