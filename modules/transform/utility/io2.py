"""
DB íŒŒì¼ ë³‘í•© íŒŒì´í”„ë¼ì¸ - ê°„í¸ ë²„ì „

ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥:
  1. ì—¬ëŸ¬ ê²½ë¡œì—ì„œ CSV/ì—‘ì…€ íŒŒì¼ ìë™ íƒìƒ‰ ë° ë³‘í•©
  2. ì¤‘ë³µ íŒŒì¼ ìë™ ì œê±° (ìµœì‹  íŒŒì¼ ìš°ì„ )
  3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì§‘ê³„
  4. Surrogate Key ìƒì„±
  5. ë¡œì»¬ DBì— CSV ì €ì¥

ğŸš€ NEW! ê°„í¸ í•¨ìˆ˜ë“¤:
  - create_auto_loader(): ìë™ ë¡œë” í•¨ìˆ˜ ìƒì„±
  - create_simple_preprocessor(): ê°„ë‹¨í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜ ìƒì„±
  - create_pipeline_tasks(): ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í•œ ë²ˆì— ìƒì„±

ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
    # ê¸°ì¡´ ë°©ì‹ (ë³µì¡í•¨)
    task_load = PythonOperator(task_id='load', python_callable=load_df, ...)
    task_preprocess = PythonOperator(task_id='preprocess', ...)
    task_save = PythonOperator(task_id='save', ...)
    task_load >> task_preprocess >> task_save
    
    # ìƒˆë¡œìš´ ë°©ì‹ (ê°„í¸í•¨)
    tasks = create_pipeline_tasks(
        dag=dag,
        pipeline_name='sales',
        loader_config={'file_pattern': 'sales_*.csv'},
        preprocess_config={'groupby_cols': ['date', 'store'], ...},
        save_config={'output_filename': 'sales.csv'}
    )
"""

import pandas as pd
import numpy as np
from pathlib import Path
import hashlib
import glob

from modules.load.load_df_glob import load_data
from modules.transform.utility.paths import TEMP_DIR, LOCAL_DB, COLLECT_DB


# ============================================================
# ê²½ë¡œ ì„¤ì •
# ============================================================
PATH_TOORDER = "/opt/airflow/download/ì—…ë¡œë“œ_temp"
PATH_BACKUP = LOCAL_DB / "ì˜ì—…ê´€ë¦¬ë¶€_DB"


# ============================================================
# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def add_surrogate_key(df: pd.DataFrame, natural_key_cols: list[str]) -> pd.DataFrame:
    """
    ìì—°í‚¤ ê¸°ë°˜ Surrogate Key ìƒì„±
    
    Args:
        df: DataFrame
        natural_key_cols: ìì—°í‚¤ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        'key' ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
        
    Example:
        df = add_surrogate_key(df, natural_key_cols=["ì¼ì", "ì£¼ë¬¸ë²ˆí˜¸"])
    """
    out = df.copy()
    key_parts = [df[col].astype(str) for col in natural_key_cols]
    uk_series = pd.concat(key_parts, axis=1).agg('|'.join, axis=1)
    out['key'] = uk_series.apply(
        lambda s: hashlib.sha1(s.encode('utf-8')).hexdigest()[:16]
    )
    cols = ['key'] + [c for c in out.columns if c != 'key']
    out = out[cols]
    return out


def get_unique_files(file_list):
    """
    íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ ì œê±° (ê°™ì€ ì´ë¦„ì´ë©´ ìµœì‹  íŒŒì¼ë§Œ ìœ ì§€)
    
    Args:
        file_list: Path ê°ì²´ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì¤‘ë³µ ì œê±°ëœ Path ê°ì²´ ë¦¬ìŠ¤íŠ¸
        
    Example:
        >>> files = [
        ...     Path('/path1/data_20260101.csv'),
        ...     Path('/path2/data_20260101.csv'),  # ê°™ì€ ì´ë¦„ (ë” ìµœì‹ )
        ...     Path('/path1/data_20260102.csv'),
        ... ]
        >>> unique = get_unique_files(files)
        >>> # ê²°ê³¼: path2ì˜ data_20260101.csv + data_20260102.csv
    """
    unique_files = {}
    for f in file_list:
        fname = f.name
        if fname not in unique_files or f.stat().st_mtime > unique_files[fname].stat().st_mtime:
            unique_files[fname] = f
    return list(unique_files.values())


def load_and_concat_csv(file_paths):
    """
    CSV íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ë³‘í•©
    
    Args:
        file_paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë³‘í•©ëœ DataFrame
        
    Example:
        >>> files = [
        ...     Path('/data/sales_2026_01.csv'),
        ...     Path('/data/sales_2026_02.csv'),
        ... ]
        >>> df = load_and_concat_csv(files)
        >>> print(len(df))  # ë‘ íŒŒì¼ì˜ ì´ í–‰ ìˆ˜
    """
    dfs = []
    for fpath in file_paths:
        print(f"   ì½ëŠ” ì¤‘: {fpath}")
        df = pd.read_csv(fpath)
        dfs.append(df)
        print(f"   âœ“ {len(df)}í–‰ ë¡œë“œ")
    
    result_df = pd.concat(dfs, ignore_index=True)
    print(f"ë³‘í•© ì™„ë£Œ: {len(result_df):,}í–‰")
    return result_df


def load_and_concat_excel(file_paths, header=3, date_extractor=None):
    """
    ì—‘ì…€ íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ë³‘í•©
    
    Args:
        file_paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        header: í—¤ë” í–‰ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘, ê¸°ë³¸ê°’ 3)
        date_extractor: íŒŒì¼ëª…ì—ì„œ ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ì„ íƒ)
        
    Returns:
        ë³‘í•©ëœ DataFrame
        
    Example:
        >>> files = [Path('/data/report_20260101.xlsx')]
        >>> df = load_and_concat_excel(
        ...     files, 
        ...     header=2,  # 3ë²ˆì§¸ í–‰ì´ í—¤ë”
        ...     date_extractor=extract_date_from_filename
        ... )
        >>> print(df.columns)  # date ì»¬ëŸ¼ ìë™ ì¶”ê°€ë¨
    """
    dfs = []
    for fpath in file_paths:
        print(f"   ì½ëŠ” ì¤‘: {fpath}")
        df = pd.read_excel(fpath, header=header)
        
        if date_extractor:
            file_name = Path(fpath).name
            date_info = date_extractor(file_name)
            df['date'] = date_info['date']
        
        dfs.append(df)
        print(f"   âœ“ {len(df)}í–‰ ë¡œë“œ")
    
    result_df = pd.concat(dfs, ignore_index=True)
    print(f"ë³‘í•© ì™„ë£Œ: {len(result_df):,}í–‰")
    return result_df


def save_to_parquet(df, context, filename_prefix):
    """
    DataFrameì„ Parquetìœ¼ë¡œ ì €ì¥
    
    Args:
        df: ì €ì¥í•  DataFrame
        context: Airflow context (ds_nodash í¬í•¨)
        filename_prefix: íŒŒì¼ëª… prefix
        
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (Path ê°ì²´)
        
    Example:
        >>> # Airflow Task ë‚´ë¶€ì—ì„œ
        >>> output_path = save_to_parquet(
        ...     df, 
        ...     context, 
        ...     "sales_data_raw"
        ... )
        >>> # ê²°ê³¼: /tmp/sales_data_raw_20260119.parquet
        >>> context['task_instance'].xcom_push(
        ...     key='data_path', 
        ...     value=str(output_path)
        ... )
    """
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    output_path = temp_dir / f"{filename_prefix}_{context['ds_nodash']}.parquet"
    df.to_parquet(output_path, index=False, engine='pyarrow')
    return output_path


def extract_date_from_filename(file_name):
    """
    íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ë§ˆì§€ë§‰ ì–¸ë”ìŠ¤ì½”ì–´ ë’¤ì˜ ìˆ«ì)
    
    Args:
        file_name: íŒŒì¼ëª… (str)
        
    Returns:
        {'date': ë‚ ì§œë¬¸ìì—´} ë˜ëŠ” {'date': None}
        
    Example:
        >>> extract_date_from_filename('toorder_review_20260119.csv')
        {'date': '20260119'}
        >>> extract_date_from_filename('data_v2_20260101.xlsx')
        {'date': '20260101'}
    """
    try:
        date_str = file_name.split('_')[-1].split('.')[0]
        return {'date': date_str}
    except Exception as e:
        print(f"[ê²½ê³ ] ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {file_name}, {e}")
        return {'date': None}


def fin_save_to_csv(
    input_task_id,
    input_xcom_key,
    output_csv_path=None,
    output_filename='toorder_review_doridang.csv',
    output_subdir='ì˜ì—…ê´€ë¦¬ë¶€_DB',
    dedup_key=None,
    **context
):
    """
    Parquet ë°ì´í„°ë¥¼ ë¡œì»¬ DBì— CSVë¡œ ì €ì¥ (ì•ˆì „í•œ ì›ìì  ì“°ê¸°)
    
    ğŸ’¾ ì£¼ìš” ê¸°ëŠ¥:
      - Parquetì„ CSVë¡œ ë³€í™˜
      - ê¸°ì¡´ íŒŒì¼ ìë™ ë°±ì—… (.bak)
      - ì„ì‹œ íŒŒì¼ë¡œ ì“´ í›„ ì›ìì  êµì²´
      - datetime ì»¬ëŸ¼ ìë™ í¬ë§·íŒ…
      - ì¤‘ë³µ ì œê±° (ì„ íƒ)
    
    Args:
        input_task_id: ì´ì „ Taskì˜ task_id
        input_xcom_key: ì½ì„ XCom í‚¤
        output_csv_path: ì§ì ‘ ê²½ë¡œ ì§€ì • (ì„ íƒ)
        output_filename: CSV íŒŒì¼ëª… (ê¸°ë³¸ê°’: 'toorder_review_doridang.csv')
        output_subdir: LOCAL_DB í•˜ìœ„ í´ë” (ê¸°ë³¸ê°’: 'ì˜ì—…ê´€ë¦¬ë¶€_DB')
        dedup_key: ì¤‘ë³µ ì œê±° í‚¤ (str ë˜ëŠ” list, ì„ íƒ)
        **context: Airflow context
    
    Returns:
        str: ì €ì¥ ê²°ê³¼ ë©”ì‹œì§€
    
    XCom ì…ë ¥:
        key=input_xcom_key: Parquet íŒŒì¼ ê²½ë¡œ
    
    ğŸ’¡ DAGì—ì„œ ì‚¬ìš© ì˜ˆì‹œ 1 (ê¸°ë³¸):
        task_save = PythonOperator(
            task_id='save_csv',
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': 'preprocess_data',
                'input_xcom_key': 'processed_data_path',
                'output_filename': 'my_data.csv',
            }
        )
        # ê²°ê³¼: LOCAL_DB/ì˜ì—…ê´€ë¦¬ë¶€_DB/my_data.csv
    
    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ 2 (ì¤‘ë³µ ì œê±° + ì»¤ìŠ¤í…€ í´ë”):
        task_save = PythonOperator(
            task_id='save_csv',
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': 'preprocess_data',
                'input_xcom_key': 'processed_data_path',
                'output_filename': 'unique_sales.csv',
                'output_subdir': 'ë§¤ì¶œ_ë°ì´í„°/ì›”ë³„',
                'dedup_key': ['date', 'order_id'],  # ì¤‘ë³µ ì œê±° í‚¤
            }
        )
        # ê²°ê³¼: LOCAL_DB/ë§¤ì¶œ_ë°ì´í„°/ì›”ë³„/unique_sales.csv
    
    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ 3 (ì ˆëŒ€ ê²½ë¡œ ì§€ì •):
        task_save = PythonOperator(
            task_id='save_csv',
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': 'preprocess_data',
                'input_xcom_key': 'processed_data_path',
                'output_csv_path': '/custom/path/data.csv',
            }
        )
    """
    import os
    import shutil
    import tempfile
    from modules.transform.utility.paths import LOCAL_DB
    
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(task_ids=input_task_id, key=input_xcom_key)
    
    if not parquet_path:
        print(f"[ê²½ê³ ] ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: ë°ì´í„° ì—†ìŒ"
    
    if not os.path.exists(parquet_path):
        print(f"[ê²½ê³ ] íŒŒì¼ ê²½ë¡œ ì—†ìŒ: {parquet_path}")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: íŒŒì¼ ì—†ìŒ"
    
    df = pd.read_parquet(parquet_path)
    
    print(f"\n{'='*60}")
    print(f"[ì…ë ¥] ë°ì´í„°: {len(df):,}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")
    
    # ì¤‘ë³µ ì œê±°
    if dedup_key:
        dedup_cols = [dedup_key] if isinstance(dedup_key, str) else dedup_key
        valid_cols = [c for c in dedup_cols if c in df.columns]
        if valid_cols:
            before = len(df)
            df.drop_duplicates(subset=valid_cols, keep='first', inplace=True)
            after = len(df)
            if before - after > 0:
                print(f"\n[ì¤‘ë³µ ì œê±°] {valid_cols} ê¸°ì¤€: {before - after:,}ê±´ ì œê±°")
    
    # ì¶œë ¥ ê²½ë¡œ
    if output_csv_path:
        local_csv_path = Path(output_csv_path)
    else:
        output_dir = LOCAL_DB / output_subdir
        output_dir.mkdir(parents=True, exist_ok=True)
        local_csv_path = output_dir / output_filename
    
    local_csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\n[ê²½ë¡œ] ì €ì¥ ìœ„ì¹˜: {local_csv_path}")
    
    # datetime ë³€í™˜
    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
    if datetime_cols:
        for col in datetime_cols:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S').fillna('')
    
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(
            mode='w', 
            delete=False, 
            dir=str(local_csv_path.parent),
            prefix='tmp_', 
            suffix='.csv', 
            encoding='utf-8-sig'
        ) as tmp_file:
            tmp_path = tmp_file.name
        
        df.to_csv(tmp_path, index=False, encoding='utf-8-sig')
        
        if local_csv_path.exists():
            backup_path = local_csv_path.parent / f"{local_csv_path.name}.bak"
            shutil.copy2(local_csv_path, backup_path)
            shutil.move(tmp_path, str(local_csv_path))
            backup_path.unlink()
        else:
            shutil.move(tmp_path, str(local_csv_path))
        
        csv_size = local_csv_path.stat().st_size / (1024 * 1024)
        print(f"[ì €ì¥] âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´ ({csv_size:.2f} MB)")
        
    except Exception as e:
        print(f"[ì—ëŸ¬] CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        if tmp_path and os.path.exists(tmp_path):
            os.remove(tmp_path)
        return f"ì €ì¥ ì‹¤íŒ¨: {e}"
    
    print(f"{'='*60}\n")
    return f"âœ… ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´"


# ============================================================
# ğŸš€ ê°„í¸ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ë“¤
# ============================================================

def create_auto_loader(
    file_pattern,
    search_paths=None,
    file_type='auto',
    excel_header=3,
    date_extractor=None,
    output_prefix='data_raw',
    xcom_key=None
):
    """
    ìë™ íŒŒì¼ ë¡œë” í•¨ìˆ˜ ìƒì„±ê¸°
    
    ğŸ“¦ ê¸°ëŠ¥:
      - ì§€ì •ëœ ê²½ë¡œì—ì„œ íŒŒì¼ ìë™ íƒìƒ‰
      - CSV/ì—‘ì…€ ìë™ ì„ íƒ
      - ì¤‘ë³µ íŒŒì¼ ì œê±°
      - Parquetìœ¼ë¡œ ì €ì¥
    
    Args:
        file_pattern: íŒŒì¼ëª… íŒ¨í„´ (ì˜ˆ: 'sales_*.csv', 'report_*.xlsx')
        search_paths: íƒìƒ‰ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: [upload_temp, onedrive_collect])
        file_type: íŒŒì¼ íƒ€ì… ('csv', 'excel', 'auto')
        excel_header: ì—‘ì…€ í—¤ë” í–‰ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 3)
        date_extractor: ë‚ ì§œ ì¶”ì¶œ í•¨ìˆ˜ (ê¸°ë³¸ê°’: extract_date_from_filename)
        output_prefix: ì¶œë ¥ íŒŒì¼ëª… prefix
        xcom_key: XCom í‚¤ (ê¸°ë³¸ê°’: output_prefix + '_path')
    
    Returns:
        Airflow Taskì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜
    
    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
        # 1. ë¡œë” ìƒì„±
        loader = create_auto_loader(
            file_pattern='sales_*.csv',
            search_paths=[Path('/data/sales'), COLLECT_DB / 'ë§¤ì¶œ'],
            output_prefix='sales_raw'
        )
        
        # 2. DAGì—ì„œ ì‚¬ìš©
        task = PythonOperator(
            task_id='load_sales',
            python_callable=loader
        )
        
        # ê²°ê³¼: XCom['sales_raw_path'] = parquet ê²½ë¡œ
    """
    if search_paths is None:
        upload_temp = Path('/opt/airflow/download/ì—…ë¡œë“œ_temp')
        onedrive = COLLECT_DB / "ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘"
        search_paths = [upload_temp, onedrive]
    
    if xcom_key is None:
        xcom_key = f"{output_prefix}_path"
    
    if date_extractor is None:
        date_extractor = extract_date_from_filename
    
    def auto_loader(**context):
        """ìë™ ìƒì„±ëœ ë¡œë” í•¨ìˆ˜"""
        ti = context['task_instance']
        
        all_files = []
        
        # íŒŒì¼ íƒìƒ‰
        for search_path in search_paths:
            if not Path(search_path).exists():
                continue
            
            if file_type == 'auto' or file_type == 'csv':
                csv_pattern = file_pattern if file_pattern.endswith('.csv') else file_pattern.replace('*', '*.csv')
                all_files.extend(list(Path(search_path).glob(csv_pattern)))
            
            if file_type == 'auto' or file_type == 'excel':
                excel_pattern = file_pattern if any(file_pattern.endswith(ext) for ext in ['.xlsx', '.xls']) else file_pattern.replace('*', '*.xlsx')
                all_files.extend(list(Path(search_path).glob(excel_pattern)))
        
        if not all_files:
            print(f"[âŒ] íŒŒì¼ ì—†ìŒ: {file_pattern}")
            ti.xcom_push(key=xcom_key, value=None)
            return "0ê±´ (íŒŒì¼ ì—†ìŒ)"
        
        print(f"[âœ…] ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ì¤‘ë³µ ì œê±°
        unique_files = get_unique_files(all_files)
        print(f"[ì¤‘ë³µ ì œê±°] {len(unique_files)}ê°œ íŒŒì¼ ì‚¬ìš©")
        
        # CSV vs ì—‘ì…€ êµ¬ë¶„
        csv_files = [f for f in unique_files if f.suffix.lower() == '.csv']
        excel_files = [f for f in unique_files if f.suffix.lower() in ['.xlsx', '.xls']]
        
        if csv_files:
            result_df = load_and_concat_csv(csv_files)
        elif excel_files:
            result_df = load_and_concat_excel(excel_files, header=excel_header, date_extractor=date_extractor)
        else:
            ti.xcom_push(key=xcom_key, value=None)
            return "0ê±´ (ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹)"
        
        # Parquet ì €ì¥
        output_path = save_to_parquet(result_df, context, output_prefix)
        ti.xcom_push(key=xcom_key, value=str(output_path))
        
        return f"âœ… {len(result_df):,}ê±´ ë¡œë“œ"
    
    return auto_loader


def create_simple_preprocessor(
    select_cols=None,
    dropna_cols=None,
    groupby_cols=None,
    agg_config=None,
    rename_map=None,
    replace_map=None,
    add_prefix=None,
    surrogate_key_cols=None,
    output_prefix='data_processed'
):
    """
    ê°„ë‹¨í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜ ìƒì„±ê¸°
    
    ğŸ”§ ê¸°ëŠ¥:
      - ì»¬ëŸ¼ ì„ íƒ
      - ê²°ì¸¡ì¹˜ ì œê±°
      - ê·¸ë£¹í™” & ì§‘ê³„
      - ì»¬ëŸ¼ëª… ë³€ê²½
      - ê°’ ì¹˜í™˜
      - Surrogate Key ìƒì„±
    
    Args:
        select_cols: ì„ íƒí•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        dropna_cols: null ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        groupby_cols: ê·¸ë£¹í™” ì»¬ëŸ¼
        agg_config: ì§‘ê³„ ì„¤ì • (dict)
        rename_map: ì»¬ëŸ¼ëª… ë³€ê²½ (dict)
        replace_map: {ì»¬ëŸ¼ëª…: {old: new}} í˜•ì‹
        add_prefix: ê°’ ì•ì— ì¶”ê°€í•  prefix (dict: {ì»¬ëŸ¼ëª…: prefix})
        surrogate_key_cols: Surrogate Key ìƒì„± ì»¬ëŸ¼
        output_prefix: ì¶œë ¥ íŒŒì¼ëª… prefix
    
    Returns:
        Airflow Taskì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜
    
    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
        preprocessor = create_simple_preprocessor(
            select_cols=['date', 'store', 'sales', 'orders'],
            dropna_cols=['store'],
            groupby_cols=['date', 'store'],
            agg_config={
                'total_sales': ('sales', 'sum'),
                'total_orders': ('orders', 'sum')
            },
            rename_map={'store': 'store_name'},
            replace_map={
                'store_name': {
                    'ì¼ì‚°ë°±ì„ì ': 'ë°±ì„ì ',
                    'êµ¬ë¡œë””ì§€í„¸ë‹¨ì§€ì ': 'êµ¬ë¡œë””ì§€í„¸ì '
                }
            },
            add_prefix={'store_name': 'ë„ë¦¬ë‹¹ '},
            surrogate_key_cols=['date', 'store_name'],
            output_prefix='sales_processed'
        )
        
        task = PythonOperator(
            task_id='preprocess',
            python_callable=preprocessor,
            op_kwargs={
                'input_task_id': 'load_data',
                'input_xcom_key': 'raw_data_path',
                'output_xcom_key': 'processed_data_path'
            }
        )
    """
    def preprocessor(input_task_id, input_xcom_key, output_xcom_key, **context):
        """ìë™ ìƒì„±ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
        ti = context['task_instance']
        
        # ë°ì´í„° ë¡œë“œ
        parquet_path = ti.xcom_pull(task_ids=input_task_id, key=input_xcom_key)
        
        if not parquet_path:
            print(f"[ê²½ê³ ] ì…ë ¥ ë°ì´í„° ì—†ìŒ")
            ti.xcom_push(key=output_xcom_key, value=None)
            return "0ê±´ (ì…ë ¥ ì—†ìŒ)"
        
        df = pd.read_parquet(parquet_path)
        print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(df):,}í–‰")
        
        # 1. ì»¬ëŸ¼ ì„ íƒ
        if select_cols:
            df = df[select_cols]
        
        # 2. ê²°ì¸¡ì¹˜ ì œê±°
        if dropna_cols:
            for col in dropna_cols:
                if col in df.columns:
                    df = df[~df[col].isnull()]
        
        # 3. ê·¸ë£¹í™” & ì§‘ê³„
        if groupby_cols and agg_config:
            df = df.groupby(groupby_cols).agg(**agg_config).reset_index()
        
        # 4. ì»¬ëŸ¼ëª… ë³€ê²½
        if rename_map:
            df.rename(columns=rename_map, inplace=True)
        
        # 5. Surrogate Key ìƒì„±
        if surrogate_key_cols:
            df = add_surrogate_key(df, natural_key_cols=surrogate_key_cols)
            df.rename(columns={'key': 'id'}, inplace=True)
        
        # 6. Prefix ì¶”ê°€
        if add_prefix:
            for col, prefix in add_prefix.items():
                if col in df.columns:
                    df[col] = prefix + df[col].astype(str)
        
        # 7. ê°’ ì¹˜í™˜
        if replace_map:
            for col, mapping in replace_map.items():
                if col in df.columns:
                    df[col] = df[col].replace(mapping)
        
        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df):,}í–‰")
        
        # Parquet ì €ì¥
        output_path = save_to_parquet(df, context, output_prefix)
        ti.xcom_push(key=output_xcom_key, value=str(output_path))
        
        return f"âœ… {len(df):,}í–‰ ì „ì²˜ë¦¬"
    
    return preprocessor


def create_pipeline_tasks(
    dag,
    pipeline_name,
    loader_config,
    preprocess_config=None,
    save_config=None
):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ Task ì„¸íŠ¸ë¥¼ í•œ ë²ˆì— ìƒì„±
    
    ğŸš€ ê¸°ëŠ¥:
      - Load, Preprocess, Save Task ìë™ ìƒì„±
      - Task ê°„ ì˜ì¡´ì„± ìë™ ì„¤ì •
      - XCom ìë™ ì—°ê²°
    
    Args:
        dag: Airflow DAG ê°ì²´
        pipeline_name: íŒŒì´í”„ë¼ì¸ ì´ë¦„ (task_id prefixë¡œ ì‚¬ìš©)
        loader_config: create_auto_loaderì— ì „ë‹¬í•  ì„¤ì •
        preprocess_config: create_simple_preprocessorì— ì „ë‹¬í•  ì„¤ì • (ì„ íƒ)
        save_config: fin_save_to_csvì— ì „ë‹¬í•  ì„¤ì • (ì„ íƒ)
    
    Returns:
        dict: {'load': task, 'preprocess': task, 'save': task}
    
    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
        with DAG(...) as dag:
            tasks = create_pipeline_tasks(
                dag=dag,
                pipeline_name='sales',
                loader_config={
                    'file_pattern': 'sales_*.csv',
                    'output_prefix': 'sales_raw'
                },
                preprocess_config={
                    'select_cols': ['date', 'store', 'amount'],
                    'groupby_cols': ['date', 'store'],
                    'agg_config': {'total': ('amount', 'sum')},
                    'output_prefix': 'sales_processed'
                },
                save_config={
                    'output_filename': 'sales_final.csv'
                }
            )
            
            # ìë™ìœ¼ë¡œ load >> preprocess >> save ì—°ê²°ë¨
    """
    from airflow.operators.python import PythonOperator
    
    tasks = {}
    
    # Task 1: Load
    loader = create_auto_loader(**loader_config)
    task_load = PythonOperator(
        task_id=f"{pipeline_name}_load",
        python_callable=loader,
        dag=dag
    )
    tasks['load'] = task_load
    
    # Task 2: Preprocess (ì„ íƒ)
    if preprocess_config:
        preprocessor = create_simple_preprocessor(**preprocess_config)
        
        load_xcom_key = loader_config.get('xcom_key', f"{loader_config['output_prefix']}_path")
        preprocess_xcom_key = f"{preprocess_config['output_prefix']}_path"
        
        task_preprocess = PythonOperator(
            task_id=f"{pipeline_name}_preprocess",
            python_callable=preprocessor,
            op_kwargs={
                'input_task_id': f"{pipeline_name}_load",
                'input_xcom_key': load_xcom_key,
                'output_xcom_key': preprocess_xcom_key
            },
            dag=dag
        )
        tasks['preprocess'] = task_preprocess
        task_load >> task_preprocess
    
    # Task 3: Save (ì„ íƒ)
    if save_config:
        if preprocess_config:
            input_task_id = f"{pipeline_name}_preprocess"
            input_xcom_key = f"{preprocess_config['output_prefix']}_path"
        else:
            input_task_id = f"{pipeline_name}_load"
            input_xcom_key = loader_config.get('xcom_key', f"{loader_config['output_prefix']}_path")
        
        task_save = PythonOperator(
            task_id=f"{pipeline_name}_save",
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': input_task_id,
                'input_xcom_key': input_xcom_key,
                **save_config
            },
            dag=dag
        )
        tasks['save'] = task_save
        
        if preprocess_config:
            task_preprocess >> task_save
        else:
            task_load >> task_save
    
    return tasks