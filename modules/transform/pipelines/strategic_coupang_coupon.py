# modules/transform/pipelines/strategic_coupang_coupon.py
"""
ì¿ íŒ¡ì´ì¸  ì¿ í° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
"""
import pandas as pd
import numpy as np
from pathlib import Path
import re

from modules.load.load_df_glob import load_data
from modules.transform.utility.paths import TEMP_DIR, LOCAL_DB, COLLECT_DB

# ============================================================
# ê²½ë¡œ ì„¤ì •
# ============================================================
PATH_COUPON = COLLECT_DB / "ì „ëµê¸°íšíŒ€_ìˆ˜ì§‘" / "coupangeats_coupon_*.csv"
PATH_FIN_COUPON = COLLECT_DB / "ì „ëµê¸°íšíŒ€_ìˆ˜ì§‘" / "ì „ë‹¬ìš©"
PATH_BACKUP = "/opt/airflow/download/ì—…ë¡œë“œ_temp"

def extract_coupon_info(file_name: str) -> dict:
    """íŒŒì¼ëª…ì—ì„œ ë§¤ì¥ëª…(ì˜ˆ: 'ë„ë¦¬ë‹¹ ê°€ë½ì '), store_idë§Œ ì¶”ì¶œ.

    ì˜ˆì‹œ íŒŒì¼ëª…:
    coupangeats_coupon_ë‹­ë„ë¦¬íƒ•_ì „ë¬¸_ë„ë¦¬ë‹¹_ê°€ë½ì _2877236_20260116 (1).csv
    â†’ stores_name/store_names: 'ë„ë¦¬ë‹¹ ê°€ë½ì '
      store_id: '2877236'
    
    âš ï¸ 'ë‚ ì§œ' ì»¬ëŸ¼ì€ CSV ì›ë³¸ ë°ì´í„° ìœ ì§€ë¥¼ ìœ„í•´ ì¶”ì¶œí•˜ì§€ ì•ŠìŒ
    """
    base = Path(file_name).stem
    tokens = re.split(r"[_\s]+", base)
    tokens = [t for t in tokens if t]

    # ê³µí†µ ì ‘ë‘ì–´ ì œê±°
    tokens = [t for t in tokens if t.lower() not in ("coupangeats", "coupon", "coupangeats_coupon")]

    # í•œê¸€ ë§¤ì¥ëª… í›„ë³´: ìˆ«ì/ê´„í˜¸ê°€ ì•„ë‹Œ í† í°ë“¤ ì¤‘ ë§ˆì§€ë§‰ 2ê°œë¥¼ ì‚¬ìš©
    non_numeric = [t for t in tokens if not t.isdigit() and not re.fullmatch(r"\(\d+\)", t)]
    store_tokens = non_numeric[-2:] if len(non_numeric) >= 2 else non_numeric
    store_name = " ".join(store_tokens).strip()

    info = {"stores_name": store_name, "store_names": store_name}

    # store_id ì¶”ì •: ìˆ«ì ì¤‘ ë‚ ì§œ(8ìë¦¬) ì œì™¸ ë§ˆì§€ë§‰ ìˆ«ì
    store_id_candidates = [t for t in tokens if t.isdigit() and len(t) != 8]
    if store_id_candidates:
        info["store_id"] = store_id_candidates[-1]

    return info

# ============================================================
# ë²”ìš© ì¬ì—…ë¡œë“œ ë¡œë” (í•µì‹¬ í•¨ìˆ˜)
# ============================================================
def load_reupload_generic(
    file_pattern: str,
    xcom_key: str,
    search_paths: list,
    fallback_func: callable,
    dedup_key: list = None,
    source_info_extractor=None,
    **context
):
    """
    ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤ë§ˆíŠ¸ ë¡œë”
    
    Args:
        file_pattern: íŒŒì¼ íŒ¨í„´ (ì˜ˆ: 'coupangeats_coupon_*.csv')
        xcom_key: XCom ì €ì¥ í‚¤
        search_paths: ê²€ìƒ‰í•  ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        fallback_func: íŒŒì¼ ì—†ì„ ë•Œ í˜¸ì¶œí•  í•¨ìˆ˜
        dedup_key: ì¤‘ë³µ ì œê±° í‚¤ (ì›ë³¸ ì»¬ëŸ¼ ê¸°ì¤€)
    """
    all_files = []
    
    # ëª¨ë“  ê²½ë¡œì—ì„œ íŒŒì¼ ì°¾ê¸°
    for path_str in search_paths:
        search_path = Path(path_str)
        if search_path.exists():
            found_files = list(search_path.glob(file_pattern))
            if found_files:
                print(f"[{Path(path_str).name}] {len(found_files)}ê°œ íŒŒì¼ ë°œê²¬")
                all_files.extend(found_files)
    
    if all_files:
        print(f"[âœ… ì¬ì‚¬ìš©] ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ğŸ¯ ì¤‘ë³µ íŒŒì¼ ì œê±° (íŒŒì¼ëª… ê¸°ì¤€, ìµœì‹  ìš°ì„ )
        unique_files = {}
        for f in all_files:
            fname = f.name
            if fname not in unique_files or f.stat().st_mtime > unique_files[fname].stat().st_mtime:
                unique_files[fname] = f
        
        file_paths = list(unique_files.values())
        print(f"[ì¤‘ë³µ ì œê±°] {len(file_paths)}ê°œ íŒŒì¼ ì‚¬ìš©")
        
        # load_data í˜¸ì¶œ
        return load_data(
            file_path=file_paths,
            xcom_key=xcom_key,
            use_glob=False,
            dedup_key=dedup_key,
            add_source_info=source_info_extractor is not None,
            source_info_extractor=source_info_extractor,
            **context
        )
    else:
        print(f"[ğŸ”„ ìƒˆë¡œ ë¡œë“œ] ëª¨ë“  ê²½ë¡œì—ì„œ íŒŒì¼ ì—†ìŒ â†’ fallback í•¨ìˆ˜ í˜¸ì¶œ")
        return fallback_func(**context)


# ============================================================
# ì¿ íŒ¡ì´ì¸  ì¿ í° ë°ì´í„° ë¡œë”
# ============================================================
def load_reupload_coupang_coupon(**context):
    """ì¿ íŒ¡ì´ì¸  ì¿ í° ë°ì´í„° ìŠ¤ë§ˆíŠ¸ ë¡œë”"""
    return load_reupload_generic(
        file_pattern='coupangeats_coupon_*.csv',
        xcom_key='coupang_coupon_path',
        search_paths=[
            PATH_BACKUP,
            str(COLLECT_DB / "ì „ëµê¸°íšíŒ€_ìˆ˜ì§‘"),
            str(COLLECT_DB / "ì „ëµê¸°íšë¶€_ìˆ˜ì§‘")
        ],
        fallback_func=load_coupang_coupon_df,
        dedup_key=None,  # âœ… ì¤‘ë³µì œê±° í•´ì œ: ëª¨ë“  ì›ë³¸ í–‰ ìœ ì§€
        source_info_extractor=extract_coupon_info,
        **context
    )


def load_coupang_coupon_df(**context):
    """ì¿ íŒ¡ì´ì¸  ì¿ í° ì›ë³¸ ë¡œë“œ (fallbackìš©)"""
    return load_data(
        file_path=PATH_COUPON,
        xcom_key='coupang_coupon_path',
        use_glob=True,
        dedup_key=["store_id", "ë‚ ì§œ"],
        add_source_info=True, # source_info_extractor=extract_coupon_info,
        **context
    )


# ============================================================
# ì „ì²˜ë¦¬ í•¨ìˆ˜ (í•„ìš”ì‹œ ì¶”ê°€)
# ============================================================
def preprocess_coupang_coupon_df(**context):
    """ì¿ íŒ¡ì´ì¸  ì¿ í° ë°ì´í„° ì „ì²˜ë¦¬"""
    ti = context['task_instance']
    parquet_path = ti.xcom_pull(task_ids='load_coupang_coupon', key='coupang_coupon_path')
    
    if not parquet_path:
        ti.xcom_push(key='processed_coupang_coupon_path', value=None)
        return "0ê±´ (ì…ë ¥ ë°ì´í„° ì—†ìŒ)"
    
    df = pd.read_parquet(parquet_path)
    
    print(f"[ë¡œë“œ] ì›ë³¸ ì»¬ëŸ¼: {list(df.columns)}")
    print(f"[ë¡œë“œ] ì›ë³¸ ë°ì´í„°: {len(df):,}í–‰")
    
    # â­ ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ê³ ì • (í˜•ì‹ ë³€í™˜ ì—†ì´ ì›ë³¸ ìœ ì§€)
    if 'ë‚ ì§œ' in df.columns:
        df['ë‚ ì§œ'] = df['ë‚ ì§œ'].astype(str)
        print(f"[ë‚ ì§œ ê³ ì •] ë¬¸ìì—´ íƒ€ì… ìœ ì§€ (ì˜ˆ: {df['ë‚ ì§œ'].iloc[0] if len(df) > 0 else 'N/A'})")
    
    # âœ… í•„ìš”í•œ ëª¨ë“  ì»¬ëŸ¼ ì„ íƒ (ìˆëŠ” ê²ƒë§Œ)
    desired_cols = [
        'ë§¤ì¥ëª…', 'stores_name', 'store_names',  # ë§¤ì¥ëª… (ì–´ë–¤ ì´ë¦„ì´ë“ )
        'ë‚ ì§œ',
        'ì¿ í°ì ìš©_ì£¼ë¬¸ë§¤ì¶œ',
        'ì¿ í°ë°œí–‰_ë¹„ìš©',
        'í‰ê· ì£¼ë¬¸ê¸ˆì•¡',
        'ì¡°íšŒìˆ˜ëŒ€ë¹„_ì£¼ë¬¸ìœ¨',
        'store_id'
    ]
    
    available_cols = [col for col in desired_cols if col in df.columns]
    
    # ë§¤ì¥ëª… ì»¬ëŸ¼ í†µì¼ (stores_name or store_names -> ë§¤ì¥ëª…)
    if 'ë§¤ì¥ëª…' not in df.columns:
        if 'stores_name' in df.columns:
            df['ë§¤ì¥ëª…'] = df['stores_name']
        elif 'store_names' in df.columns:
            df['ë§¤ì¥ëª…'] = df['store_names']
    
    # ìµœì¢… ì»¬ëŸ¼ ì„ íƒ
    final_cols = ['ë§¤ì¥ëª…', 'ë‚ ì§œ']
    for col in ['ì¿ í°ì ìš©_ì£¼ë¬¸ë§¤ì¶œ', 'ì¿ í°ë°œí–‰_ë¹„ìš©', 'í‰ê· ì£¼ë¬¸ê¸ˆì•¡', 'ì¡°íšŒìˆ˜ëŒ€ë¹„_ì£¼ë¬¸ìœ¨']:
        if col in df.columns:
            final_cols.append(col)
    
    df = df[final_cols]
    print(f"[ì»¬ëŸ¼ ì„ íƒ] {len(final_cols)}ê°œ: {final_cols}")
    
    output_path = TEMP_DIR / f"processed_coupang_coupon_{context['ds_nodash']}.parquet"
    TEMP_DIR.mkdir(exist_ok=True, parents=True)
    df.to_parquet(output_path, index=False)
    
    ti.xcom_push(key='processed_coupang_coupon_path', value=str(output_path))
    return f"ì „ì²˜ë¦¬: {len(df):,}í–‰"


def preprocess_col_coupang_coupon_df(
    input_task_id,
    input_xcom_key,
    output_xcom_key,
    **context
):
    """ë²”ìš© ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    import numpy as np
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(
        task_ids=input_task_id,
        key=input_xcom_key
    )
    
    df = pd.read_parquet(parquet_path)
    
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(df):,}í–‰")
    df = df[['store_names', 'ë‚ ì§œ', 'ì¿ í°ì ìš©_ì£¼ë¬¸ë§¤ì¶œ', 'ì¿ í°ë°œí–‰_ë¹„ìš©', 'í‰ê· ì£¼ë¬¸ê¸ˆì•¡',
       'ì¡°íšŒìˆ˜ëŒ€ë¹„_ì£¼ë¬¸ìœ¨']]
    df = df.rename(columns={
        'store_names': 'ë§¤ì¥ëª…',})
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    processed_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    df.to_parquet(processed_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(processed_path))
    
    return f"ì „ì²˜ë¦¬: {len(df):,}í–‰"


# ============================================================
# CSV ì €ì¥
# ============================================================
import datetime as dt
time = dt.datetime.now().strftime('%Y-%m-%d')


def coupon_fin_save_to_csv(
    input_task_id,
    input_xcom_key,
    output_csv_path=None,
    output_filename=f'coupang_coupon_merge_{time}.csv',
    output_subdir=PATH_FIN_COUPON,
    dedup_key=None,
    **context
):
    """Parquet ë°ì´í„°ë¥¼ ë¡œì»¬ DBì— CSVë¡œ ì €ì¥"""
    import os
    import shutil
    import tempfile
    from modules.transform.utility.paths import LOCAL_DB
    
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(task_ids=input_task_id, key=input_xcom_key)
    
    if not parquet_path:
        print(f"[ê²½ê³ ] ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: ë°ì´í„° ì—†ìŒ"
    
    if not os.path.exists(parquet_path):
        print(f"[ê²½ê³ ] íŒŒì¼ ê²½ë¡œ ì—†ìŒ: {parquet_path}")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: íŒŒì¼ ì—†ìŒ"
    
    df = pd.read_parquet(parquet_path)
    
    print(f"\n{'='*60}")
    print(f"[ì…ë ¥] ë°ì´í„°: {len(df):,}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")
    
    # ì¤‘ë³µ ì œê±°
    if dedup_key:
        dedup_cols = [dedup_key] if isinstance(dedup_key, str) else dedup_key
        valid_cols = [c for c in dedup_cols if c in df.columns]
        if valid_cols:
            before = len(df)
            df.drop_duplicates(subset=valid_cols, keep='first', inplace=True)
            after = len(df)
            if before - after > 0:
                print(f"\n[ì¤‘ë³µ ì œê±°] {valid_cols} ê¸°ì¤€: {before - after:,}ê±´ ì œê±°")
    
    # ì¶œë ¥ ê²½ë¡œ
    if output_csv_path:
        local_csv_path = Path(output_csv_path)
    else:
        output_dir = LOCAL_DB / output_subdir
        output_dir.mkdir(parents=True, exist_ok=True)
        local_csv_path = output_dir / output_filename
    
    local_csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\n[ê²½ë¡œ] ì €ì¥ ìœ„ì¹˜: {local_csv_path}")
    
    # datetime ë³€í™˜
    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
    if datetime_cols:
        for col in datetime_cols:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S').fillna('')
    
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(
            mode='w', 
            delete=False, 
            dir=str(local_csv_path.parent),
            prefix='tmp_', 
            suffix='.csv', 
            encoding='utf-8-sig'
        ) as tmp_file:
            tmp_path = tmp_file.name
        
        df.to_csv(tmp_path, index=False, encoding='utf-8-sig')
        
        if local_csv_path.exists():
            backup_path = local_csv_path.parent / f"{local_csv_path.name}.bak"
            shutil.copy2(local_csv_path, backup_path)
            shutil.move(tmp_path, str(local_csv_path))
            backup_path.unlink()
        else:
            shutil.move(tmp_path, str(local_csv_path))
        
        csv_size = local_csv_path.stat().st_size / (1024 * 1024)
        print(f"[ì €ì¥] âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´ ({csv_size:.2f} MB)")
        
    except Exception as e:
        print(f"[ì—ëŸ¬] CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        if tmp_path and os.path.exists(tmp_path):
            os.remove(tmp_path)
        return f"ì €ì¥ ì‹¤íŒ¨: {e}"
    
    print(f"{'='*60}\n")
    return f"âœ… ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´"




def move_original_coupang_files(**context):
    """ìˆ˜ì§‘í–ˆë˜ ì›ë³¸ CSV íŒŒì¼ì„ ì—…ë¡œë“œ_tempë¡œ ì´ë™"""
    import shutil
    from pathlib import Path
    
    # ìˆ˜ì§‘ ë””ë ‰í† ë¦¬ì—ì„œ ì›ë³¸ íŒŒì¼ ì°¾ê¸°
    source_dir = COLLECT_DB / "ì „ëµê¸°íšíŒ€_ìˆ˜ì§‘"
    dest_dir = Path("/opt/airflow/download/ì—…ë¡œë“œ_temp/strategic_coupangeats_coupon")
    
    if not source_dir.exists():
        print(f"[ê²½ê³ ] ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ì—†ìŒ: {source_dir}")
        return "0ê±´ (ë””ë ‰í† ë¦¬ ì—†ìŒ)"
    
    # íŒŒì¼ ì°¾ê¸°
    files = list(source_dir.glob("coupangeats_coupon_*.csv"))
    print(f"[ì°¾ìŒ] {len(files)}ê°œ íŒŒì¼: {[f.name for f in files]}")
    
    # ëª©ì ì§€ ìƒì„±
    dest_dir.mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ ì´ë™
    moved_count = 0
    for f in files:
        try:
            dest_file = dest_dir / f.name
            shutil.move(str(f), str(dest_file))
            print(f"[âœ… ì´ë™] {f.name}")
            moved_count += 1
        except Exception as e:
            print(f"[âŒ ì‹¤íŒ¨] {f.name}: {e}")
    
    return f"{moved_count}ê°œ íŒŒì¼ ì´ë™ ì™„ë£Œ"


