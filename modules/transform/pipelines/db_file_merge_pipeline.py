"""
DB íŒŒì¼ ë³‘í•© íŒŒì´í”„ë¼ì¸

ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥:
  1. ì—¬ëŸ¬ ê²½ë¡œì—ì„œ CSV/ì—‘ì…€ íŒŒì¼ ìë™ íƒìƒ‰ ë° ë³‘í•©
  2. ì¤‘ë³µ íŒŒì¼ ìë™ ì œê±° (ìµœì‹  íŒŒì¼ ìš°ì„ )
  3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì§‘ê³„
  4. Surrogate Key ìƒì„±
  5. ë¡œì»¬ DBì— CSV ì €ì¥

ğŸ”§ ê³µìš© í•¨ìˆ˜ë“¤:
  - add_surrogate_key(): ìì—°í‚¤ ê¸°ë°˜ í•´ì‹œ í‚¤ ìƒì„±
  - get_unique_files(): ì¤‘ë³µ íŒŒì¼ëª… ì œê±°
  - load_and_concat_csv(): CSV íŒŒì¼ ë³‘í•©
  - load_and_concat_excel(): ì—‘ì…€ íŒŒì¼ ë³‘í•©
  - save_to_parquet(): Parquet ì €ì¥
  - extract_date_from_filename(): íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ

ğŸ“Œ ìƒˆë¡œìš´ ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€í•˜ëŠ” ë°©ë²•:
  1. ì´ íŒŒì¼ì˜ ê³µìš© í•¨ìˆ˜ë“¤ í™œìš©
  2. load_df() í•¨ìˆ˜ íŒ¨í„´ ì°¸ê³ í•˜ì—¬ ìƒˆ í•¨ìˆ˜ ì‘ì„±
  3. DAG íŒŒì¼ì—ì„œ ìƒˆ í•¨ìˆ˜ import ë° Task ì¶”ê°€

ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œëŠ” ê° í•¨ìˆ˜ docstring ì°¸ê³ 
"""

import pandas as pd
import numpy as np
from pathlib import Path
import hashlib
import glob

from modules.load.load_df_glob import load_data
from modules.transform.utility.paths import TEMP_DIR, LOCAL_DB, COLLECT_DB
from modules.transform.utility.io2 import load_and_concat_csv, save_to_parquet


# ============================================================
# ê²½ë¡œ ì„¤ì •
# ============================================================
PATH_TOORDER = "/opt/airflow/download/ì—…ë¡œë“œ_temp"
PATH_BACKUP = LOCAL_DB / "ì˜ì—…ê´€ë¦¬ë¶€_DB"


# ============================================================
# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def add_surrogate_key(df: pd.DataFrame, natural_key_cols: list[str]) -> pd.DataFrame:
    """
    ìì—°í‚¤ ê¸°ë°˜ Surrogate Key ìƒì„±
    
    Args:
        df: DataFrame
        natural_key_cols: ìì—°í‚¤ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        'key' ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
        
    Example:
        df = add_surrogate_key(df, natural_key_cols=["ì¼ì", "ì£¼ë¬¸ë²ˆí˜¸"])
    """
    out = df.copy()
    key_parts = [df[col].astype(str) for col in natural_key_cols]
    uk_series = pd.concat(key_parts, axis=1).agg('|'.join, axis=1)
    out['key'] = uk_series.apply(
        lambda s: hashlib.sha1(s.encode('utf-8')).hexdigest()[:16]
    )
    cols = ['key'] + [c for c in out.columns if c != 'key']
    out = out[cols]
    return out


def get_unique_files(file_list):
    """
    íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ ì œê±° (ê°™ì€ ì´ë¦„ì´ë©´ ìµœì‹  íŒŒì¼ë§Œ ìœ ì§€)
    
    Args:
        file_list: Path ê°ì²´ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì¤‘ë³µ ì œê±°ëœ Path ê°ì²´ ë¦¬ìŠ¤íŠ¸
        
    Example:
        >>> files = [
        ...     Path('/path1/data_20260101.csv'),
        ...     Path('/path2/data_20260101.csv'),  # ê°™ì€ ì´ë¦„ (ë” ìµœì‹ )
        ...     Path('/path1/data_20260102.csv'),
        ... ]
        >>> unique = get_unique_files(files)
        >>> # ê²°ê³¼: path2ì˜ data_20260101.csv + data_20260102.csv
    """
    unique_files = {}
    for f in file_list:
        fname = f.name
        if fname not in unique_files or f.stat().st_mtime > unique_files[fname].stat().st_mtime:
            unique_files[fname] = f
    return list(unique_files.values())


def load_and_concat_csv(file_paths):
    """
    CSV íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ë³‘í•©
    
    Args:
        file_paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë³‘í•©ëœ DataFrame
        
    Example:
        >>> files = [
        ...     Path('/data/sales_2026_01.csv'),
        ...     Path('/data/sales_2026_02.csv'),
        ... ]
        >>> df = load_and_concat_csv(files)
        >>> print(len(df))  # ë‘ íŒŒì¼ì˜ ì´ í–‰ ìˆ˜
    """
    dfs = []
    for fpath in file_paths:
        print(f"   ì½ëŠ” ì¤‘: {fpath}")
        df = pd.read_csv(fpath)
        dfs.append(df)
        print(f"   âœ“ {len(df)}í–‰ ë¡œë“œ")
    
    result_df = pd.concat(dfs, ignore_index=True)
    print(f"ë³‘í•© ì™„ë£Œ: {len(result_df):,}í–‰")
    return result_df


def load_and_concat_excel(file_paths, header=3, date_extractor=None):
    """
    ì—‘ì…€ íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ë³‘í•©
    
    Args:
        file_paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        header: í—¤ë” í–‰ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘, ê¸°ë³¸ê°’ 3)
        date_extractor: íŒŒì¼ëª…ì—ì„œ ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ì„ íƒ)
        
    Returns:
        ë³‘í•©ëœ DataFrame
        
    Example:
        >>> files = [Path('/data/report_20260101.xlsx')]
        >>> df = load_and_concat_excel(
        ...     files, 
        ...     header=2,  # 3ë²ˆì§¸ í–‰ì´ í—¤ë”
        ...     date_extractor=extract_date_from_filename
        ... )
        >>> print(df.columns)  # date ì»¬ëŸ¼ ìë™ ì¶”ê°€ë¨
    """
    dfs = []
    for fpath in file_paths:
        print(f"   ì½ëŠ” ì¤‘: {fpath}")
        df = pd.read_excel(fpath, header=header)
        
        if date_extractor:
            file_name = Path(fpath).name
            date_info = date_extractor(file_name)
            df['date'] = date_info['date']
        
        dfs.append(df)
        print(f"   âœ“ {len(df)}í–‰ ë¡œë“œ")
    
    result_df = pd.concat(dfs, ignore_index=True)
    print(f"ë³‘í•© ì™„ë£Œ: {len(result_df):,}í–‰")
    return result_df


def save_to_parquet(df, context, filename_prefix):
    """
    DataFrameì„ Parquetìœ¼ë¡œ ì €ì¥
    
    Args:
        df: ì €ì¥í•  DataFrame
        context: Airflow context (ds_nodash í¬í•¨)
        filename_prefix: íŒŒì¼ëª… prefix
        
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (Path ê°ì²´)
        
    Example:
        >>> # Airflow Task ë‚´ë¶€ì—ì„œ
        >>> output_path = save_to_parquet(
        ...     df, 
        ...     context, 
        ...     "sales_data_raw"
        ... )
        >>> # ê²°ê³¼: /tmp/sales_data_raw_20260119.parquet
        >>> context['task_instance'].xcom_push(
        ...     key='data_path', 
        ...     value=str(output_path)
        ... )
    """
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    output_path = temp_dir / f"{filename_prefix}_{context['ds_nodash']}.parquet"
    df.to_parquet(output_path, index=False, engine='pyarrow')
    return output_path


def extract_date_from_filename(file_name):
    """
    íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ë§ˆì§€ë§‰ ì–¸ë”ìŠ¤ì½”ì–´ ë’¤ì˜ ìˆ«ì)
    
    Args:
        file_name: íŒŒì¼ëª… (str)
        
    Returns:
        {'date': ë‚ ì§œë¬¸ìì—´} ë˜ëŠ” {'date': None}
        
    Example:
        >>> extract_date_from_filename('toorder_review_20260119.csv')
        {'date': '20260119'}
        >>> extract_date_from_filename('data_v2_20260101.xlsx')
        {'date': '20260101'}
    """
    try:
        date_str = file_name.split('_')[-1].split('.')[0]
        return {'date': date_str}
    except Exception as e:
        print(f"[ê²½ê³ ] ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {file_name}, {e}")
        return {'date': None}



# ============================================================
# ë©”ì¸ ë¡œë“œ í•¨ìˆ˜
# ============================================================
def load_df(**context):
    """
    ìŠ¤ë§ˆíŠ¸ í† ë” ë¦¬ë·° ë¡œë” (ì—¬ëŸ¬ ì†ŒìŠ¤ ìë™ ë³‘í•©)
    
    ğŸ“‚ ê²€ìƒ‰ ê²½ë¡œ:
      1. /opt/airflow/download/ì—…ë¡œë“œ_temp/toorder_review_*.csv
      2. ì›ë“œë¼ì´ë¸Œ/ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘/toorder_review_*.csv
      3. /opt/airflow/download/toorder_review_doridang1_*.xlsx (CSV ì—†ì„ ë•Œ)
    
    ğŸ”„ ì²˜ë¦¬ íë¦„:
      - CSV íŒŒì¼ ìš°ì„  ê²€ìƒ‰ (ë¹ ë¦„)
      - ì¤‘ë³µ íŒŒì¼ëª…ì€ ìµœì‹  íŒŒì¼ë§Œ ì„ íƒ
      - ë³‘í•© í›„ Parquetìœ¼ë¡œ ì €ì¥
      - XComì— ê²½ë¡œ ì €ì¥
    
    Args:
        **context: Airflow context (ds_nodash, task_instance í¬í•¨)
    
    Returns:
        str: ì²˜ë¦¬ ê²°ê³¼ ë©”ì‹œì§€
    
    XCom ì¶œë ¥:
        key='toorder_review_path': Parquet íŒŒì¼ ê²½ë¡œ
    
    ğŸ’¡ ìƒˆ ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€ ì˜ˆì‹œ:
        # 1. ì´ í•¨ìˆ˜ ë³µì‚¬ í›„ ì´ë¦„ ë³€ê²½
        def load_my_data(**context):
            upload_path = Path('/opt/airflow/download/ì—…ë¡œë“œ_temp')
            
            # 2. glob íŒ¨í„´ ìˆ˜ì •
            csv_files = list(upload_path.glob('my_data_*.csv'))
            
            # 3. ê³µìš© í•¨ìˆ˜ í™œìš©
            if csv_files:
                file_paths = get_unique_files(csv_files)
                result_df = load_and_concat_csv(file_paths)
                output_path = save_to_parquet(result_df, context, "my_data_raw")
                
                # 4. XComì— ì €ì¥
                context['task_instance'].xcom_push(
                    key='my_data_path',
                    value=str(output_path)
                )
                return f"âœ… {len(result_df):,}ê±´"
            
            return "0ê±´ (íŒŒì¼ ì—†ìŒ)"
    """
    upload_temp_path = Path('/opt/airflow/download/ì—…ë¡œë“œ_temp')
    download_path = Path('/opt/airflow/download')
    onedrive_path = COLLECT_DB / "ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘"
    
    # 1. CSV íŒŒì¼ ì°¾ê¸°
    csv_files = []
    
    if upload_temp_path.exists():
        csv_files.extend(list(upload_temp_path.glob('toorder_review_*.csv')))
    
    if onedrive_path.exists():
        csv_files.extend(list(onedrive_path.glob('toorder_review_*.csv')))
    
    if csv_files:
        print(f"[âœ… CSV ì¬ì‚¬ìš©] ì´ {len(csv_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        file_paths = get_unique_files(csv_files)
        print(f"[ì¤‘ë³µ ì œê±°] {len(file_paths)}ê°œ CSV ì‚¬ìš©")
        
        result_df = load_and_concat_csv(file_paths)
        output_path = save_to_parquet(result_df, context, "toorder_review_raw")
        
        context['task_instance'].xcom_push(
            key='toorder_review_path',
            value=str(output_path)
        )
        return f"âœ… {len(result_df):,}ê±´ (CSV ì¬ì‚¬ìš©)"
    
    # 2. CSV ì—†ìœ¼ë©´ ì—‘ì…€ ì°¾ê¸°
    excel_files = []
    
    if upload_temp_path.exists():
        excel_files.extend(list(upload_temp_path.glob('toorder_review_doridang1_*.xlsx')))
    
    if download_path.exists():
        excel_files.extend(list(download_path.glob('toorder_review_doridang1_*.xlsx')))
    
    if excel_files:
        print(f"[âœ… ì—‘ì…€ ë¡œë“œ] ì´ {len(excel_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        file_paths = get_unique_files(excel_files)
        print(f"[ì¤‘ë³µ ì œê±°] {len(file_paths)}ê°œ íŒŒì¼ ì‚¬ìš©")
        
        result_df = load_and_concat_excel(file_paths, header=3, date_extractor=extract_date_from_filename)
        output_path = save_to_parquet(result_df, context, "toorder_review_raw")
        
        context['task_instance'].xcom_push(
            key='toorder_review_path',
            value=str(output_path)
        )
        return f"âœ… {len(result_df):,}ê±´ (ì—‘ì…€ ë¡œë“œ)"
    
    # 3. íŒŒì¼ ì—†ìŒ
    print(f"[âŒ ì—ëŸ¬] í† ë” ë¦¬ë·° íŒŒì¼ ì—†ìŒ")
    context['task_instance'].xcom_push(key='toorder_review_path', value=None)
    return "0ê±´ (íŒŒì¼ ì—†ìŒ)"

# ============================================================
# ê¸°ì¡´ í•¨ìˆ˜ (í˜¸í™˜ì„± ìœ ì§€)
# ============================================================
def load_toorder_review_df(**context):
    """í† ë” ë¦¬ë·° ì—‘ì…€ ë¡œë“œ (ë ˆê±°ì‹œ í•¨ìˆ˜ - load_df ì‚¬ìš© ê¶Œì¥)"""
    ti = context['task_instance']
    
    file_list = sorted(glob.glob(str(PATH_TOORDER)))
    print(f"[ë¡œë“œ] ì°¾ì€ íŒŒì¼: {len(file_list)}ê°œ")
    
    if not file_list:
        ti.xcom_push(key='toorder_review_path', value=None)
        return "0ê±´ (íŒŒì¼ ì—†ìŒ)"
    
    file_paths = [Path(f) for f in file_list]
    result_df = load_and_concat_excel(file_paths, header=3, date_extractor=extract_date_from_filename)
    output_path = save_to_parquet(result_df, context, "toorder_review_raw")
    
    ti.xcom_push(key='toorder_review_path', value=str(output_path))
    return f"{len(result_df):,}ê±´"


def preprocess_toorder_review_df(
    input_task_id,
    input_xcom_key,
    output_xcom_key,
    **context
):
    """
    í† ë” ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬ (ì§‘ê³„ + í‘œì¤€í™”)
    
    ğŸ”„ ì²˜ë¦¬ ë‹¨ê³„:
      1. ì»¬ëŸ¼ ì„ íƒ: date, ë§¤ì¥ëª….1, ì±„ë„, ì£¼ë¬¸ ìˆ˜, ë¦¬ë·° ìˆ˜, ë‹µë³€ì™„ë£Œ ìˆ˜, í‰ê·  ë³„ì 
      2. ê²°ì¸¡ì¹˜ ì œê±°: ì±„ë„ì´ nullì¸ í–‰ ì œê±°
      3. ê·¸ë£¹í™”: date + ë§¤ì¥ëª….1 ê¸°ì¤€ ì§‘ê³„
      4. ID ìƒì„±: date + stores_name ê¸°ë°˜ í•´ì‹œ í‚¤
      5. ë§¤ì¥ëª… í‘œì¤€í™”: "ë„ë¦¬ë‹¹" ì ‘ë‘ì‚¬ ì¶”ê°€, ë³„ì¹­ í†µì¼
    
    Args:
        input_task_id: ì´ì „ Taskì˜ task_id
        input_xcom_key: ì½ì„ XCom í‚¤
        output_xcom_key: ì €ì¥í•  XCom í‚¤
        **context: Airflow context
    
    Returns:
        str: ì²˜ë¦¬ ê²°ê³¼ ë©”ì‹œì§€
    
    XCom:
        ì…ë ¥: key=input_xcom_key (Parquet ê²½ë¡œ)
        ì¶œë ¥: key=output_xcom_key (ì „ì²˜ë¦¬ëœ Parquet ê²½ë¡œ)
    
    ğŸ’¡ DAGì—ì„œ ì‚¬ìš© ì˜ˆì‹œ:
        task_preprocess = PythonOperator(
            task_id='preprocess_data',
            python_callable=preprocess_toorder_review_df,
            op_kwargs={
                'input_task_id': 'load_data',             # ì´ì „ Task
                'input_xcom_key': 'raw_data_path',        # ì½ì„ í‚¤
                'output_xcom_key': 'processed_data_path', # ì €ì¥í•  í‚¤
            }
        )
    """
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(
        task_ids=input_task_id,
        key=input_xcom_key
    )
    
    if not parquet_path:
        print(f"[ê²½ê³ ] í† ë” ë¦¬ë·° ë°ì´í„° ì—†ìŒ - ìŠ¤í‚µ")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "0ê±´ (ì…ë ¥ ë°ì´í„° ì—†ìŒ)"
    
    toorder_review_df = pd.read_parquet(parquet_path)
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(toorder_review_df):,}í–‰")
    
    # ì»¬ëŸ¼ ì„ íƒ
    col = ['date', 'ë§¤ì¥ëª….1', 'ì±„ë„', 'ì£¼ë¬¸ ìˆ˜', 'ë¦¬ë·° ìˆ˜', 'ë‹µë³€ì™„ë£Œ ìˆ˜', 'í‰ê·  ë³„ì ']
    toorder_review_df = toorder_review_df[col]
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    toorder_review_df = toorder_review_df[~toorder_review_df["ì±„ë„"].isnull()]
    
    # 0ì  ì œì™¸ í‰ê·  í•¨ìˆ˜
    def mean_excluding_zero(x):
        non_zero = x[x > 0]
        return non_zero.mean() if len(non_zero) > 0 else 0
    
    # ê·¸ë£¹í™”
    toorder_review_df = toorder_review_df.groupby(["date", "ë§¤ì¥ëª….1"]).agg(
        ì „ì²´_ì£¼ë¬¸ìˆ˜=("ì£¼ë¬¸ ìˆ˜", "sum"),
        ì „ì²´_ë¦¬ë·°ìˆ˜=("ë¦¬ë·° ìˆ˜", "sum"),
        ì „ì²´_ë‹µë³€ì™„ë£Œìˆ˜=("ë‹µë³€ì™„ë£Œ ìˆ˜", "sum"),
        ì „ì²´_í‰ê· ë³„ì =("í‰ê·  ë³„ì ", mean_excluding_zero)
    ).reset_index()
    
    # ë§¤ì¥ëª… ì •ë¦¬
    toorder_review_df.rename(columns={"ë§¤ì¥ëª….1": "stores_name"}, inplace=True)
    
    # Surrogate key ì¶”ê°€ (ì§‘ê³„ í›„ ìµœì¢… ë°ì´í„°ì— ëŒ€í•´)
    toorder_review_df = add_surrogate_key(toorder_review_df, natural_key_cols=["date", "stores_name"])
    toorder_review_df.rename(columns={'key': 'id'}, inplace=True)
    toorder_review_df["stores_name"] = "ë„ë¦¬ë‹¹ " + toorder_review_df["stores_name"]
    toorder_review_df["stores_name"] = toorder_review_df["stores_name"].replace({
        "ë„ë¦¬ë‹¹ ì¼ì‚°ë°±ì„ì ": "ë„ë¦¬ë‹¹ ë°±ì„ì ",
        "ë„ë¦¬ë‹¹ ì„œìš¸ëŒ€ì ": "ë„ë¦¬ë‹¹ ì„œìš¸ëŒ€ì ",
        "ë„ë¦¬ë‹¹ êµ¬ë¡œë””ì§€í„¸ë‹¨ì§€ì ": "ë„ë¦¬ë‹¹ êµ¬ë¡œë””ì§€í„¸ì ",
        "ë„ë¦¬ë‹¹ ì¶©ì£¼ë´‰ë°©ì ": "ë„ë¦¬ë‹¹ ì¶©ì£¼ì—­ì "
    })
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(toorder_review_df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    processed_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    toorder_review_df.to_parquet(processed_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(processed_path))
    
    return f"ì „ì²˜ë¦¬: {len(toorder_review_df):,}í–‰"

# ============================================================
# CSVë¡œ ì €ì¥
# ============================================================
def fin_save_to_csv(
    input_task_id,
    input_xcom_key,
    output_csv_path=None,
    output_filename='toorder_review_doridang.csv',
    output_subdir='ì˜ì—…ê´€ë¦¬ë¶€_DB',
    dedup_key=None,
    **context
):
    """
    Parquet ë°ì´í„°ë¥¼ ë¡œì»¬ DBì— CSVë¡œ ì €ì¥ (ì•ˆì „í•œ ì›ìì  ì“°ê¸°)
    
    ğŸ’¾ ì£¼ìš” ê¸°ëŠ¥:
      - Parquetì„ CSVë¡œ ë³€í™˜
      - ê¸°ì¡´ íŒŒì¼ ìë™ ë°±ì—… (.bak)
      - ì„ì‹œ íŒŒì¼ë¡œ ì“´ í›„ ì›ìì  êµì²´
      - datetime ì»¬ëŸ¼ ìë™ í¬ë§·íŒ…
      - ì¤‘ë³µ ì œê±° (ì„ íƒ)
    
    Args:
        input_task_id: ì´ì „ Taskì˜ task_id
        input_xcom_key: ì½ì„ XCom í‚¤
        output_csv_path: ì§ì ‘ ê²½ë¡œ ì§€ì • (ì„ íƒ)
        output_filename: CSV íŒŒì¼ëª… (ê¸°ë³¸ê°’: 'toorder_review_doridang.csv')
        output_subdir: LOCAL_DB í•˜ìœ„ í´ë” (ê¸°ë³¸ê°’: 'ì˜ì—…ê´€ë¦¬ë¶€_DB')
        dedup_key: ì¤‘ë³µ ì œê±° í‚¤ (str ë˜ëŠ” list, ì„ íƒ)
        **context: Airflow context
    
    Returns:
        str: ì €ì¥ ê²°ê³¼ ë©”ì‹œì§€
    
    XCom ì…ë ¥:
        key=input_xcom_key: Parquet íŒŒì¼ ê²½ë¡œ
    
    ğŸ’¡ DAGì—ì„œ ì‚¬ìš© ì˜ˆì‹œ 1 (ê¸°ë³¸):
        task_save = PythonOperator(
            task_id='save_csv',
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': 'preprocess_data',
                'input_xcom_key': 'processed_data_path',
                'output_filename': 'my_data.csv',
            }
        )
        # ê²°ê³¼: LOCAL_DB/ì˜ì—…ê´€ë¦¬ë¶€_DB/my_data.csv
    
    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ 2 (ì¤‘ë³µ ì œê±° + ì»¤ìŠ¤í…€ í´ë”):
        task_save = PythonOperator(
            task_id='save_csv',
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': 'preprocess_data',
                'input_xcom_key': 'processed_data_path',
                'output_filename': 'unique_sales.csv',
                'output_subdir': 'ë§¤ì¶œ_ë°ì´í„°/ì›”ë³„',
                'dedup_key': ['date', 'order_id'],  # ì¤‘ë³µ ì œê±° í‚¤
            }
        )
        # ê²°ê³¼: LOCAL_DB/ë§¤ì¶œ_ë°ì´í„°/ì›”ë³„/unique_sales.csv
    
    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ 3 (ì ˆëŒ€ ê²½ë¡œ ì§€ì •):
        task_save = PythonOperator(
            task_id='save_csv',
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': 'preprocess_data',
                'input_xcom_key': 'processed_data_path',
                'output_csv_path': '/custom/path/data.csv',
            }
        )
    """
    import os
    import shutil
    import tempfile
    from modules.transform.utility.paths import LOCAL_DB
    
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(task_ids=input_task_id, key=input_xcom_key)
    
    if not parquet_path:
        print(f"[ê²½ê³ ] ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: ë°ì´í„° ì—†ìŒ"
    
    if not os.path.exists(parquet_path):
        print(f"[ê²½ê³ ] íŒŒì¼ ê²½ë¡œ ì—†ìŒ: {parquet_path}")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: íŒŒì¼ ì—†ìŒ"
    
    df = pd.read_parquet(parquet_path)
    
    print(f"\n{'='*60}")
    print(f"[ì…ë ¥] ë°ì´í„°: {len(df):,}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")
    
    # ì¤‘ë³µ ì œê±°
    if dedup_key:
        dedup_cols = [dedup_key] if isinstance(dedup_key, str) else dedup_key
        valid_cols = [c for c in dedup_cols if c in df.columns]
        if valid_cols:
            before = len(df)
            df.drop_duplicates(subset=valid_cols, keep='first', inplace=True)
            after = len(df)
            if before - after > 0:
                print(f"\n[ì¤‘ë³µ ì œê±°] {valid_cols} ê¸°ì¤€: {before - after:,}ê±´ ì œê±°")
    
    # ì¶œë ¥ ê²½ë¡œ
    if output_csv_path:
        local_csv_path = Path(output_csv_path)
    else:
        output_dir = LOCAL_DB / output_subdir
        output_dir.mkdir(parents=True, exist_ok=True)
        local_csv_path = output_dir / output_filename
    
    local_csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\n[ê²½ë¡œ] ì €ì¥ ìœ„ì¹˜: {local_csv_path}")
    
    # datetime ë³€í™˜
    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
    if datetime_cols:
        for col in datetime_cols:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S').fillna('')
    
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(
            mode='w', 
            delete=False, 
            dir=str(local_csv_path.parent),
            prefix='tmp_', 
            suffix='.csv', 
            encoding='utf-8-sig'
        ) as tmp_file:
            tmp_path = tmp_file.name
        
        df.to_csv(tmp_path, index=False, encoding='utf-8-sig')
        
        if local_csv_path.exists():
            backup_path = local_csv_path.parent / f"{local_csv_path.name}.bak"
            shutil.copy2(local_csv_path, backup_path)
            shutil.move(tmp_path, str(local_csv_path))
            backup_path.unlink()
        else:
            shutil.move(tmp_path, str(local_csv_path))
        
        csv_size = local_csv_path.stat().st_size / (1024 * 1024)
        print(f"[ì €ì¥] âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´ ({csv_size:.2f} MB)")
        
    except Exception as e:
        print(f"[ì—ëŸ¬] CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        if tmp_path and os.path.exists(tmp_path):
            os.remove(tmp_path)
        return f"ì €ì¥ ì‹¤íŒ¨: {e}"
    
    print(f"{'='*60}\n")
    return f"âœ… ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´"


# ============================================================

# ============================================================
# ê¸°ì¡´ í•¨ìˆ˜ (í˜¸í™˜ì„± ìœ ì§€) baemin_ad_change_history_*.csv ë¡œë“œ
# ============================================================
def load_baemin_ad_change_history_df(**context):
    """ë°°ë¯¼ ë¡œë“œ (ë ˆê±°ì‹œ í•¨ìˆ˜ - load_df ì‚¬ìš© ê¶Œì¥)"""
    ti = context['task_instance']
    
    file_list = sorted(glob.glob(str(PATH_TOORDER)))
    print(f"[ë¡œë“œ] ì°¾ì€ íŒŒì¼: {len(file_list)}ê°œ")
    
    if not file_list:
        ti.xcom_push(key='baemin_ad_change_history_path', value=None)
        return "0ê±´ (íŒŒì¼ ì—†ìŒ)"
    
    file_paths = [Path(f) for f in file_list]
    result_df = load_and_concat_excel(file_paths, header=3, date_extractor=extract_date_from_filename)
    output_path = save_to_parquet(result_df, context, "baemin_ad_change_history_raw")
    
    ti.xcom_push(key='baemin_ad_change_history_path', value=str(output_path))
    return f"{len(result_df):,}ê±´"

# ============================================================
# baemin_ad_change_history_df ì „ì²˜ë¦¬
# ============================================================
def preprocess_baemin_ad_change_history_df(
    input_task_id,
    input_xcom_key,
    output_xcom_key,
    **context
):
    """
    í† ë” ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬ (ì§‘ê³„ + í‘œì¤€í™”)
    
    ğŸ”„ ì²˜ë¦¬ ë‹¨ê³„:
      1. ì»¬ëŸ¼ ì„ íƒ: date, ë§¤ì¥ëª….1, ì±„ë„, ì£¼ë¬¸ ìˆ˜, ë¦¬ë·° ìˆ˜, ë‹µë³€ì™„ë£Œ ìˆ˜, í‰ê·  ë³„ì 
      2. ê²°ì¸¡ì¹˜ ì œê±°: ì±„ë„ì´ nullì¸ í–‰ ì œê±°
      3. ê·¸ë£¹í™”: date + ë§¤ì¥ëª….1 ê¸°ì¤€ ì§‘ê³„
      4. ID ìƒì„±: date + stores_name ê¸°ë°˜ í•´ì‹œ í‚¤
      5. ë§¤ì¥ëª… í‘œì¤€í™”: "ë„ë¦¬ë‹¹" ì ‘ë‘ì‚¬ ì¶”ê°€, ë³„ì¹­ í†µì¼
    
    Args:
        input_task_id: ì´ì „ Taskì˜ task_id
        input_xcom_key: ì½ì„ XCom í‚¤
        output_xcom_key: ì €ì¥í•  XCom í‚¤
        **context: Airflow context
    
    Returns:
        str: ì²˜ë¦¬ ê²°ê³¼ ë©”ì‹œì§€
    
    XCom:
        ì…ë ¥: key=input_xcom_key (Parquet ê²½ë¡œ)
        ì¶œë ¥: key=output_xcom_key (ì „ì²˜ë¦¬ëœ Parquet ê²½ë¡œ)
    
    ğŸ’¡ DAGì—ì„œ ì‚¬ìš© ì˜ˆì‹œ:
        task_preprocess = PythonOperator(
            task_id='preprocess_data',
            python_callable=preprocess_toorder_review_df,
            op_kwargs={
                'input_task_id': 'load_data',             # ì´ì „ Task
                'input_xcom_key': 'raw_data_path',        # ì½ì„ í‚¤
                'output_xcom_key': 'processed_data_path', # ì €ì¥í•  í‚¤
            }
        )
    """
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(
        task_ids=input_task_id,
        key=input_xcom_key
    )
    
    if not parquet_path:
        print(f"[ê²½ê³ ] í† ë” ë¦¬ë·° ë°ì´í„° ì—†ìŒ - ìŠ¤í‚µ")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "0ê±´ (ì…ë ¥ ë°ì´í„° ì—†ìŒ)"
    
    df = pd.read_parquet(parquet_path)
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(df):,}í–‰")
    
    
    # Surrogate key ì¶”ê°€ (ì§‘ê³„ í›„ ìµœì¢… ë°ì´í„°ì— ëŒ€í•´)
    df = add_surrogate_key(df, natural_key_cols=["ë§¤ì¥ëª…", "ë³€ê²½ì‹œê°„"])
    df.rename(columns={'key': 'id'}, inplace=True)
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    processed_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    df.to_parquet(processed_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(processed_path))
    
    return f"ì „ì²˜ë¦¬: {len(df):,}í–‰"


# ============================================================
# csv ì €ì¥
# ===========================================================
def baemin_ad_change_history_save_to_csv(
    input_task_id,
    input_xcom_key,
    output_csv_path=None,
    output_filename='baemin_ad_change_history.csv',
    output_subdir='ì˜ì—…ê´€ë¦¬ë¶€_DB',
    dedup_key=["id"],
    **context
):
    """
    Parquet ë°ì´í„°ë¥¼ ë¡œì»¬ DBì— CSVë¡œ ì €ì¥ (ì•ˆì „í•œ ì›ìì  ì“°ê¸°)
    
    ğŸ’¾ ì£¼ìš” ê¸°ëŠ¥:
      - Parquetì„ CSVë¡œ ë³€í™˜
      - ê¸°ì¡´ íŒŒì¼ ìë™ ë°±ì—… (.bak)
      - ì„ì‹œ íŒŒì¼ë¡œ ì“´ í›„ ì›ìì  êµì²´
      - datetime ì»¬ëŸ¼ ìë™ í¬ë§·íŒ…
      - ì¤‘ë³µ ì œê±° (ì„ íƒ)
    
    Args:
        input_task_id: ì´ì „ Taskì˜ task_id
        input_xcom_key: ì½ì„ XCom í‚¤
        output_csv_path: ì§ì ‘ ê²½ë¡œ ì§€ì • (ì„ íƒ)
        output_filename: CSV íŒŒì¼ëª… (ê¸°ë³¸ê°’: 'toorder_review_doridang.csv')
        output_subdir: LOCAL_DB í•˜ìœ„ í´ë” (ê¸°ë³¸ê°’: 'ì˜ì—…ê´€ë¦¬ë¶€_DB')
        dedup_key: ì¤‘ë³µ ì œê±° í‚¤ (str ë˜ëŠ” list, ì„ íƒ)
        **context: Airflow context
    
    Returns:
        str: ì €ì¥ ê²°ê³¼ ë©”ì‹œì§€
    
    XCom ì…ë ¥:
        key=input_xcom_key: Parquet íŒŒì¼ ê²½ë¡œ
    
    ğŸ’¡ DAGì—ì„œ ì‚¬ìš© ì˜ˆì‹œ 1 (ê¸°ë³¸):
        task_save = PythonOperator(
            task_id='save_csv',
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': 'preprocess_data',
                'input_xcom_key': 'processed_data_path',
                'output_filename': 'my_data.csv',
            }
        )
        # ê²°ê³¼: LOCAL_DB/ì˜ì—…ê´€ë¦¬ë¶€_DB/my_data.csv
    
    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ 2 (ì¤‘ë³µ ì œê±° + ì»¤ìŠ¤í…€ í´ë”):
        task_save = PythonOperator(
            task_id='save_csv',
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': 'preprocess_data',
                'input_xcom_key': 'processed_data_path',
                'output_filename': 'unique_sales.csv',
                'output_subdir': 'ë§¤ì¶œ_ë°ì´í„°/ì›”ë³„',
                'dedup_key': ['date', 'order_id'],  # ì¤‘ë³µ ì œê±° í‚¤
            }
        )
        # ê²°ê³¼: LOCAL_DB/ë§¤ì¶œ_ë°ì´í„°/ì›”ë³„/unique_sales.csv
    
    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ 3 (ì ˆëŒ€ ê²½ë¡œ ì§€ì •):
        task_save = PythonOperator(
            task_id='save_csv',
            python_callable=fin_save_to_csv,
            op_kwargs={
                'input_task_id': 'preprocess_data',
                'input_xcom_key': 'processed_data_path',
                'output_csv_path': '/custom/path/data.csv',
            }
        )
    """
    import os
    import shutil
    import tempfile
    from modules.transform.utility.paths import LOCAL_DB
    
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(task_ids=input_task_id, key=input_xcom_key)
    
    if not parquet_path:
        print(f"[ê²½ê³ ] ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: ë°ì´í„° ì—†ìŒ"
    
    if not os.path.exists(parquet_path):
        print(f"[ê²½ê³ ] íŒŒì¼ ê²½ë¡œ ì—†ìŒ: {parquet_path}")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: íŒŒì¼ ì—†ìŒ"
    
    df = pd.read_parquet(parquet_path)
    
    print(f"\n{'='*60}")
    print(f"[ì…ë ¥] ë°ì´í„°: {len(df):,}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")
    
    # ì¤‘ë³µ ì œê±°
    if dedup_key:
        dedup_cols = [dedup_key] if isinstance(dedup_key, str) else dedup_key
        valid_cols = [c for c in dedup_cols if c in df.columns]
        if valid_cols:
            before = len(df)
            df.drop_duplicates(subset=valid_cols, keep='first', inplace=True)
            after = len(df)
            if before - after > 0:
                print(f"\n[ì¤‘ë³µ ì œê±°] {valid_cols} ê¸°ì¤€: {before - after:,}ê±´ ì œê±°")
    
    # ì¶œë ¥ ê²½ë¡œ
    if output_csv_path:
        local_csv_path = Path(output_csv_path)
    else:
        output_dir = LOCAL_DB / output_subdir
        output_dir.mkdir(parents=True, exist_ok=True)
        local_csv_path = output_dir / output_filename
    
    local_csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\n[ê²½ë¡œ] ì €ì¥ ìœ„ì¹˜: {local_csv_path}")
    
    # datetime ë³€í™˜
    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
    if datetime_cols:
        for col in datetime_cols:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S').fillna('')
    
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(
            mode='w', 
            delete=False, 
            dir=str(local_csv_path.parent),
            prefix='tmp_', 
            suffix='.csv', 
            encoding='utf-8-sig'
        ) as tmp_file:
            tmp_path = tmp_file.name
        
        df.to_csv(tmp_path, index=False, encoding='utf-8-sig')
        
        if local_csv_path.exists():
            backup_path = local_csv_path.parent / f"{local_csv_path.name}.bak"
            shutil.copy2(local_csv_path, backup_path)
            shutil.move(tmp_path, str(local_csv_path))
            backup_path.unlink()
        else:
            shutil.move(tmp_path, str(local_csv_path))
        
        csv_size = local_csv_path.stat().st_size / (1024 * 1024)
        print(f"[ì €ì¥] âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´ ({csv_size:.2f} MB)")
        
    except Exception as e:
        print(f"[ì—ëŸ¬] CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        if tmp_path and os.path.exists(tmp_path):
            os.remove(tmp_path)
        return f"ì €ì¥ ì‹¤íŒ¨: {e}"
    
    print(f"{'='*60}\n")
    return f"âœ… ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´"
