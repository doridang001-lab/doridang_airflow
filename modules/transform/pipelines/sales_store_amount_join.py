# modules/transform/pipelines/sales_store_amount_join.py
import pandas as pd
import numpy as np
import glob
from pathlib import Path
import pandas as pd
import numpy as np
import datetime as dt

from modules.load.load_df_glob import load_data
from modules.transform.utility.paths import TEMP_DIR, LOCAL_DB, COLLECT_DB

PATH_NOW = COLLECT_DB / "ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘" / "baemin_metrics_*.csv"
PATH_HISTORY = COLLECT_DB / "ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘" / "baemin_change_history_*.csv"

PATH_TOORDER = "/opt/airflow/download/toorder_review_doridang1_*.xlsx"
PATH_ORDERS_ALERTS = LOCAL_DB / "ì˜ì—…ê´€ë¦¬ë¶€_DB" / "sales_daily_orders_alerts.csv"


# ============================================================
# ì¬ì—…ë¡œë“œ ëª¨ë“œ ì²˜ë¦¬ í•¨ìˆ˜ (ì—…ë¡œë“œ_temp + ì›ë“œë¼ì´ë¸Œ ë™ì‹œ glob)
# ============================================================
def load_reupload_baemin_store_now(**context):
    """
    ìŠ¤ë§ˆíŠ¸ ë°°ë¯¼ ìš°ë¦¬ê°€ê²Œ ë¡œë”
    - ì—…ë¡œë“œ_temp + ì›ë“œë¼ì´ë¸Œ ë™ì‹œ glob ê²€ìƒ‰
    - ì—†ìœ¼ë©´ ì›ë³¸ ìƒˆë¡œ ë¡œë“œ
    """
    upload_temp_path = Path('/opt/airflow/download/ì—…ë¡œë“œ_temp')
    onedrive_path = COLLECT_DB / "ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘"
    
    all_files = []
    
    # ì—…ë¡œë“œ_tempì—ì„œ ì°¾ê¸°
    if upload_temp_path.exists():
        temp_files = list(upload_temp_path.glob('baemin_metrics_*.csv'))
        if temp_files:
            print(f"[ì—…ë¡œë“œ_temp] {len(temp_files)}ê°œ íŒŒì¼ ë°œê²¬")
            all_files.extend(temp_files)
    
    # ì›ë“œë¼ì´ë¸Œì—ì„œ ì°¾ê¸°
    if onedrive_path.exists():
        onedrive_files = list(onedrive_path.glob('baemin_metrics_*.csv'))
        if onedrive_files:
            print(f"[ì›ë“œë¼ì´ë¸Œ] {len(onedrive_files)}ê°œ íŒŒì¼ ë°œê²¬")
            all_files.extend(onedrive_files)
    
    if all_files:
        print(f"[âœ… ì¬ì‚¬ìš©] ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ğŸ¯ ì¤‘ë³µ íŒŒì¼ ì œê±° (íŒŒì¼ëª… ê¸°ì¤€, ìµœì‹  íŒŒì¼ ìš°ì„ )
        unique_files = {}
        for f in all_files:
            fname = f.name
            if fname not in unique_files or f.stat().st_mtime > unique_files[fname].stat().st_mtime:
                unique_files[fname] = f
        
        file_paths = list(unique_files.values())
        print(f"[ì¤‘ë³µ ì œê±°] {len(file_paths)}ê°œ íŒŒì¼ ì‚¬ìš©")
        
        # load_data í˜¸ì¶œ (ì›ë³¸ ë°ì´í„°ì— ìˆëŠ” ì»¬ëŸ¼ìœ¼ë¡œë§Œ dedup)
        return load_data(
            file_path=file_paths,
            xcom_key='baemin_store_now_path',
            use_glob=False,
            dedup_key=['store_id', 'collected_at'],  # â­ collected_date â†’ collected_at (ì›ë³¸ ì»¬ëŸ¼)
            add_source_info=False,
            **context
        )
    else:
        print(f"[ğŸ”„ ìƒˆë¡œ ë¡œë“œ] ëª¨ë“  ê²½ë¡œì—ì„œ íŒŒì¼ ì—†ìŒ â†’ ë°°ë¯¼ ì›ë³¸ ë°ì´í„° ìƒˆë¡œ ë¡œë“œ")
        return load_baemin_store_now_df(**context)


def load_reupload_baemin_history(**context):
    """
    ìŠ¤ë§ˆíŠ¸ ë°°ë¯¼ ë³€ê²½ì´ë ¥ ë¡œë”
    - ì—…ë¡œë“œ_temp + ì›ë“œë¼ì´ë¸Œ ë™ì‹œ glob ê²€ìƒ‰
    - ì—†ìœ¼ë©´ ì›ë³¸ ìƒˆë¡œ ë¡œë“œ
    """
    upload_temp_path = Path('/opt/airflow/download/ì—…ë¡œë“œ_temp')
    onedrive_path = COLLECT_DB / "ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘"
    
    all_files = []
    
    # ì—…ë¡œë“œ_tempì—ì„œ ì°¾ê¸°
    if upload_temp_path.exists():
        temp_files = list(upload_temp_path.glob('baemin_change_history_*.csv'))
        if temp_files:
            print(f"[ì—…ë¡œë“œ_temp] {len(temp_files)}ê°œ íŒŒì¼ ë°œê²¬")
            all_files.extend(temp_files)
    
    # ì›ë“œë¼ì´ë¸Œì—ì„œ ì°¾ê¸°
    if onedrive_path.exists():
        onedrive_files = list(onedrive_path.glob('baemin_change_history_*.csv'))
        if onedrive_files:
            print(f"[ì›ë“œë¼ì´ë¸Œ] {len(onedrive_files)}ê°œ íŒŒì¼ ë°œê²¬")
            all_files.extend(onedrive_files)
    
    if all_files:
        print(f"[âœ… ì¬ì‚¬ìš©] ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ğŸ¯ ì¤‘ë³µ íŒŒì¼ ì œê±°
        unique_files = {}
        for f in all_files:
            fname = f.name
            if fname not in unique_files or f.stat().st_mtime > unique_files[fname].stat().st_mtime:
                unique_files[fname] = f
        
        file_paths = list(unique_files.values())
        print(f"[ì¤‘ë³µ ì œê±°] {len(file_paths)}ê°œ íŒŒì¼ ì‚¬ìš©")
        
        # load_data í˜¸ì¶œ
        return load_data(
            file_path=file_paths,
            xcom_key='baemin_history_path',
            use_glob=False,
            dedup_key=['ë³€ê²½ì‹œê°„', "store_id"],  # â­ ì›ë³¸ ì»¬ëŸ¼
            add_source_info=False,
            **context
        )
    else:
        print(f"[ğŸ”„ ìƒˆë¡œ ë¡œë“œ] ëª¨ë“  ê²½ë¡œì—ì„œ íŒŒì¼ ì—†ìŒ â†’ ë°°ë¯¼ ë³€ê²½ì´ë ¥ ì›ë³¸ ë°ì´í„° ìƒˆë¡œ ë¡œë“œ")
        return load_baemin_history_df(**context)


def load_reupload_toorder_review(**context):
    """
    ìŠ¤ë§ˆíŠ¸ í† ë” ë¦¬ë·° ë¡œë”
    - ì—…ë¡œë“œ_temp + ì›ë“œë¼ì´ë¸Œ + download í´ë” ë™ì‹œ glob ê²€ìƒ‰
    - CSV ìš°ì„ , ì—†ìœ¼ë©´ ì—‘ì…€ ë¡œë“œ
    """
    upload_temp_path = Path('/opt/airflow/download/ì—…ë¡œë“œ_temp')
    download_path = Path('/opt/airflow/download')
    onedrive_path = COLLECT_DB / "ì˜ì—…ê´€ë¦¬ë¶€_ìˆ˜ì§‘"
    
    # 1. CSV íŒŒì¼ ì°¾ê¸°
    csv_files = []
    
    if upload_temp_path.exists():
        temp_csvs = list(upload_temp_path.glob('toorder_review_*.csv'))
        if temp_csvs:
            print(f"[ì—…ë¡œë“œ_temp] {len(temp_csvs)}ê°œ CSV ë°œê²¬")
            csv_files.extend(temp_csvs)
    
    if onedrive_path.exists():
        onedrive_csvs = list(onedrive_path.glob('toorder_review_*.csv'))
        if onedrive_csvs:
            print(f"[ì›ë“œë¼ì´ë¸Œ] {len(onedrive_csvs)}ê°œ CSV ë°œê²¬")
            csv_files.extend(onedrive_csvs)
    
    if csv_files:
        print(f"[âœ… CSV ì¬ì‚¬ìš©] ì´ {len(csv_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ğŸ¯ ì¤‘ë³µ íŒŒì¼ ì œê±°
        unique_files = {}
        for f in csv_files:
            fname = f.name
            if fname not in unique_files or f.stat().st_mtime > unique_files[fname].stat().st_mtime:
                unique_files[fname] = f
        
        file_paths = list(unique_files.values())
        print(f"[ì¤‘ë³µ ì œê±°] {len(file_paths)}ê°œ CSV ì‚¬ìš©")
        
        # CSV ì½ê¸°
        dfs = []
        for fpath in file_paths:
            print(f"   ì½ëŠ” ì¤‘: {fpath}")
            df = pd.read_csv(fpath)
            dfs.append(df)
            print(f"   âœ“ {len(df)}í–‰ ë¡œë“œ")
        
        # ë³‘í•©
        result_df = pd.concat(dfs, ignore_index=True)
        print(f"ë³‘í•© ì™„ë£Œ: {len(result_df):,}í–‰")
        
        # ğŸ¯ ì¤‘ë³µ ì œê±° (ë°ì´í„° ìˆ˜ì¤€)
        before = len(result_df)
        result_df.drop_duplicates(subset=['date', 'stores_name'], keep='last', inplace=True)
        after = len(result_df)
        if before - after > 0:
            print(f"[ì¤‘ë³µ ì œê±°] {before - after:,}ê±´ ì œê±°ë¨ â†’ {after:,}í–‰")
        
        # Parquet ì €ì¥
        temp_dir = TEMP_DIR
        temp_dir.mkdir(exist_ok=True, parents=True)
        output_path = temp_dir / f"toorder_review_raw_{context['ds_nodash']}.parquet"
        result_df.to_parquet(output_path, index=False, engine='pyarrow')
        
        context['task_instance'].xcom_push(
            key='toorder_review_path',
            value=str(output_path)
        )
        return f"âœ… {len(result_df):,}ê±´ (CSV ì¬ì‚¬ìš©)"
    
    # 2. CSV ì—†ìœ¼ë©´ ì—‘ì…€ ì°¾ê¸°
    excel_files = []
    
    if upload_temp_path.exists():
        temp_excels = list(upload_temp_path.glob('toorder_review_doridang1_*.xlsx'))
        if temp_excels:
            print(f"[ì—…ë¡œë“œ_temp] {len(temp_excels)}ê°œ ì—‘ì…€ ë°œê²¬")
            excel_files.extend(temp_excels)
    
    if download_path.exists():
        download_excels = list(download_path.glob('toorder_review_doridang1_*.xlsx'))
        if download_excels:
            print(f"[download] {len(download_excels)}ê°œ ì—‘ì…€ ë°œê²¬")
            excel_files.extend(download_excels)
    
    if excel_files:
        print(f"[âœ… ì—‘ì…€ ë¡œë“œ] ì´ {len(excel_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ğŸ¯ ì¤‘ë³µ íŒŒì¼ ì œê±°
        unique_files = {}
        for f in excel_files:
            fname = f.name
            if fname not in unique_files or f.stat().st_mtime > unique_files[fname].stat().st_mtime:
                unique_files[fname] = f
        
        file_paths = list(unique_files.values())
        print(f"[ì¤‘ë³µ ì œê±°] {len(file_paths)}ê°œ íŒŒì¼ ì‚¬ìš©")
        
        # ì—‘ì…€ ì½ê¸°
        dfs = []
        for fpath in file_paths:
            print(f"   ì½ëŠ” ì¤‘: {fpath}")
            df = pd.read_excel(fpath, header=3)
            
            # ë‚ ì§œ ì¶”ì¶œ
            file_name = Path(fpath).name
            date_info = extract_date_from_toorder_filename(file_name)
            df['date'] = date_info['date']
            
            dfs.append(df)
            print(f"   âœ“ {len(df)}í–‰ ë¡œë“œ")
        
        # ë³‘í•©
        result_df = pd.concat(dfs, ignore_index=True)
        print(f"ë³‘í•© ì™„ë£Œ: {len(result_df):,}í–‰")
        
        # Parquet ì €ì¥
        temp_dir = TEMP_DIR
        temp_dir.mkdir(exist_ok=True, parents=True)
        output_path = temp_dir / f"toorder_review_raw_{context['ds_nodash']}.parquet"
        result_df.to_parquet(output_path, index=False, engine='pyarrow')
        
        context['task_instance'].xcom_push(
            key='toorder_review_path',
            value=str(output_path)
        )
        return f"âœ… {len(result_df):,}ê±´ (ì—‘ì…€ ë¡œë“œ)"
    
    # 3. íŒŒì¼ ì—†ìŒ
    print(f"[âŒ ì—ëŸ¬] í† ë” ë¦¬ë·° íŒŒì¼ ì—†ìŒ")
    context['task_instance'].xcom_push(key='toorder_review_path', value=None)
    return "0ê±´ (íŒŒì¼ ì—†ìŒ)"


# ============================================================
# í† ë” ë¦¬ë·° í—¬í¼ í•¨ìˆ˜
# ============================================================
def extract_date_from_toorder_filename(file_name):
    """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
    try:
        date_str = file_name.split('_')[-1].split('.')[0]
        return {'date': date_str}
    except Exception as e:
        print(f"[ê²½ê³ ] ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {file_name}, {e}")
        return {'date': None}


def load_toorder_review_df(**context):
    """í† ë” ë¦¬ë·° ì—‘ì…€ ë¡œë“œ (ì›ë³¸ ê²½ë¡œ)"""
    ti = context['task_instance']
    
    file_list = sorted(glob.glob(str(PATH_TOORDER)))
    print(f"[ë¡œë“œ] ì°¾ì€ íŒŒì¼: {len(file_list)}ê°œ")
    
    if not file_list:
        ti.xcom_push(key='toorder_review_path', value=None)
        return "0ê±´ (íŒŒì¼ ì—†ìŒ)"
    
    dfs = []
    for fpath in file_list:
        print(f"   ì½ëŠ” ì¤‘: {fpath}")
        df = pd.read_excel(fpath, header=3)
        
        file_name = fpath.split('/')[-1]
        date_info = extract_date_from_toorder_filename(file_name)
        df['date'] = date_info['date']
        
        dfs.append(df)
        print(f"   âœ“ {len(df)}í–‰ ë¡œë“œ")
    
    result_df = pd.concat(dfs, ignore_index=True)
    print(f"ë³‘í•© ì™„ë£Œ: {len(result_df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    output_path = temp_dir / f"toorder_review_raw_{context['ds_nodash']}.parquet"
    result_df.to_parquet(output_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key='toorder_review_path', value=str(output_path))
    return f"{len(result_df):,}ê±´"


def preprocess_toorder_review_df(
    input_task_id,
    input_xcom_key,
    output_xcom_key,
    **context
):
    """í† ë” ë¦¬ë·° ì „ì²˜ë¦¬"""
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(
        task_ids=input_task_id,
        key=input_xcom_key
    )
    
    if not parquet_path:
        print(f"[ê²½ê³ ] í† ë” ë¦¬ë·° ë°ì´í„° ì—†ìŒ - ìŠ¤í‚µ")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "0ê±´ (ì…ë ¥ ë°ì´í„° ì—†ìŒ)"
    
    toorder_review_df = pd.read_parquet(parquet_path)
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(toorder_review_df):,}í–‰")
    
    # ì»¬ëŸ¼ ì„ íƒ
    col = ['date', 'ë§¤ì¥ëª….1', 'ì±„ë„', 'ì£¼ë¬¸ ìˆ˜', 'ë¦¬ë·° ìˆ˜', 'ë‹µë³€ì™„ë£Œ ìˆ˜', 'í‰ê·  ë³„ì ']
    toorder_review_df = toorder_review_df[col]
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    toorder_review_df = toorder_review_df[~toorder_review_df["ì±„ë„"].isnull()]
    
    # 0ì  ì œì™¸ í‰ê·  í•¨ìˆ˜
    def mean_excluding_zero(x):
        non_zero = x[x > 0]
        return non_zero.mean() if len(non_zero) > 0 else 0
    
    # ê·¸ë£¹í™”
    toorder_review_df = toorder_review_df.groupby(["date", "ë§¤ì¥ëª….1"]).agg(
        ì „ì²´_ì£¼ë¬¸ìˆ˜=("ì£¼ë¬¸ ìˆ˜", "sum"),
        ì „ì²´_ë¦¬ë·°ìˆ˜=("ë¦¬ë·° ìˆ˜", "sum"),
        ì „ì²´_ë‹µë³€ì™„ë£Œìˆ˜=("ë‹µë³€ì™„ë£Œ ìˆ˜", "sum"),
        ì „ì²´_í‰ê· ë³„ì =("í‰ê·  ë³„ì ", mean_excluding_zero)
    ).reset_index()
    
    # ë§¤ì¥ëª… ì •ë¦¬
    toorder_review_df.rename(columns={"ë§¤ì¥ëª….1": "stores_name"}, inplace=True)
    toorder_review_df["stores_name"] = "ë„ë¦¬ë‹¹ " + toorder_review_df["stores_name"]
    toorder_review_df["stores_name"] = toorder_review_df["stores_name"].replace({
        "ë„ë¦¬ë‹¹ ì¼ì‚°ë°±ì„ì ": "ë„ë¦¬ë‹¹ ë°±ì„ì ",
        "ë„ë¦¬ë‹¹ ì„œìš¸ëŒ€ì ": "ë„ë¦¬ë‹¹ ì„œìš¸ëŒ€ì ",
        "ë„ë¦¬ë‹¹ êµ¬ë¡œë””ì§€í„¸ë‹¨ì§€ì ": "ë„ë¦¬ë‹¹ êµ¬ë¡œë””ì§€í„¸ì ",
        "ë„ë¦¬ë‹¹ ì¶©ì£¼ë´‰ë°©ì ": "ë„ë¦¬ë‹¹ ì¶©ì£¼ì—­ì "
    })
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(toorder_review_df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    processed_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    toorder_review_df.to_parquet(processed_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(processed_path))
    
    return f"ì „ì²˜ë¦¬: {len(toorder_review_df):,}í–‰"


# ============================================================
# ë°°ë¯¼ ìš°ë¦¬ê°€ê²Œ now
# ============================================================
def load_baemin_store_now_df(**context):
    """ë°°ë¯¼ ìš°ë¦¬ê°€ê²Œ now ë¡œë“œ (ì›ë³¸ ê²½ë¡œ)"""
    return load_data(
        file_path=PATH_NOW,
        xcom_key='baemin_store_now_path',
        use_glob=True,
        add_source_info=False,
        **context
    )


def preprocess_baemin_store_now_df(**context):
    """ë°°ë¯¼ ë§¤ì¥ í˜„í™© ì „ì²˜ë¦¬"""
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(
        task_ids='load_baemin_store_now',
        key='baemin_store_now_path'
    )
    
    if not parquet_path:
        print(f"[ê²½ê³ ] ë°°ë¯¼ í˜„í™© ë°ì´í„° ì—†ìŒ - ìŠ¤í‚µ")
        ti.xcom_push(key='processed_baemin_path', value=None)
        return "0ê±´ (ì…ë ¥ ë°ì´í„° ì—†ìŒ)"
    
    now_df = pd.read_parquet(parquet_path)
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(now_df):,}í–‰")
    
    now_df["collected_date"] = now_df["collected_at"].str[:10]
    now_df.drop_duplicates(subset=['store_id', 'collected_date'], keep='last', inplace=True)
    now_df["stores_name"] = now_df["store_name"].str.split(" ").str[-2:].str.join(" ")
    
    col = ['collected_date', 'stores_name', 'ì¡°ë¦¬ì†Œìš”ì‹œê°„',
           'ì¡°ë¦¬ì†Œìš”ì‹œê°„_ìˆœìœ„ë¹„ìœ¨', 'ì£¼ë¬¸ì ‘ìˆ˜ì‹œê°„', 'ì£¼ë¬¸ì ‘ìˆ˜ì‹œê°„_ìˆœìœ„ë¹„ìœ¨',
           'ì¡°ë¦¬ì‹œê°„ì¤€ìˆ˜ìœ¨', 'ì¡°ë¦¬ì‹œê°„ì¤€ìˆ˜ìœ¨_ìˆœìœ„ë¹„ìœ¨', 'ì£¼ë¬¸ì ‘ìˆ˜ìœ¨',
           'ì£¼ë¬¸ì ‘ìˆ˜ìœ¨_ìˆœìœ„ë¹„ìœ¨', 'ìµœê·¼ë³„ì ']
    
    now_df = now_df[col]
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(now_df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    processed_path = temp_dir / f"processed_baemin_{context['ds_nodash']}.parquet"
    now_df.to_parquet(processed_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key='processed_baemin_path', value=str(processed_path))
    return f"ì „ì²˜ë¦¬: {len(now_df):,}í–‰"


# ============================================================
# ë°°ë¯¼ ë³€ê²½ì´ë ¥
# ============================================================
def load_baemin_history_df(**context):
    """ë°°ë¯¼ ë³€ê²½ì´ë ¥ ë¡œë“œ (ì›ë³¸ ê²½ë¡œ)"""
    return load_data(
        file_path=PATH_HISTORY,
        xcom_key='baemin_history_path',
        use_glob=True,
        dedup_key=['ë³€ê²½ì‹œê°„', "store_id"],
        add_source_info=False,
        **context
    )


def preprocess_baemin_history_df(
    input_task_id,
    input_xcom_key,
    output_xcom_key,
    **context
):
    """ë°°ë¯¼ ë³€ê²½ì´ë ¥ ì „ì²˜ë¦¬"""
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(
        task_ids=input_task_id,
        key=input_xcom_key
    )
    
    if not parquet_path:
        print(f"[ê²½ê³ ] ë°°ë¯¼ ë³€ê²½ì´ë ¥ ë°ì´í„° ì—†ìŒ - ìŠ¤í‚µ")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "0ê±´ (ì…ë ¥ ë°ì´í„° ì—†ìŒ)"
    
    history_df = pd.read_parquet(parquet_path)
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(history_df):,}í–‰")
    
    history_df["change_date"] = history_df["ë³€ê²½ì‹œê°„"].str[:10]
    history_df = history_df.drop_duplicates(subset=["ë³€ê²½ì‹œê°„", "store_id"], keep='last')
    history_df["stores_name"] = history_df["ë§¤ì¥ëª…"].str.split(" ").str[-2:].str.join(" ")
    history_trans_df = history_df[["change_date", "stores_name", "ëŒ€ë¶„ë¥˜"]]

    history_trans_df = history_trans_df.groupby(
        ["change_date", "stores_name", "ëŒ€ë¶„ë¥˜"]
    ).size().reset_index(name='cnt').pivot_table(
        index=["change_date", "stores_name"],
        columns="ëŒ€ë¶„ë¥˜",
        values="cnt",
        fill_value=0,
        aggfunc='sum'
    ).reset_index().rename_axis(None, axis=1)
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(history_trans_df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    processed_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    history_trans_df.to_parquet(processed_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(processed_path))
    
    return f"ì „ì²˜ë¦¬: {len(history_trans_df):,}í–‰"


# ============================================================
# ì£¼ë¬¸ ì§‘ê³„ ë°ì´í„°
# ============================================================
def load_sales_daily_orders_alerts_df(**context):
    """
    ë§¤ì¶œ ì£¼ë¬¸ ì•Œë¦¼ ë°ì´í„° ë¡œë“œ
    
    â­ sales_daily_orders.pyì˜ filter_alerts()ì—ì„œ ì´ë¯¸ ìƒì„±ëœ
       sales_daily_orders_alerts.csvë¥¼ ì§ì ‘ ë¡œë“œ
       (ë³µì‚¬ X, ì´ë¯¸ ìƒì„±ëœ íŒŒì¼ ë¡œë“œ)
    """
    alerts_file = LOCAL_DB / 'ì˜ì—…ê´€ë¦¬ë¶€_DB' / 'sales_daily_orders_alerts.csv'
    
    print(f"[ë¡œë“œ] {alerts_file.name} ë¡œë“œ ì¤‘...")
    
    # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not alerts_file.exists():
        print(f"[âŒ ì—ëŸ¬] íŒŒì¼ ì—†ìŒ: {alerts_file}")
        print(f"[íŒíŠ¸] sales_daily_orders.pyì˜ filter_alerts()ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤")
        context['task_instance'].xcom_push(key='sales_daily_orders_alerts_path', value=None)
        return "0ê±´ (íŒŒì¼ ì—†ìŒ)"
    
    try:
        # CSV ì½ê¸°
        df = pd.read_csv(alerts_file, low_memory=False)
        
        print(f"[âœ… ë¡œë“œ] {len(df):,}ê±´ ë¡œë“œ ì™„ë£Œ")
        
        # order_daily ì»¬ëŸ¼ í™•ì¸ ë˜ëŠ” ì¶”ê°€
        if 'order_daily' not in df.columns:
            if 'order_date' in df.columns:
                df['order_daily'] = df['order_date']
                print(f"[ë³€í™˜] order_date â†’ order_daily (JOIN í˜¸í™˜ì„±)")
            else:
                print(f"[âŒ ì—ëŸ¬] order_daily/order_date ì»¬ëŸ¼ ì—†ìŒ")
                context['task_instance'].xcom_push(key='sales_daily_orders_alerts_path', value=None)
                return "0ê±´ (ì»¬ëŸ¼ ì˜¤ë¥˜)"
        
        # Parquet ì €ì¥ (JOINìš©)
        temp_dir = TEMP_DIR
        temp_dir.mkdir(exist_ok=True, parents=True)
        
        parquet_path = temp_dir / f"sales_daily_orders_alerts_{context['ds_nodash']}.parquet"
        df.to_parquet(parquet_path, index=False, engine='pyarrow')
        print(f"[âœ… ì €ì¥] Parquet ì €ì¥ ì™„ë£Œ: {parquet_path.name}")
        
        # XCom ì €ì¥
        context['task_instance'].xcom_push(key='sales_daily_orders_alerts_path', value=str(parquet_path))
        
        return f"âœ… {len(df):,}ê±´ ë¡œë“œë¨"
        
    except Exception as e:
        print(f"[âŒ ì—ëŸ¬] ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        context['task_instance'].xcom_push(key='sales_daily_orders_alerts_path', value=None)
        return f"0ê±´ (ë¡œë“œ ì—ëŸ¬: {str(e)[:50]})"


# ============================================================
# JOIN í•¨ìˆ˜ë“¤ (ë™ì¼í•˜ê²Œ ìœ ì§€)
# ============================================================
def left_join_orders_now(
    left_task,
    right_task,
    on=None,
    left_on=["order_daily", "ë§¤ì¥ëª…"],
    right_on=["collected_date", "stores_name"],
    how='left',
    drop_columns=["collected_date", "stores_name"],
    output_xcom_key='joined_orders_now_path',
    **context
):
    """ë‘ taskì˜ ë°ì´í„°ë¥¼ join"""
    ti = context['task_instance']
    
    if isinstance(left_task, str):
        left_task = {'task_id': left_task, 'xcom_key': 'sales_daily_orders_alerts_path'}
    if isinstance(right_task, str):
        right_task = {'task_id': right_task, 'xcom_key': 'processed_baemin_path'}
    
    left_path = ti.xcom_pull(
        task_ids=left_task['task_id'],
        key=left_task['xcom_key']
    )
    if not left_path:
        print(f"[ì—ëŸ¬] ì™¼ìª½ ë°ì´í„° ì—†ìŒ: {left_task['task_id']}")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "join ì‹¤íŒ¨: ì™¼ìª½ ë°ì´í„° ì—†ìŒ"
    
    left_df = pd.read_parquet(left_path)
    print(f"[ì™¼ìª½] {left_task['task_id']}: {len(left_df):,}í–‰")
    
    right_path = ti.xcom_pull(
        task_ids=right_task['task_id'],
        key=right_task['xcom_key']
    )
    if not right_path:
        print(f"[ê²½ê³ ] ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ: {right_task['task_id']} - ì™¼ìª½ ë°ì´í„°ë§Œ ì €ì¥")
        temp_dir = TEMP_DIR
        temp_dir.mkdir(exist_ok=True, parents=True)
        output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
        left_df.to_parquet(output_path, index=False, engine='pyarrow')
        ti.xcom_push(key=output_xcom_key, value=str(output_path))
        return f"âš ï¸ ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ, ì™¼ìª½ë§Œ ì €ì¥: {len(left_df):,}í–‰"
    
    right_df = pd.read_parquet(right_path)
    print(f"[ì˜¤ë¥¸ìª½] {right_task['task_id']}: {len(right_df):,}í–‰")
    
    # Join ì‹¤í–‰
    if on is not None:
        print(f"\n[JOIN] how={how}, on={on}")
        joined_df = left_df.merge(right_df, on=on, how=how)
    elif left_on is not None and right_on is not None:
        print(f"\n[JOIN] how={how}, left_on={left_on}, right_on={right_on}")
        joined_df = left_df.merge(right_df, left_on=left_on, right_on=right_on, how=how)
    else:
        raise ValueError("on ë˜ëŠ” (left_on, right_on)ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    print(f"[JOIN] ì™„ë£Œ: {len(joined_df):,}í–‰ Ã— {len(joined_df.columns)}ì»¬ëŸ¼")
    
    # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
    if drop_columns is None and left_on != right_on:
        drop_columns = right_on if isinstance(right_on, list) else [right_on]
    
    if drop_columns:
        cols_to_drop = [col for col in drop_columns if col in joined_df.columns]
        if cols_to_drop:
            joined_df.drop(columns=cols_to_drop, inplace=True)
            print(f"[ì •ë¦¬] ì œê±°ëœ ì»¬ëŸ¼: {cols_to_drop}")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    joined_df.to_parquet(output_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(output_path))
    return f"âœ… join ì™„ë£Œ: {len(joined_df):,}í–‰"


def left_join_orders_now_toorder(
    left_task,
    right_task,
    on=None,
    left_on=["order_daily", "ë§¤ì¥ëª…"],
    right_on=["date", "stores_name"],
    how='left',
    drop_columns=["date", "stores_name"],
    output_xcom_key='joined_orders_now_toorder_path',
    **context
):
    """(ì£¼ë¬¸ + ìš°ë¦¬ê°€ê²Œnow) ë°ì´í„°ì™€ í† ë” ë¦¬ë·° ì¡°ì¸"""
    ti = context['task_instance']
    
    if isinstance(left_task, str):
        left_task = {'task_id': left_task, 'xcom_key': 'joined_orders_now_path'}
    if isinstance(right_task, str):
        right_task = {'task_id': right_task, 'xcom_key': 'preprocessed_toorder_review_path'}
    
    left_path = ti.xcom_pull(
        task_ids=left_task['task_id'],
        key=left_task['xcom_key']
    )
    if not left_path:
        print(f"[ì—ëŸ¬] ì™¼ìª½ ë°ì´í„° ì—†ìŒ: {left_task['task_id']}")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "join ì‹¤íŒ¨: ì™¼ìª½ ë°ì´í„° ì—†ìŒ"
    
    left_df = pd.read_parquet(left_path)
    print(f"[ì™¼ìª½] {left_task['task_id']}: {len(left_df):,}í–‰")
    
    right_path = ti.xcom_pull(
        task_ids=right_task['task_id'],
        key=right_task['xcom_key']
    )
    
    if not right_path:
        print(f"[ê²½ê³ ] ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ: {right_task['task_id']} - ì™¼ìª½ ë°ì´í„°ë§Œ ì €ì¥")
        temp_dir = TEMP_DIR
        temp_dir.mkdir(exist_ok=True, parents=True)
        output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
        left_df.to_parquet(output_path, index=False, engine='pyarrow')
        ti.xcom_push(key=output_xcom_key, value=str(output_path))
        return f"âš ï¸ ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ, ì™¼ìª½ë§Œ ì €ì¥: {len(left_df):,}í–‰"
    
    right_df = pd.read_parquet(right_path)
    print(f"[ì˜¤ë¥¸ìª½] {right_task['task_id']}: {len(right_df):,}í–‰")
    
    # Join ì‹¤í–‰
    if on is not None:
        print(f"\n[JOIN] how={how}, on={on}")
        joined_df = left_df.merge(right_df, on=on, how=how)
    elif left_on is not None and right_on is not None:
        print(f"\n[JOIN] how={how}, left_on={left_on}, right_on={right_on}")
        joined_df = left_df.merge(right_df, left_on=left_on, right_on=right_on, how=how)
    else:
        raise ValueError("on ë˜ëŠ” (left_on, right_on)ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    print(f"[JOIN] ì™„ë£Œ: {len(joined_df):,}í–‰")
    
    # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
    if drop_columns is None and left_on != right_on:
        drop_columns = right_on if isinstance(right_on, list) else [right_on]
    
    if drop_columns:
        cols_to_drop = [col for col in drop_columns if col in joined_df.columns]
        if cols_to_drop:
            joined_df.drop(columns=cols_to_drop, inplace=True)
            print(f"[ì •ë¦¬] ì œê±°ëœ ì»¬ëŸ¼: {cols_to_drop}")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    joined_df.to_parquet(output_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(output_path))
    return f"âœ… join ì™„ë£Œ: {len(joined_df):,}í–‰"


def left_join_orders_now_toorder_history(
    left_task,
    right_task,
    on=None,
    left_on=["order_daily", "ë§¤ì¥ëª…"],
    right_on=["change_date", "stores_name"],
    how='left',
    drop_columns=["change_date", "stores_name"],
    output_xcom_key='joined_orders_now_toorder_history_path',
    **context
):
    """(ì£¼ë¬¸ + ìš°ë¦¬ê°€ê²Œnow + í† ë”) ë°ì´í„°ì™€ ë°°ë¯¼ ë³€ê²½ì´ë ¥ ì¡°ì¸"""
    ti = context['task_instance']
    
    if isinstance(left_task, str):
        left_task = {'task_id': left_task, 'xcom_key': 'joined_orders_now_toorder_path'}
    if isinstance(right_task, str):
        right_task = {'task_id': right_task, 'xcom_key': 'preprocessed_baemin_history_path'}
    
    left_path = ti.xcom_pull(
        task_ids=left_task['task_id'],
        key=left_task['xcom_key']
    )
    if not left_path:
        print(f"[ì—ëŸ¬] ì™¼ìª½ ë°ì´í„° ì—†ìŒ: {left_task['task_id']}")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "join ì‹¤íŒ¨: ì™¼ìª½ ë°ì´í„° ì—†ìŒ"
    
    left_df = pd.read_parquet(left_path)
    print(f"[ì™¼ìª½] {left_task['task_id']}: {len(left_df):,}í–‰")
    
    right_path = ti.xcom_pull(
        task_ids=right_task['task_id'],
        key=right_task['xcom_key']
    )
    
    if not right_path:
        print(f"[ê²½ê³ ] ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ: {right_task['task_id']} - ì™¼ìª½ ë°ì´í„°ë§Œ ì €ì¥")
        temp_dir = TEMP_DIR
        temp_dir.mkdir(exist_ok=True, parents=True)
        output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
        left_df.to_parquet(output_path, index=False, engine='pyarrow')
        ti.xcom_push(key=output_xcom_key, value=str(output_path))
        return f"âš ï¸ ì˜¤ë¥¸ìª½ ë°ì´í„° ì—†ìŒ, ì™¼ìª½ë§Œ ì €ì¥: {len(left_df):,}í–‰"
    
    right_df = pd.read_parquet(right_path)
    print(f"[ì˜¤ë¥¸ìª½] {right_task['task_id']}: {len(right_df):,}í–‰")
    
    # Join ì‹¤í–‰
    if on is not None:
        print(f"\n[JOIN] how={how}, on={on}")
        joined_df = left_df.merge(right_df, on=on, how=how)
    elif left_on is not None and right_on is not None:
        print(f"\n[JOIN] how={how}, left_on={left_on}, right_on={right_on}")
        joined_df = left_df.merge(right_df, left_on=left_on, right_on=right_on, how=how)
    else:
        raise ValueError("on ë˜ëŠ” (left_on, right_on)ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    print(f"[JOIN] ì™„ë£Œ: {len(joined_df):,}í–‰")
    
    # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
    if drop_columns is None and left_on != right_on:
        drop_columns = right_on if isinstance(right_on, list) else [right_on]
    
    if drop_columns:
        cols_to_drop = [col for col in drop_columns if col in joined_df.columns]
        if cols_to_drop:
            joined_df.drop(columns=cols_to_drop, inplace=True)
            print(f"[ì •ë¦¬] ì œê±°ëœ ì»¬ëŸ¼: {cols_to_drop}")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    output_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    joined_df.to_parquet(output_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(output_path))
    return f"âœ… join ì™„ë£Œ: {len(joined_df):,}í–‰"



# ì „ì²˜ë¦¬ ì¶”ê°€
def preprocess_add_main_left_join_df(
    input_task_id,
    input_xcom_key,
    output_xcom_key,
    **context
):
    """ë°°ë¯¼ ë³€ê²½ì´ë ¥ ì „ì²˜ë¦¬"""
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(
        task_ids=input_task_id,
        key=input_xcom_key
    )
    
    if not parquet_path:
        print(f"[ê²½ê³ ] ë°°ë¯¼ ë³€ê²½ì´ë ¥ ë°ì´í„° ì—†ìŒ - ìŠ¤í‚µ")
        ti.xcom_push(key=output_xcom_key, value=None)
        return "0ê±´ (ì…ë ¥ ë°ì´í„° ì—†ìŒ)"
    
    df = pd.read_parquet(parquet_path)
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: {len(df):,}í–‰")

    # ==========================================
    # 2. ë‚ ì§œ ë³€í™˜ ë° ìš”ì¼ ì¶”ê°€
    # ==========================================
    df["order_daily"] = pd.to_datetime(df["order_daily"], format="mixed")
    df["ìš”ì¼"] = df["order_daily"].dt.day_name()  # Monday, Tuesday...
    df["ìš”ì¼_í•œê¸€"] = df["order_daily"].dt.dayofweek.map({
        0: 'ì›”', 1: 'í™”', 2: 'ìˆ˜', 3: 'ëª©', 4: 'ê¸ˆ', 5: 'í† ', 6: 'ì¼'
    })

    df["order_week"] = df["order_daily"].dt.to_period("W")
    df["order_month"] = df["order_daily"].dt.to_period("M")

    # ë§¤ì¥ë³„ ì •ë ¬
    df = df.sort_values(by=["ë§¤ì¥ëª…", "order_daily"], ascending=[True, True])

    # ==========================================
    # 3. ì¡°ì¸ í‚¤ ìƒì„±
    # ==========================================
    df["join_pre_date"] = df["order_daily"] - dt.timedelta(days=1)  # ì „ì¼
    df["join_pre_week_sameday"] = df["order_daily"] - dt.timedelta(days=7)  # ì „ì£¼ë™ìš”ì¼
    df["join_pre_week"] = df["order_week"] - 1  # ì „ì£¼
    df["join_pre_month"] = df["order_month"] - 1  # ì „ì›”

    # ==========================================
    # 4. ì§‘ê³„í•  ì»¬ëŸ¼ ì •ì˜
    # ==========================================
    agg_columns = {
        "total_amount": "sum",
        "total_order_count": "sum",
        "settlement_amount": "sum",
        "total_amount_ë°°ë¯¼": "sum",
        "total_amount_ì¿ íŒ¡": "sum",
        "settlement_amount_ë°°ë¯¼": "sum",
        "settlement_amount_ì¿ íŒ¡": "sum",
        "total_order_count_ë°°ë¯¼": "sum",
        "total_order_count_ì¿ íŒ¡": "sum"
    }

    # ==========================================
    # 5. ì „ê¸° ë¹„êµ ë°ì´í„° ìƒì„±
    # ==========================================

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5-1. ì „ì¼ ë¹„êµ (ì¼ë³„ ì „ì²´ ë ˆë²¨)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ”„ ì „ì¼ ë¹„êµ ë°ì´í„° ìƒì„± ì¤‘...")
    daily_total = df.groupby("order_daily").agg(agg_columns).reset_index()

    pre_date_total = daily_total.copy()
    pre_date_total = pre_date_total.rename(columns={
        "order_daily": "join_pre_date",
        "total_amount": "ì „ì¼_ì „ì²´ë§¤ì¶œ",
        "total_order_count": "ì „ì¼_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜",
        "settlement_amount": "ì „ì¼_ì „ì²´ì •ì‚°ê¸ˆì•¡",
        "total_amount_ë°°ë¯¼": "ì „ì¼_ì „ì²´ë§¤ì¶œ_ë°°ë¯¼",
        "total_amount_ì¿ íŒ¡": "ì „ì¼_ì „ì²´ë§¤ì¶œ_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼": "ì „ì¼_ì „ì²´ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼",
        "settlement_amount_ì¿ íŒ¡": "ì „ì¼_ì „ì²´ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼": "ì „ì¼_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜_ë°°ë¯¼",
        "total_order_count_ì¿ íŒ¡": "ì „ì¼_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜_ì¿ íŒ¡"
    })

    df = df.merge(pre_date_total, on="join_pre_date", how="left")
    print("   âœ… ì „ì¼ ì „ì²´ ë ˆë²¨ ì™„ë£Œ")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5-2. ì „ì¼ ë¹„êµ (ì¼ë³„ ë§¤ì¥ ë ˆë²¨)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pre_date_store = df[[
        "order_daily", "ë§¤ì¥ëª…", 
        "total_amount", "total_order_count", "settlement_amount",
        "total_amount_ë°°ë¯¼", "total_amount_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼", "settlement_amount_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼", "total_order_count_ì¿ íŒ¡"
    ]].copy()

    pre_date_store = pre_date_store.rename(columns={
        "order_daily": "join_pre_date",
        "total_amount": "ì „ì¼_ë§¤ì¥ë§¤ì¶œ",
        "total_order_count": "ì „ì¼_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜",
        "settlement_amount": "ì „ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡",
        "total_amount_ë°°ë¯¼": "ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼",
        "total_amount_ì¿ íŒ¡": "ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼": "ì „ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼",
        "settlement_amount_ì¿ íŒ¡": "ì „ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼": "ì „ì¼_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜_ë°°ë¯¼",
        "total_order_count_ì¿ íŒ¡": "ì „ì¼_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜_ì¿ íŒ¡"
    })

    df = df.merge(pre_date_store, on=["join_pre_date", "ë§¤ì¥ëª…"], how="left")
    print("   âœ… ì „ì¼ ë§¤ì¥ ë ˆë²¨ ì™„ë£Œ")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5-3. ì „ì£¼ë™ìš”ì¼ ë¹„êµ (7ì¼ ì „, ì „ì²´ ë ˆë²¨)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ”„ ì „ì£¼ë™ìš”ì¼ ë¹„êµ ë°ì´í„° ìƒì„± ì¤‘...")
    pre_week_sameday_total = daily_total.copy()
    pre_week_sameday_total = pre_week_sameday_total.rename(columns={
        "order_daily": "join_pre_week_sameday",
        "total_amount": "ì „ì£¼ë™ìš”ì¼_ì „ì²´ë§¤ì¶œ",
        "total_order_count": "ì „ì£¼ë™ìš”ì¼_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜",
        "settlement_amount": "ì „ì£¼ë™ìš”ì¼_ì „ì²´ì •ì‚°ê¸ˆì•¡",
        "total_amount_ë°°ë¯¼": "ì „ì£¼ë™ìš”ì¼_ì „ì²´ë§¤ì¶œ_ë°°ë¯¼",
        "total_amount_ì¿ íŒ¡": "ì „ì£¼ë™ìš”ì¼_ì „ì²´ë§¤ì¶œ_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼": "ì „ì£¼ë™ìš”ì¼_ì „ì²´ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼",
        "settlement_amount_ì¿ íŒ¡": "ì „ì£¼ë™ìš”ì¼_ì „ì²´ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼": "ì „ì£¼ë™ìš”ì¼_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜_ë°°ë¯¼",
        "total_order_count_ì¿ íŒ¡": "ì „ì£¼ë™ìš”ì¼_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜_ì¿ íŒ¡"
    })

    df = df.merge(pre_week_sameday_total, on="join_pre_week_sameday", how="left")
    print("   âœ… ì „ì£¼ë™ìš”ì¼ ì „ì²´ ë ˆë²¨ ì™„ë£Œ")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5-4. ì „ì£¼ë™ìš”ì¼ ë¹„êµ (ë§¤ì¥ ë ˆë²¨)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pre_week_sameday_store = df[[
        "order_daily", "ë§¤ì¥ëª…", 
        "total_amount", "total_order_count", "settlement_amount",
        "total_amount_ë°°ë¯¼", "total_amount_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼", "settlement_amount_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼", "total_order_count_ì¿ íŒ¡"
    ]].copy()

    pre_week_sameday_store = pre_week_sameday_store.rename(columns={
        "order_daily": "join_pre_week_sameday",
        "total_amount": "ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ",
        "total_order_count": "ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜",
        "settlement_amount": "ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡",
        "total_amount_ë°°ë¯¼": "ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼",
        "total_amount_ì¿ íŒ¡": "ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼": "ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼",
        "settlement_amount_ì¿ íŒ¡": "ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼": "ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜_ë°°ë¯¼",
        "total_order_count_ì¿ íŒ¡": "ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜_ì¿ íŒ¡"
    })

    df = df.merge(pre_week_sameday_store, on=["join_pre_week_sameday", "ë§¤ì¥ëª…"], how="left")
    print("   âœ… ì „ì£¼ë™ìš”ì¼ ë§¤ì¥ ë ˆë²¨ ì™„ë£Œ")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5-5. ì „ì£¼ ë¹„êµ (ì£¼ë³„ ì „ì²´ ë ˆë²¨)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ”„ ì „ì£¼ ë¹„êµ ë°ì´í„° ìƒì„± ì¤‘...")
    weekly_total = df.groupby("order_week").agg(agg_columns).reset_index()

    pre_week_total = weekly_total.copy()
    pre_week_total = pre_week_total.rename(columns={
        "order_week": "join_pre_week",
        "total_amount": "ì „ì£¼_ì „ì²´ë§¤ì¶œ",
        "total_order_count": "ì „ì£¼_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜",
        "settlement_amount": "ì „ì£¼_ì „ì²´ì •ì‚°ê¸ˆì•¡",
        "total_amount_ë°°ë¯¼": "ì „ì£¼_ì „ì²´ë§¤ì¶œ_ë°°ë¯¼",
        "total_amount_ì¿ íŒ¡": "ì „ì£¼_ì „ì²´ë§¤ì¶œ_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼": "ì „ì£¼_ì „ì²´ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼",
        "settlement_amount_ì¿ íŒ¡": "ì „ì£¼_ì „ì²´ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼": "ì „ì£¼_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜_ë°°ë¯¼",
        "total_order_count_ì¿ íŒ¡": "ì „ì£¼_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜_ì¿ íŒ¡"
    })

    df = df.merge(pre_week_total, on="join_pre_week", how="left")
    print("   âœ… ì „ì£¼ ì „ì²´ ë ˆë²¨ ì™„ë£Œ")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5-6. ì „ì£¼ ë¹„êµ (ì£¼ë³„ ë§¤ì¥ ë ˆë²¨)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    weekly_store = df.groupby(["order_week", "ë§¤ì¥ëª…"]).agg(agg_columns).reset_index()

    pre_week_store = weekly_store.copy()
    pre_week_store = pre_week_store.rename(columns={
        "order_week": "join_pre_week",
        "total_amount": "ì „ì£¼_ë§¤ì¥ë§¤ì¶œ",
        "total_order_count": "ì „ì£¼_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜",
        "settlement_amount": "ì „ì£¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡",
        "total_amount_ë°°ë¯¼": "ì „ì£¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼",
        "total_amount_ì¿ íŒ¡": "ì „ì£¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼": "ì „ì£¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼",
        "settlement_amount_ì¿ íŒ¡": "ì „ì£¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼": "ì „ì£¼_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜_ë°°ë¯¼",
        "total_order_count_ì¿ íŒ¡": "ì „ì£¼_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜_ì¿ íŒ¡"
    })

    df = df.merge(pre_week_store, on=["join_pre_week", "ë§¤ì¥ëª…"], how="left")
    print("   âœ… ì „ì£¼ ë§¤ì¥ ë ˆë²¨ ì™„ë£Œ")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5-7. ì „ì›” ë¹„êµ (ì›”ë³„ ì „ì²´ ë ˆë²¨)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ”„ ì „ì›” ë¹„êµ ë°ì´í„° ìƒì„± ì¤‘...")
    monthly_total = df.groupby("order_month").agg(agg_columns).reset_index()

    pre_month_total = monthly_total.copy()
    pre_month_total = pre_month_total.rename(columns={
        "order_month": "join_pre_month",
        "total_amount": "ì „ì›”_ì „ì²´ë§¤ì¶œ",
        "total_order_count": "ì „ì›”_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜",
        "settlement_amount": "ì „ì›”_ì „ì²´ì •ì‚°ê¸ˆì•¡",
        "total_amount_ë°°ë¯¼": "ì „ì›”_ì „ì²´ë§¤ì¶œ_ë°°ë¯¼",
        "total_amount_ì¿ íŒ¡": "ì „ì›”_ì „ì²´ë§¤ì¶œ_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼": "ì „ì›”_ì „ì²´ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼",
        "settlement_amount_ì¿ íŒ¡": "ì „ì›”_ì „ì²´ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼": "ì „ì›”_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜_ë°°ë¯¼",
        "total_order_count_ì¿ íŒ¡": "ì „ì›”_ì „ì²´ì£¼ë¬¸ê±´ìˆ˜_ì¿ íŒ¡"
    })

    df = df.merge(pre_month_total, on="join_pre_month", how="left")
    print("   âœ… ì „ì›” ì „ì²´ ë ˆë²¨ ì™„ë£Œ")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5-8. ì „ì›” ë¹„êµ (ì›”ë³„ ë§¤ì¥ ë ˆë²¨)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    monthly_store = df.groupby(["order_month", "ë§¤ì¥ëª…"]).agg(agg_columns).reset_index()

    pre_month_store = monthly_store.copy()
    pre_month_store = pre_month_store.rename(columns={
        "order_month": "join_pre_month",
        "total_amount": "ì „ì›”_ë§¤ì¥ë§¤ì¶œ",
        "total_order_count": "ì „ì›”_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜",
        "settlement_amount": "ì „ì›”_ë§¤ì¥ì •ì‚°ê¸ˆì•¡",
        "total_amount_ë°°ë¯¼": "ì „ì›”_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼",
        "total_amount_ì¿ íŒ¡": "ì „ì›”_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡",
        "settlement_amount_ë°°ë¯¼": "ì „ì›”_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼",
        "settlement_amount_ì¿ íŒ¡": "ì „ì›”_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡",
        "total_order_count_ë°°ë¯¼": "ì „ì›”_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜_ë°°ë¯¼",
        "total_order_count_ì¿ íŒ¡": "ì „ì›”_ë§¤ì¥ì£¼ë¬¸ê±´ìˆ˜_ì¿ íŒ¡"
    })

    df = df.merge(pre_month_store, on=["join_pre_month", "ë§¤ì¥ëª…"], how="left")
    print("   âœ… ì „ì›” ë§¤ì¥ ë ˆë²¨ ì™„ë£Œ")

    # ==========================================
    # 6. ìˆ˜ìˆ˜ë£Œìœ¨ ê³„ì‚°
    # ==========================================
    print("\nğŸ’³ ìˆ˜ìˆ˜ë£Œìœ¨ ê³„ì‚° ì¤‘...")

    # í˜„ì¬ ìˆ˜ìˆ˜ë£Œìœ¨
    df["ìˆ˜ìˆ˜ë£Œìœ¨"] = ((df["total_amount"] - df["settlement_amount"]) / df["total_amount"] * 100).round(2)
    df["ìˆ˜ìˆ˜ë£Œìœ¨_ë°°ë¯¼"] = ((df["total_amount_ë°°ë¯¼"] - df["settlement_amount_ë°°ë¯¼"]) / df["total_amount_ë°°ë¯¼"] * 100).round(2)
    df["ìˆ˜ìˆ˜ë£Œìœ¨_ì¿ íŒ¡"] = ((df["total_amount_ì¿ íŒ¡"] - df["settlement_amount_ì¿ íŒ¡"]) / df["total_amount_ì¿ íŒ¡"] * 100).round(2)

    # ì „ì¼ ìˆ˜ìˆ˜ë£Œìœ¨ (ë§¤ì¥ë³„)
    df["ì „ì¼_ìˆ˜ìˆ˜ë£Œìœ¨"] = ((df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ"] - df["ì „ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡"]) / df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ"] * 100).round(2)
    df["ì „ì¼_ìˆ˜ìˆ˜ë£Œìœ¨_ë°°ë¯¼"] = ((df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼"] - df["ì „ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼"]) / df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼"] * 100).round(2)
    df["ì „ì¼_ìˆ˜ìˆ˜ë£Œìœ¨_ì¿ íŒ¡"] = ((df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡"] - df["ì „ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡"]) / df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡"] * 100).round(2)

    # ì „ì£¼ë™ìš”ì¼ ìˆ˜ìˆ˜ë£Œìœ¨ (ë§¤ì¥ë³„)
    df["ì „ì£¼ë™ìš”ì¼_ìˆ˜ìˆ˜ë£Œìœ¨"] = ((df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ"] - df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡"]) / df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ"] * 100).round(2)
    df["ì „ì£¼ë™ìš”ì¼_ìˆ˜ìˆ˜ë£Œìœ¨_ë°°ë¯¼"] = ((df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼"] - df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ë°°ë¯¼"]) / df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼"] * 100).round(2)
    df["ì „ì£¼ë™ìš”ì¼_ìˆ˜ìˆ˜ë£Œìœ¨_ì¿ íŒ¡"] = ((df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡"] - df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ì •ì‚°ê¸ˆì•¡_ì¿ íŒ¡"]) / df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡"] * 100).round(2)

    print("   âœ… ìˆ˜ìˆ˜ë£Œìœ¨ ê³„ì‚° ì™„ë£Œ")

    # ==========================================
    # 7. ì¦ê°ì•¡/ì¦ê°ë¥  ê³„ì‚°
    # ==========================================
    print("\nğŸ“Š ì¦ê° ì§€í‘œ ê³„ì‚° ì¤‘...")

    # ì „ì¼ ëŒ€ë¹„
    df["ì „ì¼ëŒ€ë¹„_ë§¤ì¶œì¦ê°ì•¡"] = df["total_amount"] - df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ"]
    df["ì „ì¼ëŒ€ë¹„_ë§¤ì¶œì¦ê°ë¥ "] = ((df["total_amount"] - df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ"]) / df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ"] * 100).round(2)
    df["ì „ì¼ëŒ€ë¹„_ìˆ˜ìˆ˜ë£Œìœ¨ì¦ê°"] = (df["ìˆ˜ìˆ˜ë£Œìœ¨"] - df["ì „ì¼_ìˆ˜ìˆ˜ë£Œìœ¨"]).round(2)

    # ì „ì¼ ëŒ€ë¹„ - ë°°ë¯¼
    df["ì „ì¼ëŒ€ë¹„_ë§¤ì¶œì¦ê°ë¥ _ë°°ë¯¼"] = ((df["total_amount_ë°°ë¯¼"] - df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼"]) / df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼"] * 100).round(2)
    df["ì „ì¼ëŒ€ë¹„_ìˆ˜ìˆ˜ë£Œìœ¨ì¦ê°_ë°°ë¯¼"] = (df["ìˆ˜ìˆ˜ë£Œìœ¨_ë°°ë¯¼"] - df["ì „ì¼_ìˆ˜ìˆ˜ë£Œìœ¨_ë°°ë¯¼"]).round(2)

    # ì „ì¼ ëŒ€ë¹„ - ì¿ íŒ¡
    df["ì „ì¼ëŒ€ë¹„_ë§¤ì¶œì¦ê°ë¥ _ì¿ íŒ¡"] = ((df["total_amount_ì¿ íŒ¡"] - df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡"]) / df["ì „ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡"] * 100).round(2)
    df["ì „ì¼ëŒ€ë¹„_ìˆ˜ìˆ˜ë£Œìœ¨ì¦ê°_ì¿ íŒ¡"] = (df["ìˆ˜ìˆ˜ë£Œìœ¨_ì¿ íŒ¡"] - df["ì „ì¼_ìˆ˜ìˆ˜ë£Œìœ¨_ì¿ íŒ¡"]).round(2)

    # ì „ì£¼ë™ìš”ì¼ ëŒ€ë¹„
    df["ì „ì£¼ë™ìš”ì¼ëŒ€ë¹„_ë§¤ì¶œì¦ê°ì•¡"] = df["total_amount"] - df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ"]
    df["ì „ì£¼ë™ìš”ì¼ëŒ€ë¹„_ë§¤ì¶œì¦ê°ë¥ "] = ((df["total_amount"] - df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ"]) / df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ"] * 100).round(2)
    df["ì „ì£¼ë™ìš”ì¼ëŒ€ë¹„_ìˆ˜ìˆ˜ë£Œìœ¨ì¦ê°"] = (df["ìˆ˜ìˆ˜ë£Œìœ¨"] - df["ì „ì£¼ë™ìš”ì¼_ìˆ˜ìˆ˜ë£Œìœ¨"]).round(2)

    # ì „ì£¼ë™ìš”ì¼ ëŒ€ë¹„ - ë°°ë¯¼
    df["ì „ì£¼ë™ìš”ì¼ëŒ€ë¹„_ë§¤ì¶œì¦ê°ë¥ _ë°°ë¯¼"] = ((df["total_amount_ë°°ë¯¼"] - df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼"]) / df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ë°°ë¯¼"] * 100).round(2)
    df["ì „ì£¼ë™ìš”ì¼ëŒ€ë¹„_ìˆ˜ìˆ˜ë£Œìœ¨ì¦ê°_ë°°ë¯¼"] = (df["ìˆ˜ìˆ˜ë£Œìœ¨_ë°°ë¯¼"] - df["ì „ì£¼ë™ìš”ì¼_ìˆ˜ìˆ˜ë£Œìœ¨_ë°°ë¯¼"]).round(2)

    # ì „ì£¼ë™ìš”ì¼ ëŒ€ë¹„ - ì¿ íŒ¡
    df["ì „ì£¼ë™ìš”ì¼ëŒ€ë¹„_ë§¤ì¶œì¦ê°ë¥ _ì¿ íŒ¡"] = ((df["total_amount_ì¿ íŒ¡"] - df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡"]) / df["ì „ì£¼ë™ìš”ì¼_ë§¤ì¥ë§¤ì¶œ_ì¿ íŒ¡"] * 100).round(2)
    df["ì „ì£¼ë™ìš”ì¼ëŒ€ë¹„_ìˆ˜ìˆ˜ë£Œìœ¨ì¦ê°_ì¿ íŒ¡"] = (df["ìˆ˜ìˆ˜ë£Œìœ¨_ì¿ íŒ¡"] - df["ì „ì£¼ë™ìš”ì¼_ìˆ˜ìˆ˜ë£Œìœ¨_ì¿ íŒ¡"]).round(2)

    print("   âœ… ì¦ê° ì§€í‘œ ê³„ì‚° ì™„ë£Œ")

    # ==========================================
    # 8. ê¸°ê°„ êµ¬ë¶„ ì»¬ëŸ¼ ì¶”ê°€ (ê¸ˆì¼/ì „ì¼/ê¸ˆì£¼/ì €ë²ˆì£¼/2ì£¼ì „/ê¸ˆì›”/ì „ì›”/2ê°œì›”ì „)
    # ==========================================
    print("\nğŸ“… ê¸°ê°„ êµ¬ë¶„ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
    
    # ê¸°ì¤€ì¼ ì„¤ì • (order_daily ìµœëŒ€ê°’)
    today = df["order_daily"].max()
    today_date = pd.to_datetime(today).date()
    
    # ë‚ ì§œ ì •ë³´ ê³„ì‚°
    yesterday = today - pd.Timedelta(days=1)
    two_weeks_ago = today - pd.Timedelta(days=14)
    two_months_ago = today - pd.DateOffset(months=2)
    
    # í˜„ì¬ ì£¼, ì €ë²ˆ ì£¼, 2ì£¼ì „ ì£¼ì˜ ì‹œì‘ ë° ì¢…ë£Œ
    today_dt = pd.to_datetime(today)
    this_week_start = today_dt - pd.Timedelta(days=today_dt.weekday())
    last_week_start = this_week_start - pd.Timedelta(days=7)
    last_week_end = this_week_start - pd.Timedelta(days=1)
    two_weeks_start = last_week_start - pd.Timedelta(days=7)
    two_weeks_end = last_week_start - pd.Timedelta(days=1)
    
    # í˜„ì¬ ë‹¬, ì§€ë‚œ ë‹¬, 2ê°œì›” ì „
    this_month_start = today_dt.replace(day=1)
    last_month_start = (this_month_start - pd.Timedelta(days=1)).replace(day=1)
    two_months_start = (last_month_start - pd.Timedelta(days=1)).replace(day=1)
    
    # ê¸°ê°„ êµ¬ë¶„ í•¨ìˆ˜
    def get_period_type(date_val):
        if pd.isna(date_val):
            return ''
        
        d = pd.to_datetime(date_val)
        
        # ê¸ˆì¼/ì „ì¼
        if d.date() == today_date:
            return 'ê¸ˆì¼'
        elif d.date() == (today_date - pd.Timedelta(days=1)):
            return 'ì „ì¼'
        
        # ê¸ˆì£¼/ì €ë²ˆì£¼/2ì£¼ì „
        if d >= this_week_start:
            return 'ê¸ˆì£¼'
        elif d >= last_week_start and d <= last_week_end:
            return 'ì €ë²ˆì£¼'
        elif d >= two_weeks_start and d <= two_weeks_end:
            return '2ì£¼ì „'
        
        # ê¸ˆì›”/ì „ì›”/2ê°œì›”ì „
        if d >= this_month_start:
            return 'ê¸ˆì›”'
        elif d >= last_month_start:
            return 'ì „ì›”'
        elif d >= two_months_start:
            return '2ê°œì›”ì „'
        
        return ''
    
    df['ê¸°ê°„êµ¬ë¶„'] = df['order_daily'].apply(get_period_type)
    
    print(f"   âœ… ê¸°ê°„ êµ¬ë¶„ ì™„ë£Œ")
    print(f"   ë¶„í¬: {df['ê¸°ê°„êµ¬ë¶„'].value_counts().to_dict()}")
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df):,}í–‰")
    
    temp_dir = TEMP_DIR
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    processed_path = temp_dir / f"{output_xcom_key}_{context['ds_nodash']}.parquet"
    df.to_parquet(processed_path, index=False, engine='pyarrow')
    
    ti.xcom_push(key=output_xcom_key, value=str(processed_path))
    
    return f"ì „ì²˜ë¦¬: {len(df):,}í–‰"



# ============================================================
# CSV ì €ì¥
# ============================================================
def fin_save_to_csv(
    input_task_id,
    input_xcom_key,
    output_csv_path=None,
    output_filename='sales_daily_orders_upload.csv',
    output_subdir='ì˜ì—…ê´€ë¦¬ë¶€_DB',
    dedup_key=None,
    **context
):
    """Parquet ë°ì´í„°ë¥¼ ë¡œì»¬ DBì— CSVë¡œ ì €ì¥"""
    import os
    import shutil
    import tempfile
    from modules.transform.utility.paths import LOCAL_DB
    
    ti = context['task_instance']
    
    parquet_path = ti.xcom_pull(task_ids=input_task_id, key=input_xcom_key)
    
    if not parquet_path:
        print(f"[ê²½ê³ ] ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: ë°ì´í„° ì—†ìŒ"
    
    if not os.path.exists(parquet_path):
        print(f"[ê²½ê³ ] íŒŒì¼ ê²½ë¡œ ì—†ìŒ: {parquet_path}")
        return "âš ï¸ ì €ì¥ ìŠ¤í‚µ: íŒŒì¼ ì—†ìŒ"
    
    df = pd.read_parquet(parquet_path)
    
    print(f"\n{'='*60}")
    print(f"[ì…ë ¥] ë°ì´í„°: {len(df):,}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")
    
    # ì¤‘ë³µ ì œê±°
    if dedup_key:
        dedup_cols = [dedup_key] if isinstance(dedup_key, str) else dedup_key
        valid_cols = [c for c in dedup_cols if c in df.columns]
        if valid_cols:
            before = len(df)
            df.drop_duplicates(subset=valid_cols, keep='first', inplace=True)
            after = len(df)
            if before - after > 0:
                print(f"\n[ì¤‘ë³µ ì œê±°] {valid_cols} ê¸°ì¤€: {before - after:,}ê±´ ì œê±°")
    
    # ì¶œë ¥ ê²½ë¡œ
    if output_csv_path:
        local_csv_path = Path(output_csv_path)
    else:
        output_dir = LOCAL_DB / output_subdir
        output_dir.mkdir(parents=True, exist_ok=True)
        local_csv_path = output_dir / output_filename
    
    local_csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\n[ê²½ë¡œ] ì €ì¥ ìœ„ì¹˜: {local_csv_path}")
    
    # datetime ë³€í™˜
    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
    if datetime_cols:
        for col in datetime_cols:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S').fillna('')
    
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(
            mode='w', 
            delete=False, 
            dir=str(local_csv_path.parent),
            prefix='tmp_', 
            suffix='.csv', 
            encoding='utf-8-sig'
        ) as tmp_file:
            tmp_path = tmp_file.name
        
        df.to_csv(tmp_path, index=False, encoding='utf-8-sig')
        
        if local_csv_path.exists():
            backup_path = local_csv_path.parent / f"{local_csv_path.name}.bak"
            shutil.copy2(local_csv_path, backup_path)
            shutil.move(tmp_path, str(local_csv_path))
            backup_path.unlink()
        else:
            shutil.move(tmp_path, str(local_csv_path))
        
        csv_size = local_csv_path.stat().st_size / (1024 * 1024)
        print(f"[ì €ì¥] âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´ ({csv_size:.2f} MB)")
        
    except Exception as e:
        print(f"[ì—ëŸ¬] CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        if tmp_path and os.path.exists(tmp_path):
            os.remove(tmp_path)
        return f"ì €ì¥ ì‹¤íŒ¨: {e}"
    
    print(f"{'='*60}\n")
    return f"âœ… ì €ì¥ ì™„ë£Œ: {len(df):,}ê±´"